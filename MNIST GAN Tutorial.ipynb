{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particle physics, Generative Adversarial Networks hold a lot of promise for studying data/MC differences.  In principle, the physics tools used for simulation of particle interaction data are quite good, but they are never perfect.  Additionally, they are always imperfect in difficult to model ways.  Many experiments spend a lot of time studying the differences between the output of their simulation and their real detector data, and a deep network that can learn these differences is really useful for making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll cover some basics of generative adversarial networks with very simple feed-forward neural networks (not even convolutional) as a demonstration of the basic techniques of GANs.  You can read the original paper on GANs here: https://arxiv.org/abs/1406.2661.  The basic idea is you train two networks to compete with each other.  The first (called the discriminator) makes a decision on whether or not the images it's looking at are real or fake.  The second (called the generator) tries to generate fake images from random noise to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the years since the original paper came out, GANs have grown increasinly more sophisticated and impressive, especially with the advent of the Deep Convolutional Generative Adversarial Network (DCGAN, original paper: https://arxiv.org/abs/1511.06434).  For this tutorial, we're going to eschew all of the recent advances to make a GAN that can generate artificial digits based on the mnist data set.  By stripping the networks down to their basics, it's easier to focus on the core aspects of the loss function and outputs, and to avoid the complications of training on GPUs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist data set is one of the most famous collections of labeled images that exists.  You can read all about it at the original website (http://yann.lecun.com/exdb/mnist/), but it has a few nice advantages that make it ideal for learning:\n",
    " * It's open, and has convient wrappers in many languages (we'll use tensorflows soon)\n",
    " * It's a large data set (60k training images) but each image is small (28x28)\n",
    " * The data has been preprocessed to center images of digits and make them easier to use\n",
    " \n",
    "Basically, you can focus on the fundamentals with this data set, which is why we'll use it here.  Some examples of loading it with tensorflow (more here: https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = mnist.train.next_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've loaded the mnist data from the tensorflow helper class, and asked for the next batch of images and labels.  Let's view those to see what the data looks like before delving into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 784)\n",
      "mnist_labels.shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n",
    "print \"mnist_labels.shape: {}\".format(mnist_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, variables are arrays with the outermost dimension equal to 5.  The images, though, comes unpacked as a 1D array per image instead of a 2D array.  We can reshape this to what we're more familar with, since we know mnist images are 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images = numpy.reshape(mnist_images, (-1, 28, 28)) # -1 can be used as a placeholder for batch size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib gives a good interface for viewing these images in a notebook (or even in general, in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is labeled as 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAJKCAYAAAAx/3HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4dbd622390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mnist_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "print \"This image is labeled as {}\".format(mnist_labels[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn like this, you can see each individual pixel clearly.  It's not a high resolution image, but you can clearly tell what the digit is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.000001\n",
    "BATCH_SIZE=64 # Keep this even\n",
    "LOGDIR=\"./mnist_gan_logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real images and random noise, so make a placeholder for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    # Input noise to the generator:\n",
    "    noise_tensor = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 10*10], name=\"noise\")\n",
    "    fake_input   = tf.reshape(noise_tensor, (tf.shape(noise_tensor)[0], 10,10, 1))\n",
    "\n",
    "    # Placeholder for the discriminator input:\n",
    "    real_flat  = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 784], name='x')\n",
    "    # label_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, 1], name='labels')\n",
    "    real_images  = tf.reshape(real_flat, (tf.shape(real_flat)[0], 28, 28, 1))\n",
    "    \n",
    "    # We augment the input to the discriminator with gaussian noise\n",
    "    # This makes it harder for the discriminator to do it's job, preventing\n",
    "    # it from always \"winning\" the GAN min/max contest\n",
    "    real_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"real_noise\")\n",
    "    fake_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"fake_noise\")\n",
    "\n",
    "    real_images = real_noise + real_images\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input tensors (noise_tensor, real_images) are shaped in the 'flattened' way: (N/2, 100) for noise, (N/2, 784) for real images.  This lets me input the mnist images directly to tensorflow, as well as the noise.  They are then reshaped to be like tensorflow images (Batch, H, W, Filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to build the discriminator, using fully connected networks.  Note that a convolutional layer with the stride equal to the image size *is* a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor, reuse,reg=0.1):\n",
    "    # Use scoping to keep the variables nicely organized in the graph.\n",
    "    # Scoping is good practice always, but it's *essential* here as we'll see later on\n",
    "    with tf.variable_scope(\"mnist_discriminator\", reuse=reuse):\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=128, #Connecting to 128 output neurons, each it's own filter\n",
    "                             kernel_size=[28,28], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        # Apply a non linearity:\n",
    "        x = tf.nn.relu(x)\n",
    "        # That maps to a hidden layer, shape is (B, 1, 1, 128), let's now map to a single output\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=1,         #Connecting to 1 output neuron\n",
    "                             kernel_size=[1,1], \n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Since we want to predict \"real\" or \"fake\", an output of 0 or 1 is desired.  sigmoid is perfect for this:\n",
    "        x = tf.nn.sigmoid(x, name=\"discriminator_sigmoid\")\n",
    "        #Reshape this to bring it down to just one output per image:\n",
    "        x = tf.reshape(x, (x.get_shape().as_list()[0],))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    real_image_logits = build_discriminator(real_images, reuse=False,reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to generate random images from noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_tensor, reg=0.2):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_generator\"):\n",
    "        # Input is Bx10x10x1, let's use two hidden layers this time to upsample to 28x28:\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=256, #256 is essentially a 16x16 image\n",
    "                             kernel_size=[10,10], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        # Another fully connected layer, no change to resolution yet:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=256, #256 is essentially a 16x16 image\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Final layer, step up to \"full\" resolution and then reshape the tensor:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=784, #784 is a 28x28 image\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_3\")\n",
    "        \n",
    "        # Reshape to match mnist images:\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))\n",
    "        \n",
    "        # The final non linearity applied here is to map the images onto the [-1,1] range.\n",
    "        x = tf.nn.tanh(x, name=\"generator_tanh\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    fake_images = build_generator(fake_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to run the discriminator on the fake images, so set that up too.  Since it trains on both real and fake images, set reuse=True here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    fake_image_logits = build_discriminator(fake_images, reuse=True, reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the generator and discriminator seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the loss functions:\n",
    "with g.as_default():\n",
    "    with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "        #Discriminator loss on real images (classify as 1):\n",
    "        d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_image_logits,\n",
    "            labels = tf.ones_like(real_image_logits)))\n",
    "        #Discriminator loss on fake images (classify as 0):\n",
    "        d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "            labels = tf.zeros_like(fake_image_logits)))\n",
    "\n",
    "        # Total discriminator loss is the sum:\n",
    "        d_loss_total = d_loss_real + d_loss_fake\n",
    "\n",
    "        # This is the adverserial step: g_loss tries to optimize fake_logits to one,\n",
    "        # While d_loss_fake tries to optimize fake_logits to zero.\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "            labels = tf.ones_like(fake_image_logits)))\n",
    "        \n",
    "        # This code is useful if you'll use tensorboard to monitor training:\n",
    "        d_loss_summary = tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "        d_loss_summary = tf.summary.scalar(\"Discriminator_Fake_Loss\", d_loss_fake)\n",
    "        d_loss_summary = tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss_total)\n",
    "        d_loss_summary = tf.summary.scalar(\"Generator_Loss\", g_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "        accuracy_real  = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_logits), \n",
    "                                                         tf.ones_like(real_image_logits)), \n",
    "                                                tf.float32))\n",
    "        accuracy_fake  = tf.reduce_mean(tf.cast(tf.equal(tf.round(fake_image_logits), \n",
    "                                                         tf.zeros_like(fake_image_logits)), \n",
    "                                                tf.float32))\n",
    "        \n",
    "        total_accuracy = 0.5*(accuracy_fake +  accuracy_real)\n",
    "    \n",
    "        # Again, useful for tensorboard:\n",
    "        acc_real_summary = tf.summary.scalar(\"Real_Accuracy\", accuracy_real)\n",
    "        acc_real_summary = tf.summary.scalar(\"Fake_Accuracy\", accuracy_fake)\n",
    "        acc_real_summary = tf.summary.scalar(\"Total_Accuracy\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        # Global steps are useful for restoring training:\n",
    "        global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        # Make sure the optimizers are only operating on their own variables:\n",
    "        \n",
    "        all_variables      = tf.trainable_variables()\n",
    "        discriminator_vars = [v for v in all_variables if v.name.startswith('mnist_discriminator/')]\n",
    "        generator_vars     = [v for v in all_variables if v.name.startswith('mnist_generator/')]\n",
    "    \n",
    "    \n",
    "        discriminator_optimizer = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "            d_loss_total, global_step=global_step, var_list=discriminator_vars)\n",
    "        generator_optimizer     = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "            g_loss, global_step=global_step, var_list=generator_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    tf.summary.image('fake_images', fake_images, max_outputs=4)\n",
    "    tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Set up a saver:\n",
    "    train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ...\n",
      "Training in progress @ global_step 0, g_loss 0.473839, d_loss 0.474636 accuracy 0.234375\n",
      "Training in progress @ global_step 50, g_loss 0.473827, d_loss 0.464991 accuracy 0.46875\n",
      "Training in progress @ global_step 100, g_loss 0.473856, d_loss 0.45017 accuracy 0.53125\n",
      "Training in progress @ global_step 150, g_loss 0.473781, d_loss 0.447407 accuracy 0.5\n",
      "Training in progress @ global_step 200, g_loss 0.473789, d_loss 0.434797 accuracy 0.5\n",
      "Training in progress @ global_step 250, g_loss 0.473789, d_loss 0.426994 accuracy 0.515625\n",
      "Training in progress @ global_step 300, g_loss 0.473739, d_loss 0.423329 accuracy 0.53125\n",
      "Training in progress @ global_step 350, g_loss 0.473734, d_loss 0.418147 accuracy 0.515625\n",
      "Training in progress @ global_step 400, g_loss 0.473724, d_loss 0.407274 accuracy 0.515625\n",
      "Training in progress @ global_step 450, g_loss 0.473785, d_loss 0.402142 accuracy 0.578125\n",
      "Training in progress @ global_step 500, g_loss 0.473679, d_loss 0.395108 accuracy 0.5\n",
      "Training in progress @ global_step 550, g_loss 0.473647, d_loss 0.392463 accuracy 0.515625\n",
      "Training in progress @ global_step 600, g_loss 0.473707, d_loss 0.378998 accuracy 0.5\n",
      "Training in progress @ global_step 650, g_loss 0.473667, d_loss 0.378366 accuracy 0.515625\n",
      "Training in progress @ global_step 700, g_loss 0.473647, d_loss 0.369057 accuracy 0.5\n",
      "Training in progress @ global_step 750, g_loss 0.473589, d_loss 0.373623 accuracy 0.5\n",
      "Training in progress @ global_step 800, g_loss 0.473633, d_loss 0.365259 accuracy 0.515625\n",
      "Training in progress @ global_step 850, g_loss 0.473586, d_loss 0.362044 accuracy 0.5\n",
      "Training in progress @ global_step 900, g_loss 0.47347, d_loss 0.357117 accuracy 0.5\n",
      "Training in progress @ global_step 950, g_loss 0.473564, d_loss 0.35297 accuracy 0.5\n",
      "Training in progress @ global_step 1000, g_loss 0.473521, d_loss 0.350423 accuracy 0.515625\n",
      "Training in progress @ global_step 1050, g_loss 0.473435, d_loss 0.350794 accuracy 0.5\n",
      "Training in progress @ global_step 1100, g_loss 0.473432, d_loss 0.346128 accuracy 0.5\n",
      "Training in progress @ global_step 1150, g_loss 0.473391, d_loss 0.343409 accuracy 0.5\n",
      "Training in progress @ global_step 1200, g_loss 0.473386, d_loss 0.340966 accuracy 0.5\n",
      "Training in progress @ global_step 1250, g_loss 0.473368, d_loss 0.339082 accuracy 0.5\n",
      "Training in progress @ global_step 1300, g_loss 0.473369, d_loss 0.341297 accuracy 0.5\n",
      "Training in progress @ global_step 1350, g_loss 0.473341, d_loss 0.337302 accuracy 0.5\n",
      "Training in progress @ global_step 1400, g_loss 0.473241, d_loss 0.337882 accuracy 0.5\n",
      "Training in progress @ global_step 1450, g_loss 0.473168, d_loss 0.334596 accuracy 0.5\n",
      "Training in progress @ global_step 1500, g_loss 0.473238, d_loss 0.334285 accuracy 0.5\n",
      "Training in progress @ global_step 1550, g_loss 0.473208, d_loss 0.332159 accuracy 0.5\n",
      "Training in progress @ global_step 1600, g_loss 0.473143, d_loss 0.331792 accuracy 0.5\n",
      "Training in progress @ global_step 1650, g_loss 0.473086, d_loss 0.329472 accuracy 0.5\n",
      "Training in progress @ global_step 1700, g_loss 0.473091, d_loss 0.329786 accuracy 0.5\n",
      "Training in progress @ global_step 1750, g_loss 0.473012, d_loss 0.328842 accuracy 0.5\n",
      "Training in progress @ global_step 1800, g_loss 0.473139, d_loss 0.329091 accuracy 0.5\n",
      "Training in progress @ global_step 1850, g_loss 0.47299, d_loss 0.328479 accuracy 0.5\n",
      "Training in progress @ global_step 1900, g_loss 0.472952, d_loss 0.326567 accuracy 0.5\n",
      "Training in progress @ global_step 1950, g_loss 0.47295, d_loss 0.326447 accuracy 0.5\n",
      "Training in progress @ global_step 2000, g_loss 0.472802, d_loss 0.325859 accuracy 0.5\n",
      "Training in progress @ global_step 2050, g_loss 0.472838, d_loss 0.325015 accuracy 0.5\n",
      "Training in progress @ global_step 2100, g_loss 0.472881, d_loss 0.325341 accuracy 0.5\n",
      "Training in progress @ global_step 2150, g_loss 0.472742, d_loss 0.323668 accuracy 0.5\n",
      "Training in progress @ global_step 2200, g_loss 0.47271, d_loss 0.325135 accuracy 0.5\n",
      "Training in progress @ global_step 2250, g_loss 0.472676, d_loss 0.324401 accuracy 0.5\n",
      "Training in progress @ global_step 2300, g_loss 0.472564, d_loss 0.322669 accuracy 0.5\n",
      "Training in progress @ global_step 2350, g_loss 0.47263, d_loss 0.323223 accuracy 0.5\n",
      "Training in progress @ global_step 2400, g_loss 0.472428, d_loss 0.322961 accuracy 0.5\n",
      "Training in progress @ global_step 2450, g_loss 0.472438, d_loss 0.323542 accuracy 0.5\n",
      "Training in progress @ global_step 2500, g_loss 0.4724, d_loss 0.322644 accuracy 0.5\n",
      "Training in progress @ global_step 2550, g_loss 0.47244, d_loss 0.322361 accuracy 0.5\n",
      "Training in progress @ global_step 2600, g_loss 0.472231, d_loss 0.323175 accuracy 0.5\n",
      "Training in progress @ global_step 2650, g_loss 0.472212, d_loss 0.321985 accuracy 0.5\n",
      "Training in progress @ global_step 2700, g_loss 0.472229, d_loss 0.321686 accuracy 0.5\n",
      "Training in progress @ global_step 2750, g_loss 0.472176, d_loss 0.321529 accuracy 0.5\n",
      "Training in progress @ global_step 2800, g_loss 0.472056, d_loss 0.322261 accuracy 0.5\n",
      "Training in progress @ global_step 2850, g_loss 0.472124, d_loss 0.320768 accuracy 0.5\n",
      "Training in progress @ global_step 2900, g_loss 0.472033, d_loss 0.321354 accuracy 0.5\n",
      "Training in progress @ global_step 2950, g_loss 0.472025, d_loss 0.319476 accuracy 0.5\n",
      "Training in progress @ global_step 3000, g_loss 0.472051, d_loss 0.320678 accuracy 0.5\n",
      "Training in progress @ global_step 3050, g_loss 0.471868, d_loss 0.320752 accuracy 0.5\n",
      "Training in progress @ global_step 3100, g_loss 0.47175, d_loss 0.320714 accuracy 0.5\n",
      "Training in progress @ global_step 3150, g_loss 0.471758, d_loss 0.320562 accuracy 0.5\n",
      "Training in progress @ global_step 3200, g_loss 0.471847, d_loss 0.319811 accuracy 0.5\n",
      "Training in progress @ global_step 3250, g_loss 0.471764, d_loss 0.318824 accuracy 0.5\n",
      "Training in progress @ global_step 3300, g_loss 0.471896, d_loss 0.319142 accuracy 0.5\n",
      "Training in progress @ global_step 3350, g_loss 0.471648, d_loss 0.319595 accuracy 0.5\n",
      "Training in progress @ global_step 3400, g_loss 0.471632, d_loss 0.318946 accuracy 0.5\n",
      "Training in progress @ global_step 3450, g_loss 0.471548, d_loss 0.318556 accuracy 0.5\n",
      "Training in progress @ global_step 3500, g_loss 0.471548, d_loss 0.318375 accuracy 0.5\n",
      "Training in progress @ global_step 3550, g_loss 0.471804, d_loss 0.318874 accuracy 0.5\n",
      "Training in progress @ global_step 3600, g_loss 0.471662, d_loss 0.318169 accuracy 0.5\n",
      "Training in progress @ global_step 3650, g_loss 0.471574, d_loss 0.318171 accuracy 0.5\n",
      "Training in progress @ global_step 3700, g_loss 0.47146, d_loss 0.318228 accuracy 0.5\n",
      "Training in progress @ global_step 3750, g_loss 0.47143, d_loss 0.318774 accuracy 0.5\n",
      "Training in progress @ global_step 3800, g_loss 0.471301, d_loss 0.318826 accuracy 0.5\n",
      "Training in progress @ global_step 3850, g_loss 0.4714, d_loss 0.31836 accuracy 0.5\n",
      "Training in progress @ global_step 3900, g_loss 0.471285, d_loss 0.317926 accuracy 0.5\n",
      "Training in progress @ global_step 3950, g_loss 0.471453, d_loss 0.318111 accuracy 0.5\n",
      "Training in progress @ global_step 4000, g_loss 0.471675, d_loss 0.317748 accuracy 0.5\n",
      "Training in progress @ global_step 4050, g_loss 0.471424, d_loss 0.318116 accuracy 0.5\n",
      "Training in progress @ global_step 4100, g_loss 0.471583, d_loss 0.31841 accuracy 0.5\n",
      "Training in progress @ global_step 4150, g_loss 0.471547, d_loss 0.318594 accuracy 0.5\n",
      "Training in progress @ global_step 4200, g_loss 0.471609, d_loss 0.318175 accuracy 0.5\n",
      "Training in progress @ global_step 4250, g_loss 0.471605, d_loss 0.318775 accuracy 0.5\n",
      "Training in progress @ global_step 4300, g_loss 0.47143, d_loss 0.31873 accuracy 0.5\n",
      "Training in progress @ global_step 4350, g_loss 0.471542, d_loss 0.318295 accuracy 0.5\n",
      "Training in progress @ global_step 4400, g_loss 0.471494, d_loss 0.317221 accuracy 0.5\n",
      "Training in progress @ global_step 4450, g_loss 0.47151, d_loss 0.317836 accuracy 0.5\n",
      "Training in progress @ global_step 4500, g_loss 0.471458, d_loss 0.317275 accuracy 0.5\n",
      "Training in progress @ global_step 4550, g_loss 0.471586, d_loss 0.317974 accuracy 0.5\n",
      "Training in progress @ global_step 4600, g_loss 0.471504, d_loss 0.317662 accuracy 0.5\n",
      "Training in progress @ global_step 4650, g_loss 0.471725, d_loss 0.317649 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 4700, g_loss 0.471726, d_loss 0.317888 accuracy 0.5\n",
      "Training in progress @ global_step 4750, g_loss 0.471581, d_loss 0.317089 accuracy 0.5\n",
      "Training in progress @ global_step 4800, g_loss 0.471761, d_loss 0.317041 accuracy 0.5\n",
      "Training in progress @ global_step 4850, g_loss 0.471887, d_loss 0.317794 accuracy 0.5\n",
      "Training in progress @ global_step 4900, g_loss 0.471679, d_loss 0.317141 accuracy 0.5\n",
      "Training in progress @ global_step 4950, g_loss 0.471837, d_loss 0.317434 accuracy 0.5\n",
      "Training in progress @ global_step 5000, g_loss 0.472023, d_loss 0.318543 accuracy 0.5\n",
      "Training in progress @ global_step 5050, g_loss 0.471893, d_loss 0.317199 accuracy 0.5\n",
      "Training in progress @ global_step 5100, g_loss 0.471875, d_loss 0.31708 accuracy 0.5\n",
      "Training in progress @ global_step 5150, g_loss 0.472077, d_loss 0.317486 accuracy 0.5\n",
      "Training in progress @ global_step 5200, g_loss 0.471978, d_loss 0.317503 accuracy 0.5\n",
      "Training in progress @ global_step 5250, g_loss 0.47202, d_loss 0.317733 accuracy 0.5\n",
      "Training in progress @ global_step 5300, g_loss 0.472161, d_loss 0.318106 accuracy 0.5\n",
      "Training in progress @ global_step 5350, g_loss 0.472201, d_loss 0.317474 accuracy 0.5\n",
      "Training in progress @ global_step 5400, g_loss 0.472366, d_loss 0.317327 accuracy 0.5\n",
      "Training in progress @ global_step 5450, g_loss 0.472387, d_loss 0.317611 accuracy 0.5\n",
      "Training in progress @ global_step 5500, g_loss 0.472506, d_loss 0.317511 accuracy 0.5\n",
      "Training in progress @ global_step 5550, g_loss 0.472517, d_loss 0.31754 accuracy 0.5\n",
      "Training in progress @ global_step 5600, g_loss 0.472344, d_loss 0.317758 accuracy 0.5\n",
      "Training in progress @ global_step 5650, g_loss 0.472734, d_loss 0.317992 accuracy 0.5\n",
      "Training in progress @ global_step 5700, g_loss 0.472831, d_loss 0.317248 accuracy 0.5\n",
      "Training in progress @ global_step 5750, g_loss 0.47269, d_loss 0.318216 accuracy 0.5\n",
      "Training in progress @ global_step 5800, g_loss 0.47269, d_loss 0.317726 accuracy 0.5\n",
      "Training in progress @ global_step 5850, g_loss 0.472891, d_loss 0.317624 accuracy 0.515625\n",
      "Training in progress @ global_step 5900, g_loss 0.473047, d_loss 0.317685 accuracy 0.5\n",
      "Training in progress @ global_step 5950, g_loss 0.473157, d_loss 0.3172 accuracy 0.515625\n",
      "Training in progress @ global_step 6000, g_loss 0.473074, d_loss 0.318063 accuracy 0.5\n",
      "Training in progress @ global_step 6050, g_loss 0.473074, d_loss 0.317838 accuracy 0.515625\n",
      "Training in progress @ global_step 6100, g_loss 0.473401, d_loss 0.318192 accuracy 0.5625\n",
      "Training in progress @ global_step 6150, g_loss 0.473176, d_loss 0.317848 accuracy 0.515625\n",
      "Training in progress @ global_step 6200, g_loss 0.473119, d_loss 0.317581 accuracy 0.53125\n",
      "Training in progress @ global_step 6250, g_loss 0.473271, d_loss 0.318024 accuracy 0.515625\n",
      "Training in progress @ global_step 6300, g_loss 0.473407, d_loss 0.317897 accuracy 0.515625\n",
      "Training in progress @ global_step 6350, g_loss 0.473285, d_loss 0.318084 accuracy 0.515625\n",
      "Training in progress @ global_step 6400, g_loss 0.473562, d_loss 0.317761 accuracy 0.578125\n",
      "Training in progress @ global_step 6450, g_loss 0.473565, d_loss 0.31792 accuracy 0.59375\n",
      "Training in progress @ global_step 6500, g_loss 0.473612, d_loss 0.318424 accuracy 0.625\n",
      "Training in progress @ global_step 6550, g_loss 0.473726, d_loss 0.318144 accuracy 0.5625\n",
      "Training in progress @ global_step 6600, g_loss 0.47364, d_loss 0.318533 accuracy 0.59375\n",
      "Training in progress @ global_step 6650, g_loss 0.473782, d_loss 0.317994 accuracy 0.65625\n",
      "Training in progress @ global_step 6700, g_loss 0.473861, d_loss 0.318241 accuracy 0.671875\n",
      "Training in progress @ global_step 6750, g_loss 0.473722, d_loss 0.318425 accuracy 0.609375\n",
      "Training in progress @ global_step 6800, g_loss 0.473862, d_loss 0.318316 accuracy 0.71875\n",
      "Training in progress @ global_step 6850, g_loss 0.474025, d_loss 0.317903 accuracy 0.734375\n",
      "Training in progress @ global_step 6900, g_loss 0.47382, d_loss 0.317921 accuracy 0.625\n",
      "Training in progress @ global_step 6950, g_loss 0.474131, d_loss 0.317881 accuracy 0.78125\n",
      "Training in progress @ global_step 7000, g_loss 0.474228, d_loss 0.318065 accuracy 0.796875\n",
      "Training in progress @ global_step 7050, g_loss 0.473947, d_loss 0.318489 accuracy 0.71875\n",
      "Training in progress @ global_step 7100, g_loss 0.474175, d_loss 0.318074 accuracy 0.8125\n",
      "Training in progress @ global_step 7150, g_loss 0.474135, d_loss 0.318384 accuracy 0.78125\n",
      "Training in progress @ global_step 7200, g_loss 0.474122, d_loss 0.318268 accuracy 0.75\n",
      "Training in progress @ global_step 7250, g_loss 0.473993, d_loss 0.318349 accuracy 0.734375\n",
      "Training in progress @ global_step 7300, g_loss 0.473967, d_loss 0.318617 accuracy 0.703125\n",
      "Training in progress @ global_step 7350, g_loss 0.474122, d_loss 0.318114 accuracy 0.796875\n",
      "Training in progress @ global_step 7400, g_loss 0.474071, d_loss 0.318484 accuracy 0.734375\n",
      "Training in progress @ global_step 7450, g_loss 0.474187, d_loss 0.317999 accuracy 0.796875\n",
      "Training in progress @ global_step 7500, g_loss 0.474062, d_loss 0.318878 accuracy 0.765625\n",
      "Training in progress @ global_step 7550, g_loss 0.474033, d_loss 0.31833 accuracy 0.765625\n",
      "Training in progress @ global_step 7600, g_loss 0.474112, d_loss 0.318188 accuracy 0.78125\n",
      "Training in progress @ global_step 7650, g_loss 0.474307, d_loss 0.318824 accuracy 0.8125\n",
      "Training in progress @ global_step 7700, g_loss 0.47425, d_loss 0.317484 accuracy 0.859375\n",
      "Training in progress @ global_step 7750, g_loss 0.473991, d_loss 0.318614 accuracy 0.765625\n",
      "Training in progress @ global_step 7800, g_loss 0.474179, d_loss 0.318638 accuracy 0.765625\n",
      "Training in progress @ global_step 7850, g_loss 0.474233, d_loss 0.318137 accuracy 0.796875\n",
      "Training in progress @ global_step 7900, g_loss 0.474052, d_loss 0.31824 accuracy 0.71875\n",
      "Training in progress @ global_step 7950, g_loss 0.474049, d_loss 0.318426 accuracy 0.78125\n",
      "Training in progress @ global_step 8000, g_loss 0.474181, d_loss 0.318793 accuracy 0.765625\n",
      "Training in progress @ global_step 8050, g_loss 0.474123, d_loss 0.318912 accuracy 0.765625\n",
      "Training in progress @ global_step 8100, g_loss 0.474226, d_loss 0.318546 accuracy 0.8125\n",
      "Training in progress @ global_step 8150, g_loss 0.474165, d_loss 0.318582 accuracy 0.8125\n",
      "Training in progress @ global_step 8200, g_loss 0.474037, d_loss 0.318399 accuracy 0.734375\n",
      "Training in progress @ global_step 8250, g_loss 0.474093, d_loss 0.318788 accuracy 0.78125\n",
      "Training in progress @ global_step 8300, g_loss 0.474004, d_loss 0.319097 accuracy 0.734375\n",
      "Training in progress @ global_step 8350, g_loss 0.474003, d_loss 0.31879 accuracy 0.703125\n",
      "Training in progress @ global_step 8400, g_loss 0.474177, d_loss 0.318791 accuracy 0.78125\n",
      "Training in progress @ global_step 8450, g_loss 0.473977, d_loss 0.318565 accuracy 0.734375\n",
      "Training in progress @ global_step 8500, g_loss 0.474088, d_loss 0.318149 accuracy 0.765625\n",
      "Training in progress @ global_step 8550, g_loss 0.474154, d_loss 0.319592 accuracy 0.765625\n",
      "Training in progress @ global_step 8600, g_loss 0.473743, d_loss 0.318732 accuracy 0.65625\n",
      "Training in progress @ global_step 8650, g_loss 0.47387, d_loss 0.318735 accuracy 0.703125\n",
      "Training in progress @ global_step 8700, g_loss 0.473906, d_loss 0.31846 accuracy 0.671875\n",
      "Training in progress @ global_step 8750, g_loss 0.473691, d_loss 0.318484 accuracy 0.6875\n",
      "Training in progress @ global_step 8800, g_loss 0.4739, d_loss 0.319202 accuracy 0.734375\n",
      "Training in progress @ global_step 8850, g_loss 0.473576, d_loss 0.318886 accuracy 0.59375\n",
      "Training in progress @ global_step 8900, g_loss 0.473917, d_loss 0.319074 accuracy 0.71875\n",
      "Training in progress @ global_step 8950, g_loss 0.473792, d_loss 0.31894 accuracy 0.6875\n",
      "Training in progress @ global_step 9000, g_loss 0.473615, d_loss 0.319005 accuracy 0.59375\n",
      "Training in progress @ global_step 9050, g_loss 0.473454, d_loss 0.31909 accuracy 0.59375\n",
      "Training in progress @ global_step 9100, g_loss 0.473305, d_loss 0.318191 accuracy 0.546875\n",
      "Training in progress @ global_step 9150, g_loss 0.473612, d_loss 0.318533 accuracy 0.625\n",
      "Training in progress @ global_step 9200, g_loss 0.473375, d_loss 0.319176 accuracy 0.578125\n",
      "Training in progress @ global_step 9250, g_loss 0.473344, d_loss 0.319239 accuracy 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 9300, g_loss 0.472992, d_loss 0.318825 accuracy 0.546875\n",
      "Training in progress @ global_step 9350, g_loss 0.473149, d_loss 0.318372 accuracy 0.53125\n",
      "Training in progress @ global_step 9400, g_loss 0.473417, d_loss 0.319422 accuracy 0.625\n",
      "Training in progress @ global_step 9450, g_loss 0.473004, d_loss 0.319936 accuracy 0.53125\n",
      "Training in progress @ global_step 9500, g_loss 0.472659, d_loss 0.318699 accuracy 0.546875\n",
      "Training in progress @ global_step 9550, g_loss 0.473236, d_loss 0.318913 accuracy 0.546875\n",
      "Training in progress @ global_step 9600, g_loss 0.472767, d_loss 0.319384 accuracy 0.515625\n",
      "Training in progress @ global_step 9650, g_loss 0.472759, d_loss 0.319226 accuracy 0.5\n",
      "Training in progress @ global_step 9700, g_loss 0.472972, d_loss 0.319111 accuracy 0.546875\n",
      "Training in progress @ global_step 9750, g_loss 0.472843, d_loss 0.319181 accuracy 0.515625\n",
      "Training in progress @ global_step 9800, g_loss 0.472609, d_loss 0.319804 accuracy 0.5\n",
      "Training in progress @ global_step 9850, g_loss 0.472418, d_loss 0.319333 accuracy 0.5\n",
      "Training in progress @ global_step 9900, g_loss 0.472536, d_loss 0.318777 accuracy 0.5\n",
      "Training in progress @ global_step 9950, g_loss 0.472566, d_loss 0.319273 accuracy 0.5\n",
      "Training in progress @ global_step 10000, g_loss 0.472627, d_loss 0.319455 accuracy 0.5\n",
      "Training in progress @ global_step 10050, g_loss 0.472285, d_loss 0.319687 accuracy 0.5\n",
      "Training in progress @ global_step 10100, g_loss 0.472279, d_loss 0.319915 accuracy 0.5\n",
      "Training in progress @ global_step 10150, g_loss 0.472252, d_loss 0.319538 accuracy 0.5\n",
      "Training in progress @ global_step 10200, g_loss 0.472056, d_loss 0.319878 accuracy 0.515625\n",
      "Training in progress @ global_step 10250, g_loss 0.472156, d_loss 0.319799 accuracy 0.5\n",
      "Training in progress @ global_step 10300, g_loss 0.471804, d_loss 0.318722 accuracy 0.5\n",
      "Training in progress @ global_step 10350, g_loss 0.471627, d_loss 0.319525 accuracy 0.5\n",
      "Training in progress @ global_step 10400, g_loss 0.471586, d_loss 0.319966 accuracy 0.5\n",
      "Training in progress @ global_step 10450, g_loss 0.471714, d_loss 0.319646 accuracy 0.5\n",
      "Training in progress @ global_step 10500, g_loss 0.471658, d_loss 0.319554 accuracy 0.5\n",
      "Training in progress @ global_step 10550, g_loss 0.471182, d_loss 0.319918 accuracy 0.5\n",
      "Training in progress @ global_step 10600, g_loss 0.471353, d_loss 0.319387 accuracy 0.5\n",
      "Training in progress @ global_step 10650, g_loss 0.471089, d_loss 0.319225 accuracy 0.5\n",
      "Training in progress @ global_step 10700, g_loss 0.471479, d_loss 0.321827 accuracy 0.5\n",
      "Training in progress @ global_step 10750, g_loss 0.471281, d_loss 0.319659 accuracy 0.5\n",
      "Training in progress @ global_step 10800, g_loss 0.471441, d_loss 0.319949 accuracy 0.5\n",
      "Training in progress @ global_step 10850, g_loss 0.470889, d_loss 0.319083 accuracy 0.5\n",
      "Training in progress @ global_step 10900, g_loss 0.470965, d_loss 0.320305 accuracy 0.5\n",
      "Training in progress @ global_step 10950, g_loss 0.470827, d_loss 0.320061 accuracy 0.5\n",
      "Training in progress @ global_step 11000, g_loss 0.470791, d_loss 0.319917 accuracy 0.5\n",
      "Training in progress @ global_step 11050, g_loss 0.47051, d_loss 0.320294 accuracy 0.5\n",
      "Training in progress @ global_step 11100, g_loss 0.470392, d_loss 0.320217 accuracy 0.5\n",
      "Training in progress @ global_step 11150, g_loss 0.470514, d_loss 0.320135 accuracy 0.5\n",
      "Training in progress @ global_step 11200, g_loss 0.470475, d_loss 0.320356 accuracy 0.5\n",
      "Training in progress @ global_step 11250, g_loss 0.470103, d_loss 0.321072 accuracy 0.5\n",
      "Training in progress @ global_step 11300, g_loss 0.469833, d_loss 0.320969 accuracy 0.5\n",
      "Training in progress @ global_step 11350, g_loss 0.469991, d_loss 0.320319 accuracy 0.5\n",
      "Training in progress @ global_step 11400, g_loss 0.469865, d_loss 0.320598 accuracy 0.5\n",
      "Training in progress @ global_step 11450, g_loss 0.469314, d_loss 0.31975 accuracy 0.5\n",
      "Training in progress @ global_step 11500, g_loss 0.469661, d_loss 0.32001 accuracy 0.5\n",
      "Training in progress @ global_step 11550, g_loss 0.470051, d_loss 0.321288 accuracy 0.5\n",
      "Training in progress @ global_step 11600, g_loss 0.469385, d_loss 0.320349 accuracy 0.5\n",
      "Training in progress @ global_step 11650, g_loss 0.469649, d_loss 0.320351 accuracy 0.5\n",
      "Training in progress @ global_step 11700, g_loss 0.469485, d_loss 0.32065 accuracy 0.5\n",
      "Training in progress @ global_step 11750, g_loss 0.46911, d_loss 0.319847 accuracy 0.5\n",
      "Training in progress @ global_step 11800, g_loss 0.469105, d_loss 0.320442 accuracy 0.5\n",
      "Training in progress @ global_step 11850, g_loss 0.469013, d_loss 0.321029 accuracy 0.5\n",
      "Training in progress @ global_step 11900, g_loss 0.469557, d_loss 0.319977 accuracy 0.5\n",
      "Training in progress @ global_step 11950, g_loss 0.468553, d_loss 0.321206 accuracy 0.5\n",
      "Training in progress @ global_step 12000, g_loss 0.468873, d_loss 0.320864 accuracy 0.5\n",
      "Training in progress @ global_step 12050, g_loss 0.4689, d_loss 0.32077 accuracy 0.5\n",
      "Training in progress @ global_step 12100, g_loss 0.468858, d_loss 0.320371 accuracy 0.5\n",
      "Training in progress @ global_step 12150, g_loss 0.46793, d_loss 0.321499 accuracy 0.5\n",
      "Training in progress @ global_step 12200, g_loss 0.46851, d_loss 0.320552 accuracy 0.5\n",
      "Training in progress @ global_step 12250, g_loss 0.468375, d_loss 0.320676 accuracy 0.5\n",
      "Training in progress @ global_step 12300, g_loss 0.468037, d_loss 0.321737 accuracy 0.5\n",
      "Training in progress @ global_step 12350, g_loss 0.467951, d_loss 0.319975 accuracy 0.5\n",
      "Training in progress @ global_step 12400, g_loss 0.468271, d_loss 0.320824 accuracy 0.5\n",
      "Training in progress @ global_step 12450, g_loss 0.467302, d_loss 0.322756 accuracy 0.5\n",
      "Training in progress @ global_step 12500, g_loss 0.467801, d_loss 0.322124 accuracy 0.5\n",
      "Training in progress @ global_step 12550, g_loss 0.467777, d_loss 0.320777 accuracy 0.5\n",
      "Training in progress @ global_step 12600, g_loss 0.467835, d_loss 0.321604 accuracy 0.5\n",
      "Training in progress @ global_step 12650, g_loss 0.468302, d_loss 0.321246 accuracy 0.5\n",
      "Training in progress @ global_step 12700, g_loss 0.467507, d_loss 0.321595 accuracy 0.5\n",
      "Training in progress @ global_step 12750, g_loss 0.467051, d_loss 0.32092 accuracy 0.5\n",
      "Training in progress @ global_step 12800, g_loss 0.466832, d_loss 0.321804 accuracy 0.5\n",
      "Training in progress @ global_step 12850, g_loss 0.467305, d_loss 0.322107 accuracy 0.5\n",
      "Training in progress @ global_step 12900, g_loss 0.466925, d_loss 0.322741 accuracy 0.5\n",
      "Training in progress @ global_step 12950, g_loss 0.466639, d_loss 0.321432 accuracy 0.5\n",
      "Training in progress @ global_step 13000, g_loss 0.466874, d_loss 0.321398 accuracy 0.5\n",
      "Training in progress @ global_step 13050, g_loss 0.466385, d_loss 0.32263 accuracy 0.5\n",
      "Training in progress @ global_step 13100, g_loss 0.466543, d_loss 0.322072 accuracy 0.5\n",
      "Training in progress @ global_step 13150, g_loss 0.466464, d_loss 0.321733 accuracy 0.5\n",
      "Training in progress @ global_step 13200, g_loss 0.466288, d_loss 0.321761 accuracy 0.5\n",
      "Training in progress @ global_step 13250, g_loss 0.466232, d_loss 0.321378 accuracy 0.5\n",
      "Training in progress @ global_step 13300, g_loss 0.465927, d_loss 0.322447 accuracy 0.5\n",
      "Training in progress @ global_step 13350, g_loss 0.466223, d_loss 0.321608 accuracy 0.5\n",
      "Training in progress @ global_step 13400, g_loss 0.46595, d_loss 0.321718 accuracy 0.5\n",
      "Training in progress @ global_step 13450, g_loss 0.465974, d_loss 0.323209 accuracy 0.5\n",
      "Training in progress @ global_step 13500, g_loss 0.46592, d_loss 0.322119 accuracy 0.5\n",
      "Training in progress @ global_step 13550, g_loss 0.465961, d_loss 0.322164 accuracy 0.5\n",
      "Training in progress @ global_step 13600, g_loss 0.465013, d_loss 0.322192 accuracy 0.5\n",
      "Training in progress @ global_step 13650, g_loss 0.465399, d_loss 0.323809 accuracy 0.5\n",
      "Training in progress @ global_step 13700, g_loss 0.465587, d_loss 0.322818 accuracy 0.5\n",
      "Training in progress @ global_step 13750, g_loss 0.465449, d_loss 0.322323 accuracy 0.5\n",
      "Training in progress @ global_step 13800, g_loss 0.465555, d_loss 0.323102 accuracy 0.5\n",
      "Training in progress @ global_step 13850, g_loss 0.465604, d_loss 0.322091 accuracy 0.5\n",
      "Training in progress @ global_step 13900, g_loss 0.465108, d_loss 0.321948 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 13950, g_loss 0.465212, d_loss 0.322889 accuracy 0.5\n",
      "Training in progress @ global_step 14000, g_loss 0.464886, d_loss 0.322247 accuracy 0.5\n",
      "Training in progress @ global_step 14050, g_loss 0.465196, d_loss 0.323449 accuracy 0.5\n",
      "Training in progress @ global_step 14100, g_loss 0.464485, d_loss 0.323914 accuracy 0.5\n",
      "Training in progress @ global_step 14150, g_loss 0.465078, d_loss 0.322771 accuracy 0.5\n",
      "Training in progress @ global_step 14200, g_loss 0.464334, d_loss 0.32327 accuracy 0.5\n",
      "Training in progress @ global_step 14250, g_loss 0.464704, d_loss 0.32237 accuracy 0.5\n",
      "Training in progress @ global_step 14300, g_loss 0.464828, d_loss 0.322945 accuracy 0.5\n",
      "Training in progress @ global_step 14350, g_loss 0.46454, d_loss 0.323497 accuracy 0.5\n",
      "Training in progress @ global_step 14400, g_loss 0.464028, d_loss 0.322406 accuracy 0.5\n",
      "Training in progress @ global_step 14450, g_loss 0.464795, d_loss 0.323905 accuracy 0.5\n",
      "Training in progress @ global_step 14500, g_loss 0.464411, d_loss 0.322882 accuracy 0.5\n",
      "Training in progress @ global_step 14550, g_loss 0.464563, d_loss 0.323978 accuracy 0.5\n",
      "Training in progress @ global_step 14600, g_loss 0.464088, d_loss 0.324999 accuracy 0.5\n",
      "Training in progress @ global_step 14650, g_loss 0.463663, d_loss 0.324757 accuracy 0.5\n",
      "Training in progress @ global_step 14700, g_loss 0.463986, d_loss 0.32317 accuracy 0.5\n",
      "Training in progress @ global_step 14750, g_loss 0.464375, d_loss 0.32372 accuracy 0.5\n",
      "Training in progress @ global_step 14800, g_loss 0.464033, d_loss 0.324046 accuracy 0.5\n",
      "Training in progress @ global_step 14850, g_loss 0.464229, d_loss 0.322785 accuracy 0.5\n",
      "Training in progress @ global_step 14900, g_loss 0.464412, d_loss 0.323489 accuracy 0.5\n",
      "Training in progress @ global_step 14950, g_loss 0.463879, d_loss 0.324355 accuracy 0.5\n",
      "Training in progress @ global_step 15000, g_loss 0.464187, d_loss 0.324096 accuracy 0.5\n",
      "Training in progress @ global_step 15050, g_loss 0.463556, d_loss 0.324735 accuracy 0.5\n",
      "Training in progress @ global_step 15100, g_loss 0.464102, d_loss 0.324477 accuracy 0.5\n",
      "Training in progress @ global_step 15150, g_loss 0.463742, d_loss 0.323897 accuracy 0.5\n",
      "Training in progress @ global_step 15200, g_loss 0.46439, d_loss 0.325204 accuracy 0.5\n",
      "Training in progress @ global_step 15250, g_loss 0.464069, d_loss 0.324393 accuracy 0.5\n",
      "Training in progress @ global_step 15300, g_loss 0.463425, d_loss 0.323534 accuracy 0.5\n",
      "Training in progress @ global_step 15350, g_loss 0.464459, d_loss 0.323756 accuracy 0.5\n",
      "Training in progress @ global_step 15400, g_loss 0.463446, d_loss 0.32412 accuracy 0.5\n",
      "Training in progress @ global_step 15450, g_loss 0.463355, d_loss 0.32533 accuracy 0.5\n",
      "Training in progress @ global_step 15500, g_loss 0.463397, d_loss 0.323891 accuracy 0.5\n",
      "Training in progress @ global_step 15550, g_loss 0.463311, d_loss 0.324432 accuracy 0.5\n",
      "Training in progress @ global_step 15600, g_loss 0.463789, d_loss 0.323916 accuracy 0.5\n",
      "Training in progress @ global_step 15650, g_loss 0.46392, d_loss 0.325927 accuracy 0.5\n",
      "Training in progress @ global_step 15700, g_loss 0.46353, d_loss 0.324797 accuracy 0.5\n",
      "Training in progress @ global_step 15750, g_loss 0.463307, d_loss 0.32519 accuracy 0.5\n",
      "Training in progress @ global_step 15800, g_loss 0.463148, d_loss 0.324547 accuracy 0.5\n",
      "Training in progress @ global_step 15850, g_loss 0.463121, d_loss 0.324944 accuracy 0.5\n",
      "Training in progress @ global_step 15900, g_loss 0.463512, d_loss 0.325783 accuracy 0.5\n",
      "Training in progress @ global_step 15950, g_loss 0.463243, d_loss 0.325453 accuracy 0.5\n",
      "Training in progress @ global_step 16000, g_loss 0.463261, d_loss 0.324835 accuracy 0.5\n",
      "Training in progress @ global_step 16050, g_loss 0.463581, d_loss 0.325034 accuracy 0.5\n",
      "Training in progress @ global_step 16100, g_loss 0.462899, d_loss 0.32551 accuracy 0.5\n",
      "Training in progress @ global_step 16150, g_loss 0.462849, d_loss 0.325863 accuracy 0.5\n",
      "Training in progress @ global_step 16200, g_loss 0.462737, d_loss 0.324992 accuracy 0.5\n",
      "Training in progress @ global_step 16250, g_loss 0.462965, d_loss 0.325118 accuracy 0.5\n",
      "Training in progress @ global_step 16300, g_loss 0.462716, d_loss 0.325977 accuracy 0.5\n",
      "Training in progress @ global_step 16350, g_loss 0.462994, d_loss 0.326854 accuracy 0.5\n",
      "Training in progress @ global_step 16400, g_loss 0.462992, d_loss 0.325614 accuracy 0.5\n",
      "Training in progress @ global_step 16450, g_loss 0.462034, d_loss 0.326114 accuracy 0.5\n",
      "Training in progress @ global_step 16500, g_loss 0.463196, d_loss 0.325707 accuracy 0.5\n",
      "Training in progress @ global_step 16550, g_loss 0.462716, d_loss 0.326149 accuracy 0.5\n",
      "Training in progress @ global_step 16600, g_loss 0.463285, d_loss 0.326082 accuracy 0.5\n",
      "Training in progress @ global_step 16650, g_loss 0.462527, d_loss 0.326889 accuracy 0.5\n",
      "Training in progress @ global_step 16700, g_loss 0.462102, d_loss 0.326855 accuracy 0.5\n",
      "Training in progress @ global_step 16750, g_loss 0.462323, d_loss 0.326505 accuracy 0.5\n",
      "Training in progress @ global_step 16800, g_loss 0.462376, d_loss 0.32689 accuracy 0.5\n",
      "Training in progress @ global_step 16850, g_loss 0.462784, d_loss 0.32571 accuracy 0.5\n",
      "Training in progress @ global_step 16900, g_loss 0.462223, d_loss 0.326174 accuracy 0.5\n",
      "Training in progress @ global_step 16950, g_loss 0.462391, d_loss 0.327244 accuracy 0.5\n",
      "Training in progress @ global_step 17000, g_loss 0.462161, d_loss 0.32759 accuracy 0.5\n",
      "Training in progress @ global_step 17050, g_loss 0.462744, d_loss 0.326096 accuracy 0.5\n",
      "Training in progress @ global_step 17100, g_loss 0.462479, d_loss 0.326868 accuracy 0.5\n",
      "Training in progress @ global_step 17150, g_loss 0.462164, d_loss 0.327104 accuracy 0.5\n",
      "Training in progress @ global_step 17200, g_loss 0.462282, d_loss 0.326881 accuracy 0.5\n",
      "Training in progress @ global_step 17250, g_loss 0.461834, d_loss 0.326536 accuracy 0.5\n",
      "Training in progress @ global_step 17300, g_loss 0.461964, d_loss 0.326545 accuracy 0.5\n",
      "Training in progress @ global_step 17350, g_loss 0.46233, d_loss 0.327001 accuracy 0.5\n",
      "Training in progress @ global_step 17400, g_loss 0.462255, d_loss 0.328948 accuracy 0.5\n",
      "Training in progress @ global_step 17450, g_loss 0.462277, d_loss 0.328299 accuracy 0.5\n",
      "Training in progress @ global_step 17500, g_loss 0.462613, d_loss 0.328351 accuracy 0.5\n",
      "Training in progress @ global_step 17550, g_loss 0.461963, d_loss 0.328262 accuracy 0.5\n",
      "Training in progress @ global_step 17600, g_loss 0.461349, d_loss 0.327855 accuracy 0.5\n",
      "Training in progress @ global_step 17650, g_loss 0.461366, d_loss 0.328444 accuracy 0.5\n",
      "Training in progress @ global_step 17700, g_loss 0.461749, d_loss 0.327409 accuracy 0.5\n",
      "Training in progress @ global_step 17750, g_loss 0.461778, d_loss 0.328468 accuracy 0.5\n",
      "Training in progress @ global_step 17800, g_loss 0.46152, d_loss 0.328586 accuracy 0.5\n",
      "Training in progress @ global_step 17850, g_loss 0.460715, d_loss 0.328224 accuracy 0.5\n",
      "Training in progress @ global_step 17900, g_loss 0.46142, d_loss 0.32887 accuracy 0.5\n",
      "Training in progress @ global_step 17950, g_loss 0.461461, d_loss 0.328586 accuracy 0.5\n",
      "Training in progress @ global_step 18000, g_loss 0.461101, d_loss 0.328525 accuracy 0.5\n",
      "Training in progress @ global_step 18050, g_loss 0.460959, d_loss 0.326933 accuracy 0.5\n",
      "Training in progress @ global_step 18100, g_loss 0.461506, d_loss 0.329191 accuracy 0.5\n",
      "Training in progress @ global_step 18150, g_loss 0.461867, d_loss 0.328151 accuracy 0.5\n",
      "Training in progress @ global_step 18200, g_loss 0.461356, d_loss 0.328468 accuracy 0.5\n",
      "Training in progress @ global_step 18250, g_loss 0.460984, d_loss 0.328455 accuracy 0.5\n",
      "Training in progress @ global_step 18300, g_loss 0.460836, d_loss 0.329253 accuracy 0.5\n",
      "Training in progress @ global_step 18350, g_loss 0.460051, d_loss 0.329265 accuracy 0.5\n",
      "Training in progress @ global_step 18400, g_loss 0.459813, d_loss 0.328741 accuracy 0.5\n",
      "Training in progress @ global_step 18450, g_loss 0.460294, d_loss 0.329158 accuracy 0.5\n",
      "Training in progress @ global_step 18500, g_loss 0.460527, d_loss 0.329896 accuracy 0.5\n",
      "Training in progress @ global_step 18550, g_loss 0.460149, d_loss 0.329999 accuracy 0.5\n",
      "Training in progress @ global_step 18600, g_loss 0.461307, d_loss 0.330135 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 18650, g_loss 0.460807, d_loss 0.329826 accuracy 0.5\n",
      "Training in progress @ global_step 18700, g_loss 0.460185, d_loss 0.328816 accuracy 0.5\n",
      "Training in progress @ global_step 18750, g_loss 0.460347, d_loss 0.3285 accuracy 0.5\n",
      "Training in progress @ global_step 18800, g_loss 0.460069, d_loss 0.329702 accuracy 0.5\n",
      "Training in progress @ global_step 18850, g_loss 0.460364, d_loss 0.330372 accuracy 0.5\n",
      "Training in progress @ global_step 18900, g_loss 0.460647, d_loss 0.332217 accuracy 0.5\n",
      "Training in progress @ global_step 18950, g_loss 0.460572, d_loss 0.330694 accuracy 0.5\n",
      "Training in progress @ global_step 19000, g_loss 0.459681, d_loss 0.330113 accuracy 0.5\n",
      "Training in progress @ global_step 19050, g_loss 0.460147, d_loss 0.331225 accuracy 0.5\n",
      "Training in progress @ global_step 19100, g_loss 0.459493, d_loss 0.331264 accuracy 0.5\n",
      "Training in progress @ global_step 19150, g_loss 0.460121, d_loss 0.329348 accuracy 0.5\n",
      "Training in progress @ global_step 19200, g_loss 0.460452, d_loss 0.329238 accuracy 0.5\n",
      "Training in progress @ global_step 19250, g_loss 0.460154, d_loss 0.331155 accuracy 0.5\n",
      "Training in progress @ global_step 19300, g_loss 0.460346, d_loss 0.329734 accuracy 0.5\n",
      "Training in progress @ global_step 19350, g_loss 0.459837, d_loss 0.330564 accuracy 0.5\n",
      "Training in progress @ global_step 19400, g_loss 0.459504, d_loss 0.33129 accuracy 0.5\n",
      "Training in progress @ global_step 19450, g_loss 0.459553, d_loss 0.33068 accuracy 0.5\n",
      "Training in progress @ global_step 19500, g_loss 0.459139, d_loss 0.331048 accuracy 0.5\n",
      "Training in progress @ global_step 19550, g_loss 0.458144, d_loss 0.332492 accuracy 0.5\n",
      "Training in progress @ global_step 19600, g_loss 0.459344, d_loss 0.332644 accuracy 0.5\n",
      "Training in progress @ global_step 19650, g_loss 0.459769, d_loss 0.332487 accuracy 0.5\n",
      "Training in progress @ global_step 19700, g_loss 0.45936, d_loss 0.331562 accuracy 0.5\n",
      "Training in progress @ global_step 19750, g_loss 0.458915, d_loss 0.331895 accuracy 0.5\n",
      "Training in progress @ global_step 19800, g_loss 0.458689, d_loss 0.333542 accuracy 0.5\n",
      "Training in progress @ global_step 19850, g_loss 0.459533, d_loss 0.332806 accuracy 0.5\n",
      "Training in progress @ global_step 19900, g_loss 0.459255, d_loss 0.331848 accuracy 0.5\n",
      "Training in progress @ global_step 19950, g_loss 0.458872, d_loss 0.333666 accuracy 0.5\n",
      "Training in progress @ global_step 20000, g_loss 0.458233, d_loss 0.3329 accuracy 0.5\n",
      "Training in progress @ global_step 20050, g_loss 0.458673, d_loss 0.331753 accuracy 0.5\n",
      "Training in progress @ global_step 20100, g_loss 0.4587, d_loss 0.333547 accuracy 0.5\n",
      "Training in progress @ global_step 20150, g_loss 0.458812, d_loss 0.332887 accuracy 0.5\n",
      "Training in progress @ global_step 20200, g_loss 0.458858, d_loss 0.333253 accuracy 0.5\n",
      "Training in progress @ global_step 20250, g_loss 0.458402, d_loss 0.333497 accuracy 0.5\n",
      "Training in progress @ global_step 20300, g_loss 0.459305, d_loss 0.333453 accuracy 0.5\n",
      "Training in progress @ global_step 20350, g_loss 0.459062, d_loss 0.333331 accuracy 0.5\n",
      "Training in progress @ global_step 20400, g_loss 0.458408, d_loss 0.333386 accuracy 0.5\n",
      "Training in progress @ global_step 20450, g_loss 0.458703, d_loss 0.333586 accuracy 0.5\n",
      "Training in progress @ global_step 20500, g_loss 0.457914, d_loss 0.335528 accuracy 0.5\n",
      "Training in progress @ global_step 20550, g_loss 0.458013, d_loss 0.334188 accuracy 0.5\n",
      "Training in progress @ global_step 20600, g_loss 0.457919, d_loss 0.333731 accuracy 0.5\n",
      "Training in progress @ global_step 20650, g_loss 0.457994, d_loss 0.334749 accuracy 0.5\n",
      "Training in progress @ global_step 20700, g_loss 0.457414, d_loss 0.332855 accuracy 0.5\n",
      "Training in progress @ global_step 20750, g_loss 0.45745, d_loss 0.33303 accuracy 0.5\n",
      "Training in progress @ global_step 20800, g_loss 0.457369, d_loss 0.335536 accuracy 0.5\n",
      "Training in progress @ global_step 20850, g_loss 0.4571, d_loss 0.33295 accuracy 0.5\n",
      "Training in progress @ global_step 20900, g_loss 0.458218, d_loss 0.335874 accuracy 0.5\n",
      "Training in progress @ global_step 20950, g_loss 0.457083, d_loss 0.334672 accuracy 0.5\n",
      "Training in progress @ global_step 21000, g_loss 0.457041, d_loss 0.335826 accuracy 0.5\n",
      "Training in progress @ global_step 21050, g_loss 0.457468, d_loss 0.33536 accuracy 0.5\n",
      "Training in progress @ global_step 21100, g_loss 0.457178, d_loss 0.333429 accuracy 0.5\n",
      "Training in progress @ global_step 21150, g_loss 0.457012, d_loss 0.336083 accuracy 0.5\n",
      "Training in progress @ global_step 21200, g_loss 0.456476, d_loss 0.337262 accuracy 0.5\n",
      "Training in progress @ global_step 21250, g_loss 0.456922, d_loss 0.335729 accuracy 0.5\n",
      "Training in progress @ global_step 21300, g_loss 0.45734, d_loss 0.335571 accuracy 0.5\n",
      "Training in progress @ global_step 21350, g_loss 0.457864, d_loss 0.336195 accuracy 0.5\n",
      "Training in progress @ global_step 21400, g_loss 0.456948, d_loss 0.33698 accuracy 0.5\n",
      "Training in progress @ global_step 21450, g_loss 0.457323, d_loss 0.333657 accuracy 0.5\n",
      "Training in progress @ global_step 21500, g_loss 0.457728, d_loss 0.336361 accuracy 0.5\n",
      "Training in progress @ global_step 21550, g_loss 0.457048, d_loss 0.3359 accuracy 0.5\n",
      "Training in progress @ global_step 21600, g_loss 0.455955, d_loss 0.335801 accuracy 0.5\n",
      "Training in progress @ global_step 21650, g_loss 0.45693, d_loss 0.336667 accuracy 0.5\n",
      "Training in progress @ global_step 21700, g_loss 0.457388, d_loss 0.336283 accuracy 0.5\n",
      "Training in progress @ global_step 21750, g_loss 0.457168, d_loss 0.3375 accuracy 0.5\n",
      "Training in progress @ global_step 21800, g_loss 0.456249, d_loss 0.336216 accuracy 0.5\n",
      "Training in progress @ global_step 21850, g_loss 0.456322, d_loss 0.335804 accuracy 0.5\n",
      "Training in progress @ global_step 21900, g_loss 0.455309, d_loss 0.337085 accuracy 0.5\n",
      "Training in progress @ global_step 21950, g_loss 0.456885, d_loss 0.336726 accuracy 0.5\n",
      "Training in progress @ global_step 22000, g_loss 0.456483, d_loss 0.336422 accuracy 0.5\n",
      "Training in progress @ global_step 22050, g_loss 0.455884, d_loss 0.338347 accuracy 0.5\n",
      "Training in progress @ global_step 22100, g_loss 0.456235, d_loss 0.336645 accuracy 0.5\n",
      "Training in progress @ global_step 22150, g_loss 0.455849, d_loss 0.338749 accuracy 0.5\n",
      "Training in progress @ global_step 22200, g_loss 0.456247, d_loss 0.339113 accuracy 0.5\n",
      "Training in progress @ global_step 22250, g_loss 0.45638, d_loss 0.338073 accuracy 0.5\n",
      "Training in progress @ global_step 22300, g_loss 0.455716, d_loss 0.339738 accuracy 0.5\n",
      "Training in progress @ global_step 22350, g_loss 0.456249, d_loss 0.338712 accuracy 0.5\n",
      "Training in progress @ global_step 22400, g_loss 0.45607, d_loss 0.338882 accuracy 0.5\n",
      "Training in progress @ global_step 22450, g_loss 0.455986, d_loss 0.338749 accuracy 0.5\n",
      "Training in progress @ global_step 22500, g_loss 0.455704, d_loss 0.338226 accuracy 0.5\n",
      "Training in progress @ global_step 22550, g_loss 0.455609, d_loss 0.341017 accuracy 0.5\n",
      "Training in progress @ global_step 22600, g_loss 0.454482, d_loss 0.337987 accuracy 0.5\n",
      "Training in progress @ global_step 22650, g_loss 0.455809, d_loss 0.338246 accuracy 0.5\n",
      "Training in progress @ global_step 22700, g_loss 0.455173, d_loss 0.341069 accuracy 0.5\n",
      "Training in progress @ global_step 22750, g_loss 0.455522, d_loss 0.339043 accuracy 0.5\n",
      "Training in progress @ global_step 22800, g_loss 0.45574, d_loss 0.338976 accuracy 0.5\n",
      "Training in progress @ global_step 22850, g_loss 0.455101, d_loss 0.338659 accuracy 0.5\n",
      "Training in progress @ global_step 22900, g_loss 0.454604, d_loss 0.340685 accuracy 0.5\n",
      "Training in progress @ global_step 22950, g_loss 0.455602, d_loss 0.341679 accuracy 0.5\n",
      "Training in progress @ global_step 23000, g_loss 0.455031, d_loss 0.339825 accuracy 0.5\n",
      "Training in progress @ global_step 23050, g_loss 0.454486, d_loss 0.339389 accuracy 0.5\n",
      "Training in progress @ global_step 23100, g_loss 0.454549, d_loss 0.339716 accuracy 0.5\n",
      "Training in progress @ global_step 23150, g_loss 0.453331, d_loss 0.341558 accuracy 0.5\n",
      "Training in progress @ global_step 23200, g_loss 0.454765, d_loss 0.340809 accuracy 0.5\n",
      "Training in progress @ global_step 23250, g_loss 0.455141, d_loss 0.342782 accuracy 0.5\n",
      "Training in progress @ global_step 23300, g_loss 0.454293, d_loss 0.341955 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 23350, g_loss 0.453617, d_loss 0.341321 accuracy 0.5\n",
      "Training in progress @ global_step 23400, g_loss 0.453533, d_loss 0.340929 accuracy 0.5\n",
      "Training in progress @ global_step 23450, g_loss 0.453855, d_loss 0.340154 accuracy 0.5\n",
      "Training in progress @ global_step 23500, g_loss 0.454595, d_loss 0.342926 accuracy 0.5\n",
      "Training in progress @ global_step 23550, g_loss 0.454406, d_loss 0.341142 accuracy 0.5\n",
      "Training in progress @ global_step 23600, g_loss 0.454062, d_loss 0.342187 accuracy 0.5\n",
      "Training in progress @ global_step 23650, g_loss 0.454041, d_loss 0.342445 accuracy 0.5\n",
      "Training in progress @ global_step 23700, g_loss 0.454047, d_loss 0.342953 accuracy 0.5\n",
      "Training in progress @ global_step 23750, g_loss 0.454129, d_loss 0.341285 accuracy 0.5\n",
      "Training in progress @ global_step 23800, g_loss 0.453534, d_loss 0.342161 accuracy 0.5\n",
      "Training in progress @ global_step 23850, g_loss 0.453046, d_loss 0.342614 accuracy 0.5\n",
      "Training in progress @ global_step 23900, g_loss 0.454701, d_loss 0.342751 accuracy 0.5\n",
      "Training in progress @ global_step 23950, g_loss 0.453716, d_loss 0.343287 accuracy 0.5\n",
      "Training in progress @ global_step 24000, g_loss 0.453117, d_loss 0.342775 accuracy 0.5\n",
      "Training in progress @ global_step 24050, g_loss 0.453611, d_loss 0.345834 accuracy 0.5\n",
      "Training in progress @ global_step 24100, g_loss 0.45297, d_loss 0.343384 accuracy 0.5\n",
      "Training in progress @ global_step 24150, g_loss 0.454173, d_loss 0.343249 accuracy 0.5\n",
      "Training in progress @ global_step 24200, g_loss 0.453181, d_loss 0.345747 accuracy 0.5\n",
      "Training in progress @ global_step 24250, g_loss 0.453388, d_loss 0.344783 accuracy 0.5\n",
      "Training in progress @ global_step 24300, g_loss 0.453907, d_loss 0.34361 accuracy 0.5\n",
      "Training in progress @ global_step 24350, g_loss 0.452955, d_loss 0.344732 accuracy 0.5\n",
      "Training in progress @ global_step 24400, g_loss 0.453394, d_loss 0.345441 accuracy 0.5\n",
      "Training in progress @ global_step 24450, g_loss 0.453879, d_loss 0.344617 accuracy 0.5\n",
      "Training in progress @ global_step 24500, g_loss 0.452232, d_loss 0.34401 accuracy 0.5\n",
      "Training in progress @ global_step 24550, g_loss 0.454008, d_loss 0.344332 accuracy 0.5\n",
      "Training in progress @ global_step 24600, g_loss 0.453078, d_loss 0.346012 accuracy 0.5\n",
      "Training in progress @ global_step 24650, g_loss 0.453235, d_loss 0.344431 accuracy 0.5\n",
      "Training in progress @ global_step 24700, g_loss 0.453197, d_loss 0.345897 accuracy 0.5\n",
      "Training in progress @ global_step 24750, g_loss 0.453105, d_loss 0.347242 accuracy 0.5\n",
      "Training in progress @ global_step 24800, g_loss 0.452927, d_loss 0.348737 accuracy 0.5\n",
      "Training in progress @ global_step 24850, g_loss 0.45316, d_loss 0.346442 accuracy 0.5\n",
      "Training in progress @ global_step 24900, g_loss 0.452498, d_loss 0.34754 accuracy 0.5\n",
      "Training in progress @ global_step 24950, g_loss 0.452529, d_loss 0.346319 accuracy 0.5\n",
      "Training in progress @ global_step 25000, g_loss 0.453519, d_loss 0.349225 accuracy 0.5\n",
      "Training in progress @ global_step 25050, g_loss 0.453379, d_loss 0.346913 accuracy 0.5\n",
      "Training in progress @ global_step 25100, g_loss 0.452605, d_loss 0.349484 accuracy 0.5\n",
      "Training in progress @ global_step 25150, g_loss 0.453203, d_loss 0.346131 accuracy 0.5\n",
      "Training in progress @ global_step 25200, g_loss 0.45243, d_loss 0.34676 accuracy 0.5\n",
      "Training in progress @ global_step 25250, g_loss 0.452771, d_loss 0.347758 accuracy 0.5\n",
      "Training in progress @ global_step 25300, g_loss 0.452268, d_loss 0.347507 accuracy 0.5\n",
      "Training in progress @ global_step 25350, g_loss 0.451553, d_loss 0.348039 accuracy 0.5\n",
      "Training in progress @ global_step 25400, g_loss 0.452866, d_loss 0.347894 accuracy 0.5\n",
      "Training in progress @ global_step 25450, g_loss 0.452599, d_loss 0.348683 accuracy 0.5\n",
      "Training in progress @ global_step 25500, g_loss 0.452633, d_loss 0.34904 accuracy 0.5\n",
      "Training in progress @ global_step 25550, g_loss 0.452033, d_loss 0.350015 accuracy 0.5\n",
      "Training in progress @ global_step 25600, g_loss 0.452384, d_loss 0.348749 accuracy 0.5\n",
      "Training in progress @ global_step 25650, g_loss 0.450157, d_loss 0.348661 accuracy 0.5\n",
      "Training in progress @ global_step 25700, g_loss 0.45195, d_loss 0.350048 accuracy 0.5\n",
      "Training in progress @ global_step 25750, g_loss 0.450853, d_loss 0.350655 accuracy 0.5\n",
      "Training in progress @ global_step 25800, g_loss 0.4524, d_loss 0.350921 accuracy 0.5\n",
      "Training in progress @ global_step 25850, g_loss 0.451281, d_loss 0.348949 accuracy 0.5\n",
      "Training in progress @ global_step 25900, g_loss 0.451376, d_loss 0.349678 accuracy 0.5\n",
      "Training in progress @ global_step 25950, g_loss 0.451261, d_loss 0.350819 accuracy 0.5\n",
      "Training in progress @ global_step 26000, g_loss 0.452322, d_loss 0.350632 accuracy 0.5\n",
      "Training in progress @ global_step 26050, g_loss 0.450361, d_loss 0.352222 accuracy 0.5\n",
      "Training in progress @ global_step 26100, g_loss 0.451474, d_loss 0.351656 accuracy 0.5\n",
      "Training in progress @ global_step 26150, g_loss 0.451228, d_loss 0.351283 accuracy 0.5\n",
      "Training in progress @ global_step 26200, g_loss 0.451566, d_loss 0.351923 accuracy 0.5\n",
      "Training in progress @ global_step 26250, g_loss 0.451768, d_loss 0.351626 accuracy 0.5\n",
      "Training in progress @ global_step 26300, g_loss 0.450544, d_loss 0.352752 accuracy 0.5\n",
      "Training in progress @ global_step 26350, g_loss 0.451052, d_loss 0.352642 accuracy 0.5\n",
      "Training in progress @ global_step 26400, g_loss 0.451301, d_loss 0.350988 accuracy 0.5\n",
      "Training in progress @ global_step 26450, g_loss 0.451178, d_loss 0.352869 accuracy 0.5\n",
      "Training in progress @ global_step 26500, g_loss 0.450434, d_loss 0.354285 accuracy 0.5\n",
      "Training in progress @ global_step 26550, g_loss 0.451524, d_loss 0.353724 accuracy 0.5\n",
      "Training in progress @ global_step 26600, g_loss 0.45139, d_loss 0.354006 accuracy 0.5\n",
      "Training in progress @ global_step 26650, g_loss 0.451826, d_loss 0.35375 accuracy 0.5\n",
      "Training in progress @ global_step 26700, g_loss 0.450962, d_loss 0.353818 accuracy 0.5\n",
      "Training in progress @ global_step 26750, g_loss 0.451917, d_loss 0.354519 accuracy 0.5\n",
      "Training in progress @ global_step 26800, g_loss 0.450559, d_loss 0.355665 accuracy 0.5\n",
      "Training in progress @ global_step 26850, g_loss 0.449796, d_loss 0.354135 accuracy 0.5\n",
      "Training in progress @ global_step 26900, g_loss 0.451316, d_loss 0.35504 accuracy 0.5\n",
      "Training in progress @ global_step 26950, g_loss 0.450624, d_loss 0.356506 accuracy 0.5\n",
      "Training in progress @ global_step 27000, g_loss 0.451463, d_loss 0.355288 accuracy 0.5\n",
      "Training in progress @ global_step 27050, g_loss 0.451347, d_loss 0.3554 accuracy 0.5\n",
      "Training in progress @ global_step 27100, g_loss 0.452313, d_loss 0.354837 accuracy 0.5\n",
      "Training in progress @ global_step 27150, g_loss 0.450589, d_loss 0.355671 accuracy 0.5\n",
      "Training in progress @ global_step 27200, g_loss 0.45001, d_loss 0.356913 accuracy 0.5\n",
      "Training in progress @ global_step 27250, g_loss 0.450274, d_loss 0.357831 accuracy 0.5\n",
      "Training in progress @ global_step 27300, g_loss 0.450661, d_loss 0.357453 accuracy 0.5\n",
      "Training in progress @ global_step 27350, g_loss 0.450607, d_loss 0.355308 accuracy 0.5\n",
      "Training in progress @ global_step 27400, g_loss 0.452017, d_loss 0.356837 accuracy 0.5\n",
      "Training in progress @ global_step 27450, g_loss 0.449639, d_loss 0.356896 accuracy 0.5\n",
      "Training in progress @ global_step 27500, g_loss 0.449897, d_loss 0.358558 accuracy 0.5\n",
      "Training in progress @ global_step 27550, g_loss 0.450383, d_loss 0.357437 accuracy 0.5\n",
      "Training in progress @ global_step 27600, g_loss 0.449887, d_loss 0.358265 accuracy 0.5\n",
      "Training in progress @ global_step 27650, g_loss 0.449966, d_loss 0.358596 accuracy 0.5\n",
      "Training in progress @ global_step 27700, g_loss 0.450589, d_loss 0.35842 accuracy 0.5\n",
      "Training in progress @ global_step 27750, g_loss 0.450285, d_loss 0.35803 accuracy 0.5\n",
      "Training in progress @ global_step 27800, g_loss 0.449964, d_loss 0.358456 accuracy 0.5\n",
      "Training in progress @ global_step 27850, g_loss 0.450285, d_loss 0.359733 accuracy 0.5\n",
      "Training in progress @ global_step 27900, g_loss 0.450098, d_loss 0.359073 accuracy 0.5\n",
      "Training in progress @ global_step 27950, g_loss 0.450762, d_loss 0.358741 accuracy 0.5\n",
      "Training in progress @ global_step 28000, g_loss 0.44943, d_loss 0.358833 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 28050, g_loss 0.450065, d_loss 0.360911 accuracy 0.5\n",
      "Training in progress @ global_step 28100, g_loss 0.45064, d_loss 0.362326 accuracy 0.5\n",
      "Training in progress @ global_step 28150, g_loss 0.449214, d_loss 0.361487 accuracy 0.5\n",
      "Training in progress @ global_step 28200, g_loss 0.448558, d_loss 0.361758 accuracy 0.5\n",
      "Training in progress @ global_step 28250, g_loss 0.450557, d_loss 0.360929 accuracy 0.5\n",
      "Training in progress @ global_step 28300, g_loss 0.449272, d_loss 0.362253 accuracy 0.5\n",
      "Training in progress @ global_step 28350, g_loss 0.448418, d_loss 0.361731 accuracy 0.5\n",
      "Training in progress @ global_step 28400, g_loss 0.45061, d_loss 0.362108 accuracy 0.5\n",
      "Training in progress @ global_step 28450, g_loss 0.449694, d_loss 0.361935 accuracy 0.5\n",
      "Training in progress @ global_step 28500, g_loss 0.449697, d_loss 0.36194 accuracy 0.5\n",
      "Training in progress @ global_step 28550, g_loss 0.449502, d_loss 0.36202 accuracy 0.5\n",
      "Training in progress @ global_step 28600, g_loss 0.44928, d_loss 0.363227 accuracy 0.5\n",
      "Training in progress @ global_step 28650, g_loss 0.448687, d_loss 0.361914 accuracy 0.5\n",
      "Training in progress @ global_step 28700, g_loss 0.450093, d_loss 0.36404 accuracy 0.5\n",
      "Training in progress @ global_step 28750, g_loss 0.449583, d_loss 0.363463 accuracy 0.5\n",
      "Training in progress @ global_step 28800, g_loss 0.449591, d_loss 0.363225 accuracy 0.5\n",
      "Training in progress @ global_step 28850, g_loss 0.449175, d_loss 0.36301 accuracy 0.5\n",
      "Training in progress @ global_step 28900, g_loss 0.448908, d_loss 0.364787 accuracy 0.5\n",
      "Training in progress @ global_step 28950, g_loss 0.450039, d_loss 0.363412 accuracy 0.5\n",
      "Training in progress @ global_step 29000, g_loss 0.449625, d_loss 0.365245 accuracy 0.5\n",
      "Training in progress @ global_step 29050, g_loss 0.449721, d_loss 0.364678 accuracy 0.5\n",
      "Training in progress @ global_step 29100, g_loss 0.450761, d_loss 0.365626 accuracy 0.5\n",
      "Training in progress @ global_step 29150, g_loss 0.448521, d_loss 0.365021 accuracy 0.5\n",
      "Training in progress @ global_step 29200, g_loss 0.448294, d_loss 0.366667 accuracy 0.5\n",
      "Training in progress @ global_step 29250, g_loss 0.449724, d_loss 0.367302 accuracy 0.5\n",
      "Training in progress @ global_step 29300, g_loss 0.45059, d_loss 0.36813 accuracy 0.5\n",
      "Training in progress @ global_step 29350, g_loss 0.448377, d_loss 0.368843 accuracy 0.5\n",
      "Training in progress @ global_step 29400, g_loss 0.44891, d_loss 0.366888 accuracy 0.5\n",
      "Training in progress @ global_step 29450, g_loss 0.450064, d_loss 0.368037 accuracy 0.5\n",
      "Training in progress @ global_step 29500, g_loss 0.450197, d_loss 0.366005 accuracy 0.5\n",
      "Training in progress @ global_step 29550, g_loss 0.449119, d_loss 0.368867 accuracy 0.5\n",
      "Training in progress @ global_step 29600, g_loss 0.449105, d_loss 0.368071 accuracy 0.5\n",
      "Training in progress @ global_step 29650, g_loss 0.449797, d_loss 0.368763 accuracy 0.5\n",
      "Training in progress @ global_step 29700, g_loss 0.448744, d_loss 0.370861 accuracy 0.5\n",
      "Training in progress @ global_step 29750, g_loss 0.449646, d_loss 0.368578 accuracy 0.5\n",
      "Training in progress @ global_step 29800, g_loss 0.449466, d_loss 0.36935 accuracy 0.5\n",
      "Training in progress @ global_step 29850, g_loss 0.44958, d_loss 0.372549 accuracy 0.5\n",
      "Training in progress @ global_step 29900, g_loss 0.45016, d_loss 0.370729 accuracy 0.5\n",
      "Training in progress @ global_step 29950, g_loss 0.449933, d_loss 0.368793 accuracy 0.5\n",
      "Training in progress @ global_step 30000, g_loss 0.448969, d_loss 0.369284 accuracy 0.5\n",
      "Training in progress @ global_step 30050, g_loss 0.45006, d_loss 0.370019 accuracy 0.5\n",
      "Training in progress @ global_step 30100, g_loss 0.44803, d_loss 0.369373 accuracy 0.5\n",
      "Training in progress @ global_step 30150, g_loss 0.449436, d_loss 0.371962 accuracy 0.5\n",
      "Training in progress @ global_step 30200, g_loss 0.450572, d_loss 0.370573 accuracy 0.5\n",
      "Training in progress @ global_step 30250, g_loss 0.448626, d_loss 0.370858 accuracy 0.5\n",
      "Training in progress @ global_step 30300, g_loss 0.449556, d_loss 0.37206 accuracy 0.5\n",
      "Training in progress @ global_step 30350, g_loss 0.44801, d_loss 0.372765 accuracy 0.5\n",
      "Training in progress @ global_step 30400, g_loss 0.448406, d_loss 0.371962 accuracy 0.5\n",
      "Training in progress @ global_step 30450, g_loss 0.44892, d_loss 0.373578 accuracy 0.5\n",
      "Training in progress @ global_step 30500, g_loss 0.450101, d_loss 0.373461 accuracy 0.5\n",
      "Training in progress @ global_step 30550, g_loss 0.449181, d_loss 0.374424 accuracy 0.5\n",
      "Training in progress @ global_step 30600, g_loss 0.449836, d_loss 0.374656 accuracy 0.5\n",
      "Training in progress @ global_step 30650, g_loss 0.450936, d_loss 0.374442 accuracy 0.5\n",
      "Training in progress @ global_step 30700, g_loss 0.449877, d_loss 0.374974 accuracy 0.5\n",
      "Training in progress @ global_step 30750, g_loss 0.449494, d_loss 0.374579 accuracy 0.5\n",
      "Training in progress @ global_step 30800, g_loss 0.450291, d_loss 0.374593 accuracy 0.5\n",
      "Training in progress @ global_step 30850, g_loss 0.449917, d_loss 0.376109 accuracy 0.5\n",
      "Training in progress @ global_step 30900, g_loss 0.448836, d_loss 0.376001 accuracy 0.5\n",
      "Training in progress @ global_step 30950, g_loss 0.449337, d_loss 0.375851 accuracy 0.5\n",
      "Training in progress @ global_step 31000, g_loss 0.449271, d_loss 0.376798 accuracy 0.5\n",
      "Training in progress @ global_step 31050, g_loss 0.449395, d_loss 0.37476 accuracy 0.5\n",
      "Training in progress @ global_step 31100, g_loss 0.449912, d_loss 0.376217 accuracy 0.5\n",
      "Training in progress @ global_step 31150, g_loss 0.450717, d_loss 0.377225 accuracy 0.5\n",
      "Training in progress @ global_step 31200, g_loss 0.449451, d_loss 0.377264 accuracy 0.5\n",
      "Training in progress @ global_step 31250, g_loss 0.450225, d_loss 0.379153 accuracy 0.5\n",
      "Training in progress @ global_step 31300, g_loss 0.450893, d_loss 0.377537 accuracy 0.5\n",
      "Training in progress @ global_step 31350, g_loss 0.448911, d_loss 0.379191 accuracy 0.5\n",
      "Training in progress @ global_step 31400, g_loss 0.450672, d_loss 0.379326 accuracy 0.5\n",
      "Training in progress @ global_step 31450, g_loss 0.450263, d_loss 0.379051 accuracy 0.5\n",
      "Training in progress @ global_step 31500, g_loss 0.450435, d_loss 0.379819 accuracy 0.5\n",
      "Training in progress @ global_step 31550, g_loss 0.450802, d_loss 0.382295 accuracy 0.5\n",
      "Training in progress @ global_step 31600, g_loss 0.449957, d_loss 0.379147 accuracy 0.5\n",
      "Training in progress @ global_step 31650, g_loss 0.449616, d_loss 0.381952 accuracy 0.5\n",
      "Training in progress @ global_step 31700, g_loss 0.449712, d_loss 0.382187 accuracy 0.5\n",
      "Training in progress @ global_step 31750, g_loss 0.450292, d_loss 0.380438 accuracy 0.5\n",
      "Training in progress @ global_step 31800, g_loss 0.449628, d_loss 0.381601 accuracy 0.5\n",
      "Training in progress @ global_step 31850, g_loss 0.450468, d_loss 0.382212 accuracy 0.5\n",
      "Training in progress @ global_step 31900, g_loss 0.450599, d_loss 0.382934 accuracy 0.5\n",
      "Training in progress @ global_step 31950, g_loss 0.449802, d_loss 0.381597 accuracy 0.5\n",
      "Training in progress @ global_step 32000, g_loss 0.450787, d_loss 0.381526 accuracy 0.5\n",
      "Training in progress @ global_step 32050, g_loss 0.450616, d_loss 0.383762 accuracy 0.5\n",
      "Training in progress @ global_step 32100, g_loss 0.450075, d_loss 0.385226 accuracy 0.5\n",
      "Training in progress @ global_step 32150, g_loss 0.450516, d_loss 0.383461 accuracy 0.5\n",
      "Training in progress @ global_step 32200, g_loss 0.451285, d_loss 0.383307 accuracy 0.5\n",
      "Training in progress @ global_step 32250, g_loss 0.450297, d_loss 0.38337 accuracy 0.5\n",
      "Training in progress @ global_step 32300, g_loss 0.450376, d_loss 0.386149 accuracy 0.5\n",
      "Training in progress @ global_step 32350, g_loss 0.450353, d_loss 0.386388 accuracy 0.5\n",
      "Training in progress @ global_step 32400, g_loss 0.449684, d_loss 0.385153 accuracy 0.5\n",
      "Training in progress @ global_step 32450, g_loss 0.450776, d_loss 0.387429 accuracy 0.5\n",
      "Training in progress @ global_step 32500, g_loss 0.450946, d_loss 0.385787 accuracy 0.5\n",
      "Training in progress @ global_step 32550, g_loss 0.450276, d_loss 0.386324 accuracy 0.5\n",
      "Training in progress @ global_step 32600, g_loss 0.450933, d_loss 0.386509 accuracy 0.5\n",
      "Training in progress @ global_step 32650, g_loss 0.451276, d_loss 0.388857 accuracy 0.5\n",
      "Training in progress @ global_step 32700, g_loss 0.4502, d_loss 0.38819 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 32750, g_loss 0.450876, d_loss 0.387498 accuracy 0.5\n",
      "Training in progress @ global_step 32800, g_loss 0.450547, d_loss 0.387868 accuracy 0.5\n",
      "Training in progress @ global_step 32850, g_loss 0.450461, d_loss 0.389234 accuracy 0.5\n",
      "Training in progress @ global_step 32900, g_loss 0.450353, d_loss 0.388305 accuracy 0.5\n",
      "Training in progress @ global_step 32950, g_loss 0.451656, d_loss 0.389261 accuracy 0.5\n",
      "Training in progress @ global_step 33000, g_loss 0.45194, d_loss 0.39013 accuracy 0.5\n",
      "Training in progress @ global_step 33050, g_loss 0.451681, d_loss 0.389829 accuracy 0.5\n",
      "Training in progress @ global_step 33100, g_loss 0.450515, d_loss 0.389409 accuracy 0.5\n",
      "Training in progress @ global_step 33150, g_loss 0.4513, d_loss 0.390121 accuracy 0.5\n",
      "Training in progress @ global_step 33200, g_loss 0.451406, d_loss 0.39097 accuracy 0.5\n",
      "Training in progress @ global_step 33250, g_loss 0.451387, d_loss 0.393034 accuracy 0.5\n",
      "Training in progress @ global_step 33300, g_loss 0.450792, d_loss 0.39139 accuracy 0.5\n",
      "Training in progress @ global_step 33350, g_loss 0.45111, d_loss 0.391648 accuracy 0.5\n",
      "Training in progress @ global_step 33400, g_loss 0.450479, d_loss 0.393006 accuracy 0.5\n",
      "Training in progress @ global_step 33450, g_loss 0.451864, d_loss 0.393181 accuracy 0.5\n",
      "Training in progress @ global_step 33500, g_loss 0.452919, d_loss 0.393496 accuracy 0.5\n",
      "Training in progress @ global_step 33550, g_loss 0.45181, d_loss 0.394595 accuracy 0.5\n",
      "Training in progress @ global_step 33600, g_loss 0.451413, d_loss 0.394946 accuracy 0.5\n",
      "Training in progress @ global_step 33650, g_loss 0.452477, d_loss 0.395229 accuracy 0.5\n",
      "Training in progress @ global_step 33700, g_loss 0.453065, d_loss 0.395765 accuracy 0.5\n",
      "Training in progress @ global_step 33750, g_loss 0.452916, d_loss 0.395741 accuracy 0.5\n",
      "Training in progress @ global_step 33800, g_loss 0.451267, d_loss 0.394878 accuracy 0.5\n",
      "Training in progress @ global_step 33850, g_loss 0.452197, d_loss 0.396333 accuracy 0.5\n",
      "Training in progress @ global_step 33900, g_loss 0.452855, d_loss 0.396263 accuracy 0.5\n",
      "Training in progress @ global_step 33950, g_loss 0.452889, d_loss 0.396846 accuracy 0.5\n",
      "Training in progress @ global_step 34000, g_loss 0.45257, d_loss 0.396527 accuracy 0.5\n",
      "Training in progress @ global_step 34050, g_loss 0.451755, d_loss 0.396933 accuracy 0.5\n",
      "Training in progress @ global_step 34100, g_loss 0.452476, d_loss 0.396481 accuracy 0.5\n",
      "Training in progress @ global_step 34150, g_loss 0.453671, d_loss 0.399905 accuracy 0.5\n",
      "Training in progress @ global_step 34200, g_loss 0.452254, d_loss 0.398786 accuracy 0.5\n",
      "Training in progress @ global_step 34250, g_loss 0.453255, d_loss 0.399466 accuracy 0.5\n",
      "Training in progress @ global_step 34300, g_loss 0.45241, d_loss 0.398925 accuracy 0.5\n",
      "Training in progress @ global_step 34350, g_loss 0.45396, d_loss 0.398658 accuracy 0.5\n",
      "Training in progress @ global_step 34400, g_loss 0.453963, d_loss 0.400306 accuracy 0.5\n",
      "Training in progress @ global_step 34450, g_loss 0.454126, d_loss 0.398573 accuracy 0.5\n",
      "Training in progress @ global_step 34500, g_loss 0.454353, d_loss 0.400651 accuracy 0.5\n",
      "Training in progress @ global_step 34550, g_loss 0.45399, d_loss 0.40205 accuracy 0.5\n",
      "Training in progress @ global_step 34600, g_loss 0.453216, d_loss 0.401013 accuracy 0.5\n",
      "Training in progress @ global_step 34650, g_loss 0.453353, d_loss 0.401677 accuracy 0.5\n",
      "Training in progress @ global_step 34700, g_loss 0.453886, d_loss 0.403674 accuracy 0.5\n",
      "Training in progress @ global_step 34750, g_loss 0.453174, d_loss 0.401449 accuracy 0.5\n",
      "Training in progress @ global_step 34800, g_loss 0.454649, d_loss 0.403377 accuracy 0.5\n",
      "Training in progress @ global_step 34850, g_loss 0.454639, d_loss 0.404041 accuracy 0.5\n",
      "Training in progress @ global_step 34900, g_loss 0.454364, d_loss 0.402613 accuracy 0.5\n",
      "Training in progress @ global_step 34950, g_loss 0.455234, d_loss 0.403916 accuracy 0.5\n",
      "Training in progress @ global_step 35000, g_loss 0.455198, d_loss 0.404709 accuracy 0.5\n",
      "Training in progress @ global_step 35050, g_loss 0.455087, d_loss 0.403669 accuracy 0.5\n",
      "Training in progress @ global_step 35100, g_loss 0.455028, d_loss 0.405955 accuracy 0.5\n",
      "Training in progress @ global_step 35150, g_loss 0.453929, d_loss 0.406061 accuracy 0.5\n",
      "Training in progress @ global_step 35200, g_loss 0.454184, d_loss 0.40874 accuracy 0.5\n",
      "Training in progress @ global_step 35250, g_loss 0.455796, d_loss 0.407748 accuracy 0.5\n",
      "Training in progress @ global_step 35300, g_loss 0.455463, d_loss 0.406931 accuracy 0.5\n",
      "Training in progress @ global_step 35350, g_loss 0.455865, d_loss 0.408277 accuracy 0.5\n",
      "Training in progress @ global_step 35400, g_loss 0.456092, d_loss 0.409116 accuracy 0.5\n",
      "Training in progress @ global_step 35450, g_loss 0.455477, d_loss 0.40899 accuracy 0.5\n",
      "Training in progress @ global_step 35500, g_loss 0.456373, d_loss 0.409494 accuracy 0.5\n",
      "Training in progress @ global_step 35550, g_loss 0.455123, d_loss 0.410047 accuracy 0.5\n",
      "Training in progress @ global_step 35600, g_loss 0.456618, d_loss 0.410357 accuracy 0.5\n",
      "Training in progress @ global_step 35650, g_loss 0.456686, d_loss 0.41063 accuracy 0.5\n",
      "Training in progress @ global_step 35700, g_loss 0.45569, d_loss 0.409916 accuracy 0.5\n",
      "Training in progress @ global_step 35750, g_loss 0.455611, d_loss 0.409985 accuracy 0.5\n",
      "Training in progress @ global_step 35800, g_loss 0.456678, d_loss 0.411548 accuracy 0.5\n",
      "Training in progress @ global_step 35850, g_loss 0.456932, d_loss 0.411599 accuracy 0.5\n",
      "Training in progress @ global_step 35900, g_loss 0.456724, d_loss 0.412981 accuracy 0.5\n",
      "Training in progress @ global_step 35950, g_loss 0.458019, d_loss 0.412113 accuracy 0.5\n",
      "Training in progress @ global_step 36000, g_loss 0.458083, d_loss 0.414618 accuracy 0.5\n",
      "Training in progress @ global_step 36050, g_loss 0.457628, d_loss 0.413395 accuracy 0.5\n",
      "Training in progress @ global_step 36100, g_loss 0.457613, d_loss 0.414472 accuracy 0.5\n",
      "Training in progress @ global_step 36150, g_loss 0.457652, d_loss 0.414762 accuracy 0.5\n",
      "Training in progress @ global_step 36200, g_loss 0.45869, d_loss 0.414974 accuracy 0.5\n",
      "Training in progress @ global_step 36250, g_loss 0.458161, d_loss 0.414086 accuracy 0.5\n",
      "Training in progress @ global_step 36300, g_loss 0.458388, d_loss 0.417042 accuracy 0.5\n",
      "Training in progress @ global_step 36350, g_loss 0.457116, d_loss 0.416543 accuracy 0.5\n",
      "Training in progress @ global_step 36400, g_loss 0.45824, d_loss 0.417417 accuracy 0.5\n",
      "Training in progress @ global_step 36450, g_loss 0.458749, d_loss 0.417694 accuracy 0.5\n",
      "Training in progress @ global_step 36500, g_loss 0.458466, d_loss 0.418823 accuracy 0.5\n",
      "Training in progress @ global_step 36550, g_loss 0.459419, d_loss 0.418429 accuracy 0.5\n",
      "Training in progress @ global_step 36600, g_loss 0.458884, d_loss 0.417883 accuracy 0.5\n",
      "Training in progress @ global_step 36650, g_loss 0.458644, d_loss 0.418154 accuracy 0.5\n",
      "Training in progress @ global_step 36700, g_loss 0.458447, d_loss 0.419943 accuracy 0.5\n",
      "Training in progress @ global_step 36750, g_loss 0.460829, d_loss 0.420258 accuracy 0.5\n",
      "Training in progress @ global_step 36800, g_loss 0.459883, d_loss 0.420687 accuracy 0.5\n",
      "Training in progress @ global_step 36850, g_loss 0.458992, d_loss 0.420452 accuracy 0.5\n",
      "Training in progress @ global_step 36900, g_loss 0.459758, d_loss 0.420327 accuracy 0.5\n",
      "Training in progress @ global_step 36950, g_loss 0.459782, d_loss 0.422229 accuracy 0.5\n",
      "Training in progress @ global_step 37000, g_loss 0.460886, d_loss 0.42236 accuracy 0.5\n",
      "Training in progress @ global_step 37050, g_loss 0.461785, d_loss 0.425283 accuracy 0.5\n",
      "Training in progress @ global_step 37100, g_loss 0.461424, d_loss 0.42169 accuracy 0.5\n",
      "Training in progress @ global_step 37150, g_loss 0.461631, d_loss 0.424881 accuracy 0.5\n",
      "Training in progress @ global_step 37200, g_loss 0.461907, d_loss 0.42386 accuracy 0.5\n",
      "Training in progress @ global_step 37250, g_loss 0.462292, d_loss 0.425659 accuracy 0.5\n",
      "Training in progress @ global_step 37300, g_loss 0.462144, d_loss 0.426667 accuracy 0.5\n",
      "Training in progress @ global_step 37350, g_loss 0.46199, d_loss 0.426238 accuracy 0.5\n",
      "Training in progress @ global_step 37400, g_loss 0.462392, d_loss 0.427471 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 37450, g_loss 0.46173, d_loss 0.426558 accuracy 0.5\n",
      "Training in progress @ global_step 37500, g_loss 0.462997, d_loss 0.427473 accuracy 0.5\n",
      "Training in progress @ global_step 37550, g_loss 0.463477, d_loss 0.42913 accuracy 0.5\n",
      "Training in progress @ global_step 37600, g_loss 0.463157, d_loss 0.427389 accuracy 0.5\n",
      "Training in progress @ global_step 37650, g_loss 0.463636, d_loss 0.428417 accuracy 0.5\n",
      "Training in progress @ global_step 37700, g_loss 0.463212, d_loss 0.427937 accuracy 0.5\n",
      "Training in progress @ global_step 37750, g_loss 0.463584, d_loss 0.428765 accuracy 0.5\n",
      "Training in progress @ global_step 37800, g_loss 0.463513, d_loss 0.430782 accuracy 0.5\n",
      "Training in progress @ global_step 37850, g_loss 0.46322, d_loss 0.43036 accuracy 0.5\n",
      "Training in progress @ global_step 37900, g_loss 0.463562, d_loss 0.430725 accuracy 0.5\n",
      "Training in progress @ global_step 37950, g_loss 0.464458, d_loss 0.431623 accuracy 0.5\n",
      "Training in progress @ global_step 38000, g_loss 0.464198, d_loss 0.431778 accuracy 0.5\n",
      "Training in progress @ global_step 38050, g_loss 0.464535, d_loss 0.432951 accuracy 0.5\n",
      "Training in progress @ global_step 38100, g_loss 0.465044, d_loss 0.432443 accuracy 0.5\n",
      "Training in progress @ global_step 38150, g_loss 0.466177, d_loss 0.43524 accuracy 0.5\n",
      "Training in progress @ global_step 38200, g_loss 0.465328, d_loss 0.43403 accuracy 0.5\n",
      "Training in progress @ global_step 38250, g_loss 0.466232, d_loss 0.434817 accuracy 0.5\n",
      "Training in progress @ global_step 38300, g_loss 0.466449, d_loss 0.435513 accuracy 0.5\n",
      "Training in progress @ global_step 38350, g_loss 0.466619, d_loss 0.43619 accuracy 0.5\n",
      "Training in progress @ global_step 38400, g_loss 0.466883, d_loss 0.437713 accuracy 0.5\n",
      "Training in progress @ global_step 38450, g_loss 0.467245, d_loss 0.436708 accuracy 0.5\n",
      "Training in progress @ global_step 38500, g_loss 0.466983, d_loss 0.437096 accuracy 0.5\n",
      "Training in progress @ global_step 38550, g_loss 0.466714, d_loss 0.438104 accuracy 0.5\n",
      "Training in progress @ global_step 38600, g_loss 0.467594, d_loss 0.438703 accuracy 0.5\n",
      "Training in progress @ global_step 38650, g_loss 0.467548, d_loss 0.438972 accuracy 0.515625\n",
      "Training in progress @ global_step 38700, g_loss 0.468527, d_loss 0.440019 accuracy 0.5\n",
      "Training in progress @ global_step 38750, g_loss 0.467342, d_loss 0.440082 accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    sess = tf.InteractiveSession()\n",
    "    RESTORE=False\n",
    "    if not RESTORE:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer.add_graph(sess.graph)\n",
    "        saver = tf.train.Saver()\n",
    "    else: \n",
    "        latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "        print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "        \n",
    "        \n",
    "    print \"Begin training ...\"\n",
    "    # Run training loop\n",
    "    for i in xrange(50000):\n",
    "        step = sess.run(global_step)\n",
    "\n",
    "        # Receive data (this will hang if IO thread is still running = this\n",
    "        # will wait for thread to finish & receive data)\n",
    "        \n",
    "        sigma = max(1.0*(40000. - step) / (40000), 0.01)\n",
    "\n",
    "        # Update the generator:\n",
    "        # Prepare the input to the networks:\n",
    "        fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "        real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "        real_data = 2*(real_data - 0.5)\n",
    "        \n",
    "        real_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "        fake_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "        \n",
    "        \n",
    "        [  acc_fake, _ ] = sess.run(\n",
    "            [accuracy_fake, \n",
    "             generator_optimizer], \n",
    "            feed_dict = {noise_tensor: fake_input,\n",
    "                         real_flat : real_data,\n",
    "                         real_noise: real_noise_addition,\n",
    "                         fake_noise: fake_noise_addition})\n",
    "\n",
    "        # Update the discriminator:\n",
    "        # Prepare the input to the networks:\n",
    "        fake = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "        real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "        real_data = 2*(real_data - 0.5)\n",
    "        [generated_mnist, _] = sess.run([fake_images, \n",
    "                                        discriminator_optimizer], \n",
    "                                        feed_dict = {noise_tensor : fake_input,\n",
    "                                                     real_flat : real_data,\n",
    "                                                     real_noise: real_noise_addition,\n",
    "                                                     fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        [summary, g_l, acc_fake, d_l_r, acc] = sess.run(\n",
    "            [merged_summary, g_loss, accuracy_fake,\n",
    "             d_loss_real, total_accuracy],\n",
    "            feed_dict = {noise_tensor : fake,\n",
    "                         real_flat : real_data,\n",
    "                         real_noise: real_noise_addition,\n",
    "                         fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "        if step != 0 and step % 500 == 0:\n",
    "            saver.save(\n",
    "                sess,\n",
    "                LOGDIR+\"/checkpoints/save\",\n",
    "                global_step=step)\n",
    "\n",
    "\n",
    "        # train_writer.add_summary(summary, i)\n",
    "        # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "        if step % 50 == 0:\n",
    "            print 'Training in progress @ global_step %d, g_loss %g, d_loss %g accuracy %g' % (step, g_l, d_l_r, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
