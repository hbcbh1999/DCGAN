{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particle physics, Generative Adversarial Networks hold a lot of promise for studying data/MC differences.  In principle, the physics tools used for simulation of particle interaction data are quite good, but they are never perfect.  Additionally, they are always imperfect in difficult to model ways.  Many experiments spend a lot of time studying the differences between the output of their simulation and their real detector data, and a deep network that can learn these differences is really useful for making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll cover some basics of generative adversarial networks with very simple feed-forward neural networks (not even convolutional) as a demonstration of the basic techniques of GANs.  You can read the original paper on GANs here: https://arxiv.org/abs/1406.2661.  The basic idea is you train two networks to compete with each other.  The first (called the discriminator) makes a decision on whether or not the images it's looking at are real or fake.  The second (called the generator) tries to generate fake images from random noise to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the years since the original paper came out, GANs have grown increasinly more sophisticated and impressive, especially with the advent of the Deep Convolutional Generative Adversarial Network (DCGAN, original paper: https://arxiv.org/abs/1511.06434).  For this tutorial, we're going to eschew all of the recent advances to make a GAN that can generate artificial digits based on the mnist data set.  By stripping the networks down to their basics, it's easier to focus on the core aspects of the loss function and outputs, and to avoid the complications of training on GPUs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist data set is one of the most famous collections of labeled images that exists.  You can read all about it at the original website (http://yann.lecun.com/exdb/mnist/), but it has a few nice advantages that make it ideal for learning:\n",
    " * It's open, and has convient wrappers in many languages (we'll use tensorflows soon)\n",
    " * It's a large data set (60k training images) but each image is small (28x28)\n",
    " * The data has been preprocessed to center images of digits and make them easier to use\n",
    " \n",
    "Basically, you can focus on the fundamentals with this data set, which is why we'll use it here.  Some examples of loading it with tensorflow (more here: https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, you can specify which device to use.  The next cell will tell you what's available, and you can select from there.  By default, I select \"/gpu:0\" but you can change this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10429023499755835644\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11946606592\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15153145144965522907\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()\n",
    "default_device = \"/gpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = mnist.train.next_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've loaded the mnist data from the tensorflow helper class, and asked for the next batch of images and labels.  Let's view those to see what the data looks like before delving into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 784)\n",
      "mnist_labels.shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n",
    "print \"mnist_labels.shape: {}\".format(mnist_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, variables are arrays with the outermost dimension equal to 5.  The images, though, comes unpacked as a 1D array per image instead of a 2D array.  We can reshape this to what we're more familar with, since we know mnist images are 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images = numpy.reshape(mnist_images, (-1, 28, 28)) # -1 can be used as a placeholder for batch size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib gives a good interface for viewing these images in a notebook (or even in general, in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is labeled as 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJKCAYAAAA84QGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAAAAAAAAAACAteb/Ad7qdDjR0GZFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f669a1a2f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mnist_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "print \"This image is labeled as {}\".format(mnist_labels[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn like this, you can see each individual pixel clearly.  It's not a high resolution image, but you can clearly tell what the digit is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.000001\n",
    "BATCH_SIZE=64 # Keep this even\n",
    "LOGDIR=\"./mnist_gan_logs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real images and random noise, so make a placeholder for both.  Additionally, based on this blog post (http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/) I add random gaussian noise to the real and fake images as they are fed to the discriminator to help stabalize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        # Input noise to the generator:\n",
    "        noise_tensor = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 10*10], name=\"noise\")\n",
    "        fake_input   = tf.reshape(noise_tensor, (tf.shape(noise_tensor)[0], 10,10, 1))\n",
    "\n",
    "        # Placeholder for the discriminator input:\n",
    "        real_flat  = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 784], name='x')\n",
    "        # label_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, 1], name='labels')\n",
    "        real_images  = tf.reshape(real_flat, (tf.shape(real_flat)[0], 28, 28, 1))\n",
    "\n",
    "        # We augment the input to the discriminator with gaussian noise\n",
    "        # This makes it harder for the discriminator to do it's job, preventing\n",
    "        # it from always \"winning\" the GAN min/max contest\n",
    "        real_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"real_noise\")\n",
    "        fake_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"fake_noise\")\n",
    "\n",
    "        real_images = real_noise + real_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input tensors (noise_tensor, real_images) are shaped in the 'flattened' way: (N/2, 100) for noise, (N/2, 784) for real images.  This lets me input the mnist images directly to tensorflow, as well as the noise.  They are then reshaped to be like tensorflow images (Batch, H, W, Filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to build the discriminator, using fully connected networks.  Note that a convolutional layer with the stride equal to the image size *is* a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor, reuse,reg=0.1):\n",
    "    # Use scoping to keep the variables nicely organized in the graph.\n",
    "    # Scoping is good practice always, but it's *essential* here as we'll see later on\n",
    "    with tf.variable_scope(\"mnist_discriminator\", reuse=reuse):\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=128, #Connecting to 128 output neurons, each it's own filter\n",
    "                             kernel_size=[28,28], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        # Apply a non linearity:\n",
    "        x = tf.nn.relu(x)\n",
    "        # That maps to a hidden layer, shape is (B, 1, 1, 128), let's now map to a single output\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=1,         #Connecting to 1 output neuron\n",
    "                             kernel_size=[1,1], \n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Since we want to predict \"real\" or \"fake\", an output of 0 or 1 is desired.  sigmoid is perfect for this:\n",
    "        x = tf.nn.sigmoid(x, name=\"discriminator_sigmoid\")\n",
    "        #Reshape this to bring it down to just one output per image:\n",
    "        x = tf.reshape(x, (x.get_shape().as_list()[0],))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        real_image_logits = build_discriminator(real_images, reuse=False,reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to generate random images from noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator(input_tensor, reg=0.2):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_generator\"):\n",
    "        # Input is Bx10x10x1, let's use two hidden layers this time to upsample to 28x28:\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=1028,\n",
    "                             kernel_size=[10,10], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        # Another fully connected layer, no change to resolution yet:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=1028,\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Final layer, step up to \"full\" resolution and then reshape the tensor:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=784, #784 is a 28x28 image\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_3\")\n",
    "        \n",
    "        # Reshape to match mnist images:\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))\n",
    "        \n",
    "        # The final non linearity applied here is to map the images onto the [-1,1] range.\n",
    "        x = tf.nn.tanh(x, name=\"generator_tanh\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_images = build_generator(fake_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to run the discriminator on the fake images, so set that up too.  Since it trains on both real and fake images, set reuse=True here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_image_logits = build_discriminator(fake_images, reuse=True, reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the generator and discriminator seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    # Build the loss functions:\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "            #Discriminator loss on real images (classify as 1):\n",
    "            d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_image_logits,\n",
    "                labels = tf.ones_like(real_image_logits)))\n",
    "            #Discriminator loss on fake images (classify as 0):\n",
    "            d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "                labels = tf.zeros_like(fake_image_logits)))\n",
    "\n",
    "            # Total discriminator loss is the sum:\n",
    "            d_loss_total = d_loss_real + d_loss_fake\n",
    "\n",
    "            # This is the adverserial step: g_loss tries to optimize fake_logits to one,\n",
    "            # While d_loss_fake tries to optimize fake_logits to zero.\n",
    "            g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "                labels = tf.ones_like(fake_image_logits)))\n",
    "\n",
    "            # This code is useful if you'll use tensorboard to monitor training:\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Fake_Loss\", d_loss_fake)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss_total)\n",
    "            d_loss_summary = tf.summary.scalar(\"Generator_Loss\", g_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"accuracy\") as scope:\n",
    "            # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "            accuracy_real  = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_logits), \n",
    "                                                             tf.ones_like(real_image_logits)), \n",
    "                                                    tf.float32))\n",
    "            accuracy_fake  = tf.reduce_mean(tf.cast(tf.equal(tf.round(fake_image_logits), \n",
    "                                                             tf.zeros_like(fake_image_logits)), \n",
    "                                                    tf.float32))\n",
    "\n",
    "            total_accuracy = 0.5*(accuracy_fake +  accuracy_real)\n",
    "\n",
    "            # Again, useful for tensorboard:\n",
    "            acc_real_summary = tf.summary.scalar(\"Real_Accuracy\", accuracy_real)\n",
    "            acc_real_summary = tf.summary.scalar(\"Fake_Accuracy\", accuracy_fake)\n",
    "            acc_real_summary = tf.summary.scalar(\"Total_Accuracy\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            # Global steps are useful for restoring training:\n",
    "            global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            # Make sure the optimizers are only operating on their own variables:\n",
    "\n",
    "            all_variables      = tf.trainable_variables()\n",
    "            discriminator_vars = [v for v in all_variables if v.name.startswith('mnist_discriminator/')]\n",
    "            generator_vars     = [v for v in all_variables if v.name.startswith('mnist_generator/')]\n",
    "\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                d_loss_total, global_step=global_step, var_list=discriminator_vars)\n",
    "            generator_optimizer     = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                g_loss, global_step=global_step, var_list=generator_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        tf.summary.image('fake_images', fake_images, max_outputs=4)\n",
    "        tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Set up a saver:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ...\n",
      "Training in progress @ global_step 0, g_loss 0.474298, d_loss 0.488224 accuracy 0.546875\n",
      "Training in progress @ global_step 50, g_loss 0.47426, d_loss 0.476399 accuracy 0.671875\n",
      "Training in progress @ global_step 100, g_loss 0.474177, d_loss 0.466159 accuracy 0.78125\n",
      "Training in progress @ global_step 150, g_loss 0.474094, d_loss 0.456197 accuracy 0.703125\n",
      "Training in progress @ global_step 200, g_loss 0.47405, d_loss 0.452239 accuracy 0.71875\n",
      "Training in progress @ global_step 250, g_loss 0.473955, d_loss 0.444132 accuracy 0.5625\n",
      "Training in progress @ global_step 300, g_loss 0.473882, d_loss 0.434409 accuracy 0.53125\n",
      "Training in progress @ global_step 350, g_loss 0.473785, d_loss 0.427326 accuracy 0.515625\n",
      "Training in progress @ global_step 400, g_loss 0.473747, d_loss 0.420297 accuracy 0.515625\n",
      "Training in progress @ global_step 450, g_loss 0.473643, d_loss 0.416134 accuracy 0.5\n",
      "Training in progress @ global_step 500, g_loss 0.473587, d_loss 0.4077 accuracy 0.5\n",
      "Training in progress @ global_step 550, g_loss 0.47349, d_loss 0.402113 accuracy 0.5\n",
      "Training in progress @ global_step 600, g_loss 0.473358, d_loss 0.400362 accuracy 0.5\n",
      "Training in progress @ global_step 650, g_loss 0.473305, d_loss 0.390148 accuracy 0.5\n",
      "Training in progress @ global_step 700, g_loss 0.473119, d_loss 0.38637 accuracy 0.5\n",
      "Training in progress @ global_step 750, g_loss 0.472996, d_loss 0.379553 accuracy 0.5\n",
      "Training in progress @ global_step 800, g_loss 0.472947, d_loss 0.377133 accuracy 0.5\n",
      "Training in progress @ global_step 850, g_loss 0.472789, d_loss 0.373355 accuracy 0.5\n",
      "Training in progress @ global_step 900, g_loss 0.472671, d_loss 0.372355 accuracy 0.5\n",
      "Training in progress @ global_step 950, g_loss 0.472503, d_loss 0.365512 accuracy 0.5\n",
      "Training in progress @ global_step 1000, g_loss 0.472384, d_loss 0.360965 accuracy 0.5\n",
      "Training in progress @ global_step 1050, g_loss 0.472174, d_loss 0.35813 accuracy 0.5\n",
      "Training in progress @ global_step 1100, g_loss 0.472031, d_loss 0.357955 accuracy 0.5\n",
      "Training in progress @ global_step 1150, g_loss 0.471901, d_loss 0.355378 accuracy 0.5\n",
      "Training in progress @ global_step 1200, g_loss 0.471711, d_loss 0.353758 accuracy 0.5\n",
      "Training in progress @ global_step 1250, g_loss 0.471642, d_loss 0.349664 accuracy 0.5\n",
      "Training in progress @ global_step 1300, g_loss 0.471367, d_loss 0.349737 accuracy 0.5\n",
      "Training in progress @ global_step 1350, g_loss 0.471169, d_loss 0.346531 accuracy 0.5\n",
      "Training in progress @ global_step 1400, g_loss 0.470885, d_loss 0.344237 accuracy 0.5\n",
      "Training in progress @ global_step 1450, g_loss 0.470616, d_loss 0.342284 accuracy 0.5\n",
      "Training in progress @ global_step 1500, g_loss 0.470531, d_loss 0.346491 accuracy 0.5\n",
      "Training in progress @ global_step 1550, g_loss 0.470312, d_loss 0.34257 accuracy 0.5\n",
      "Training in progress @ global_step 1600, g_loss 0.47002, d_loss 0.33988 accuracy 0.5\n",
      "Training in progress @ global_step 1650, g_loss 0.469722, d_loss 0.339924 accuracy 0.5\n",
      "Training in progress @ global_step 1700, g_loss 0.469471, d_loss 0.337047 accuracy 0.5\n",
      "Training in progress @ global_step 1750, g_loss 0.469263, d_loss 0.338487 accuracy 0.5\n",
      "Training in progress @ global_step 1800, g_loss 0.469045, d_loss 0.335991 accuracy 0.5\n",
      "Training in progress @ global_step 1850, g_loss 0.468649, d_loss 0.336567 accuracy 0.5\n",
      "Training in progress @ global_step 1900, g_loss 0.468351, d_loss 0.336313 accuracy 0.5\n",
      "Training in progress @ global_step 1950, g_loss 0.468006, d_loss 0.334498 accuracy 0.5\n",
      "Training in progress @ global_step 2000, g_loss 0.467567, d_loss 0.335685 accuracy 0.5\n",
      "Training in progress @ global_step 2050, g_loss 0.467353, d_loss 0.3338 accuracy 0.5\n",
      "Training in progress @ global_step 2100, g_loss 0.466911, d_loss 0.33278 accuracy 0.5\n",
      "Training in progress @ global_step 2150, g_loss 0.466735, d_loss 0.330049 accuracy 0.5\n",
      "Training in progress @ global_step 2200, g_loss 0.466344, d_loss 0.331697 accuracy 0.5\n",
      "Training in progress @ global_step 2250, g_loss 0.465868, d_loss 0.330677 accuracy 0.5\n",
      "Training in progress @ global_step 2300, g_loss 0.465338, d_loss 0.328675 accuracy 0.5\n",
      "Training in progress @ global_step 2350, g_loss 0.465354, d_loss 0.330159 accuracy 0.5\n",
      "Training in progress @ global_step 2400, g_loss 0.464861, d_loss 0.328106 accuracy 0.5\n",
      "Training in progress @ global_step 2450, g_loss 0.46442, d_loss 0.32895 accuracy 0.5\n",
      "Training in progress @ global_step 2500, g_loss 0.463867, d_loss 0.327197 accuracy 0.5\n",
      "Training in progress @ global_step 2550, g_loss 0.463514, d_loss 0.326902 accuracy 0.5\n",
      "Training in progress @ global_step 2600, g_loss 0.463423, d_loss 0.328426 accuracy 0.5\n",
      "Training in progress @ global_step 2650, g_loss 0.462597, d_loss 0.327979 accuracy 0.5\n",
      "Training in progress @ global_step 2700, g_loss 0.462646, d_loss 0.325882 accuracy 0.5\n",
      "Training in progress @ global_step 2750, g_loss 0.46188, d_loss 0.327919 accuracy 0.5\n",
      "Training in progress @ global_step 2800, g_loss 0.461429, d_loss 0.32606 accuracy 0.5\n",
      "Training in progress @ global_step 2850, g_loss 0.460967, d_loss 0.325967 accuracy 0.5\n",
      "Training in progress @ global_step 2900, g_loss 0.460673, d_loss 0.326075 accuracy 0.5\n",
      "Training in progress @ global_step 2950, g_loss 0.460315, d_loss 0.325178 accuracy 0.5\n",
      "Training in progress @ global_step 3000, g_loss 0.459547, d_loss 0.326173 accuracy 0.5\n",
      "Training in progress @ global_step 3050, g_loss 0.45901, d_loss 0.32542 accuracy 0.5\n",
      "Training in progress @ global_step 3100, g_loss 0.458951, d_loss 0.324837 accuracy 0.5\n",
      "Training in progress @ global_step 3150, g_loss 0.457902, d_loss 0.325584 accuracy 0.5\n",
      "Training in progress @ global_step 3200, g_loss 0.457828, d_loss 0.326253 accuracy 0.5\n",
      "Training in progress @ global_step 3250, g_loss 0.457229, d_loss 0.325449 accuracy 0.5\n",
      "Training in progress @ global_step 3300, g_loss 0.456827, d_loss 0.32517 accuracy 0.5\n",
      "Training in progress @ global_step 3350, g_loss 0.45664, d_loss 0.32479 accuracy 0.5\n",
      "Training in progress @ global_step 3400, g_loss 0.456476, d_loss 0.323881 accuracy 0.5\n",
      "Training in progress @ global_step 3450, g_loss 0.455659, d_loss 0.325515 accuracy 0.5\n",
      "Training in progress @ global_step 3500, g_loss 0.455218, d_loss 0.323577 accuracy 0.5\n",
      "Training in progress @ global_step 3550, g_loss 0.45435, d_loss 0.324054 accuracy 0.5\n",
      "Training in progress @ global_step 3600, g_loss 0.454675, d_loss 0.324151 accuracy 0.5\n",
      "Training in progress @ global_step 3650, g_loss 0.453656, d_loss 0.32415 accuracy 0.5\n",
      "Training in progress @ global_step 3700, g_loss 0.45336, d_loss 0.326575 accuracy 0.5\n",
      "Training in progress @ global_step 3750, g_loss 0.453022, d_loss 0.324293 accuracy 0.5\n",
      "Training in progress @ global_step 3800, g_loss 0.452526, d_loss 0.323732 accuracy 0.5\n",
      "Training in progress @ global_step 3850, g_loss 0.452126, d_loss 0.324761 accuracy 0.5\n",
      "Training in progress @ global_step 3900, g_loss 0.451595, d_loss 0.324231 accuracy 0.5\n",
      "Training in progress @ global_step 3950, g_loss 0.451022, d_loss 0.32438 accuracy 0.5\n",
      "Training in progress @ global_step 4000, g_loss 0.451186, d_loss 0.324568 accuracy 0.5\n",
      "Training in progress @ global_step 4050, g_loss 0.45064, d_loss 0.325108 accuracy 0.5\n",
      "Training in progress @ global_step 4100, g_loss 0.450352, d_loss 0.324277 accuracy 0.5\n",
      "Training in progress @ global_step 4150, g_loss 0.449961, d_loss 0.326219 accuracy 0.5\n",
      "Training in progress @ global_step 4200, g_loss 0.449862, d_loss 0.324952 accuracy 0.5\n",
      "Training in progress @ global_step 4250, g_loss 0.449351, d_loss 0.326004 accuracy 0.5\n",
      "Training in progress @ global_step 4300, g_loss 0.449321, d_loss 0.32505 accuracy 0.5\n",
      "Training in progress @ global_step 4350, g_loss 0.448719, d_loss 0.325157 accuracy 0.5\n",
      "Training in progress @ global_step 4400, g_loss 0.448969, d_loss 0.326521 accuracy 0.5\n",
      "Training in progress @ global_step 4450, g_loss 0.448568, d_loss 0.327 accuracy 0.5\n",
      "Training in progress @ global_step 4500, g_loss 0.448131, d_loss 0.325478 accuracy 0.5\n",
      "Training in progress @ global_step 4550, g_loss 0.44833, d_loss 0.325818 accuracy 0.5\n",
      "Training in progress @ global_step 4600, g_loss 0.44784, d_loss 0.32796 accuracy 0.5\n",
      "Training in progress @ global_step 4650, g_loss 0.447256, d_loss 0.327311 accuracy 0.5\n",
      "Training in progress @ global_step 4700, g_loss 0.447556, d_loss 0.328765 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 4750, g_loss 0.44737, d_loss 0.329173 accuracy 0.5\n",
      "Training in progress @ global_step 4800, g_loss 0.447346, d_loss 0.328091 accuracy 0.5\n",
      "Training in progress @ global_step 4850, g_loss 0.447032, d_loss 0.328989 accuracy 0.5\n",
      "Training in progress @ global_step 4900, g_loss 0.446346, d_loss 0.328675 accuracy 0.5\n",
      "Training in progress @ global_step 4950, g_loss 0.447159, d_loss 0.329456 accuracy 0.5\n",
      "Training in progress @ global_step 5000, g_loss 0.447148, d_loss 0.3299 accuracy 0.5\n",
      "Training in progress @ global_step 5050, g_loss 0.446806, d_loss 0.330066 accuracy 0.5\n",
      "Training in progress @ global_step 5100, g_loss 0.44622, d_loss 0.330048 accuracy 0.5\n",
      "Training in progress @ global_step 5150, g_loss 0.446756, d_loss 0.330171 accuracy 0.5\n",
      "Training in progress @ global_step 5200, g_loss 0.446526, d_loss 0.331282 accuracy 0.5\n",
      "Training in progress @ global_step 5250, g_loss 0.446657, d_loss 0.332836 accuracy 0.5\n",
      "Training in progress @ global_step 5300, g_loss 0.445915, d_loss 0.332444 accuracy 0.5\n",
      "Training in progress @ global_step 5350, g_loss 0.445915, d_loss 0.331945 accuracy 0.5\n",
      "Training in progress @ global_step 5400, g_loss 0.446306, d_loss 0.333253 accuracy 0.5\n",
      "Training in progress @ global_step 5450, g_loss 0.446002, d_loss 0.332822 accuracy 0.5\n",
      "Training in progress @ global_step 5500, g_loss 0.446074, d_loss 0.332892 accuracy 0.5\n",
      "Training in progress @ global_step 5550, g_loss 0.446348, d_loss 0.334887 accuracy 0.5\n",
      "Training in progress @ global_step 5600, g_loss 0.445676, d_loss 0.334278 accuracy 0.5\n",
      "Training in progress @ global_step 5650, g_loss 0.445923, d_loss 0.334699 accuracy 0.5\n",
      "Training in progress @ global_step 5700, g_loss 0.446207, d_loss 0.335119 accuracy 0.5\n",
      "Training in progress @ global_step 5750, g_loss 0.445796, d_loss 0.335324 accuracy 0.5\n",
      "Training in progress @ global_step 5800, g_loss 0.445743, d_loss 0.33711 accuracy 0.5\n",
      "Training in progress @ global_step 5850, g_loss 0.445259, d_loss 0.337249 accuracy 0.5\n",
      "Training in progress @ global_step 5900, g_loss 0.445957, d_loss 0.336429 accuracy 0.5\n",
      "Training in progress @ global_step 5950, g_loss 0.446025, d_loss 0.338162 accuracy 0.5\n",
      "Training in progress @ global_step 6000, g_loss 0.445166, d_loss 0.338828 accuracy 0.5\n",
      "Training in progress @ global_step 6050, g_loss 0.4459, d_loss 0.338894 accuracy 0.5\n",
      "Training in progress @ global_step 6100, g_loss 0.445841, d_loss 0.340786 accuracy 0.5\n",
      "Training in progress @ global_step 6150, g_loss 0.445719, d_loss 0.337521 accuracy 0.5\n",
      "Training in progress @ global_step 6200, g_loss 0.445579, d_loss 0.340006 accuracy 0.5\n",
      "Training in progress @ global_step 6250, g_loss 0.444939, d_loss 0.341751 accuracy 0.5\n",
      "Training in progress @ global_step 6300, g_loss 0.444813, d_loss 0.342503 accuracy 0.5\n",
      "Training in progress @ global_step 6350, g_loss 0.445553, d_loss 0.341717 accuracy 0.5\n",
      "Training in progress @ global_step 6400, g_loss 0.445158, d_loss 0.342415 accuracy 0.5\n",
      "Training in progress @ global_step 6450, g_loss 0.444391, d_loss 0.343681 accuracy 0.5\n",
      "Training in progress @ global_step 6500, g_loss 0.444631, d_loss 0.342348 accuracy 0.5\n",
      "Training in progress @ global_step 6550, g_loss 0.445209, d_loss 0.344209 accuracy 0.5\n",
      "Training in progress @ global_step 6600, g_loss 0.44451, d_loss 0.34455 accuracy 0.5\n",
      "Training in progress @ global_step 6650, g_loss 0.444776, d_loss 0.344188 accuracy 0.5\n",
      "Training in progress @ global_step 6700, g_loss 0.444939, d_loss 0.344944 accuracy 0.5\n",
      "Training in progress @ global_step 6750, g_loss 0.444173, d_loss 0.346838 accuracy 0.5\n",
      "Training in progress @ global_step 6800, g_loss 0.444089, d_loss 0.345316 accuracy 0.5\n",
      "Training in progress @ global_step 6850, g_loss 0.444423, d_loss 0.345301 accuracy 0.5\n",
      "Training in progress @ global_step 6900, g_loss 0.444423, d_loss 0.346926 accuracy 0.5\n",
      "Training in progress @ global_step 6950, g_loss 0.443552, d_loss 0.347845 accuracy 0.5\n",
      "Training in progress @ global_step 7000, g_loss 0.44385, d_loss 0.34857 accuracy 0.5\n",
      "Training in progress @ global_step 7050, g_loss 0.443893, d_loss 0.348671 accuracy 0.5\n",
      "Training in progress @ global_step 7100, g_loss 0.443872, d_loss 0.3486 accuracy 0.5\n",
      "Training in progress @ global_step 7150, g_loss 0.443035, d_loss 0.350309 accuracy 0.5\n",
      "Training in progress @ global_step 7200, g_loss 0.44309, d_loss 0.348395 accuracy 0.5\n",
      "Training in progress @ global_step 7250, g_loss 0.442968, d_loss 0.351742 accuracy 0.5\n",
      "Training in progress @ global_step 7300, g_loss 0.442982, d_loss 0.349757 accuracy 0.5\n",
      "Training in progress @ global_step 7350, g_loss 0.443154, d_loss 0.35186 accuracy 0.5\n",
      "Training in progress @ global_step 7400, g_loss 0.443054, d_loss 0.350969 accuracy 0.5\n",
      "Training in progress @ global_step 7450, g_loss 0.442795, d_loss 0.350457 accuracy 0.5\n",
      "Training in progress @ global_step 7500, g_loss 0.442617, d_loss 0.352728 accuracy 0.5\n",
      "Training in progress @ global_step 7550, g_loss 0.442625, d_loss 0.354093 accuracy 0.5\n",
      "Training in progress @ global_step 7600, g_loss 0.441879, d_loss 0.353433 accuracy 0.5\n",
      "Training in progress @ global_step 7650, g_loss 0.442359, d_loss 0.352864 accuracy 0.5\n",
      "Training in progress @ global_step 7700, g_loss 0.441744, d_loss 0.355509 accuracy 0.5\n",
      "Training in progress @ global_step 7750, g_loss 0.441691, d_loss 0.354173 accuracy 0.5\n",
      "Training in progress @ global_step 7800, g_loss 0.441562, d_loss 0.356157 accuracy 0.5\n",
      "Training in progress @ global_step 7850, g_loss 0.441066, d_loss 0.356302 accuracy 0.5\n",
      "Training in progress @ global_step 7900, g_loss 0.44131, d_loss 0.354168 accuracy 0.5\n",
      "Training in progress @ global_step 7950, g_loss 0.441158, d_loss 0.355426 accuracy 0.5\n",
      "Training in progress @ global_step 8000, g_loss 0.441326, d_loss 0.358701 accuracy 0.5\n",
      "Training in progress @ global_step 8050, g_loss 0.441205, d_loss 0.357428 accuracy 0.5\n",
      "Training in progress @ global_step 8100, g_loss 0.441043, d_loss 0.356299 accuracy 0.5\n",
      "Training in progress @ global_step 8150, g_loss 0.440094, d_loss 0.359014 accuracy 0.5\n",
      "Training in progress @ global_step 8200, g_loss 0.440374, d_loss 0.356838 accuracy 0.5\n",
      "Training in progress @ global_step 8250, g_loss 0.44077, d_loss 0.360553 accuracy 0.5\n",
      "Training in progress @ global_step 8300, g_loss 0.439771, d_loss 0.360389 accuracy 0.5\n",
      "Training in progress @ global_step 8350, g_loss 0.43968, d_loss 0.360146 accuracy 0.5\n",
      "Training in progress @ global_step 8400, g_loss 0.439466, d_loss 0.363719 accuracy 0.5\n",
      "Training in progress @ global_step 8450, g_loss 0.440003, d_loss 0.362114 accuracy 0.5\n",
      "Training in progress @ global_step 8500, g_loss 0.439223, d_loss 0.361535 accuracy 0.5\n",
      "Training in progress @ global_step 8550, g_loss 0.439395, d_loss 0.363001 accuracy 0.5\n",
      "Training in progress @ global_step 8600, g_loss 0.439389, d_loss 0.361787 accuracy 0.5\n",
      "Training in progress @ global_step 8650, g_loss 0.439463, d_loss 0.361549 accuracy 0.5\n",
      "Training in progress @ global_step 8700, g_loss 0.438996, d_loss 0.365308 accuracy 0.5\n",
      "Training in progress @ global_step 8750, g_loss 0.438359, d_loss 0.364497 accuracy 0.5\n",
      "Training in progress @ global_step 8800, g_loss 0.438824, d_loss 0.365835 accuracy 0.5\n",
      "Training in progress @ global_step 8850, g_loss 0.438876, d_loss 0.366731 accuracy 0.5\n",
      "Training in progress @ global_step 8900, g_loss 0.439166, d_loss 0.366286 accuracy 0.5\n",
      "Training in progress @ global_step 8950, g_loss 0.438133, d_loss 0.3674 accuracy 0.5\n",
      "Training in progress @ global_step 9000, g_loss 0.438129, d_loss 0.365044 accuracy 0.5\n",
      "Training in progress @ global_step 9050, g_loss 0.437929, d_loss 0.370045 accuracy 0.5\n",
      "Training in progress @ global_step 9100, g_loss 0.437878, d_loss 0.369735 accuracy 0.5\n",
      "Training in progress @ global_step 9150, g_loss 0.438008, d_loss 0.370604 accuracy 0.5\n",
      "Training in progress @ global_step 9200, g_loss 0.438287, d_loss 0.369919 accuracy 0.5\n",
      "Training in progress @ global_step 9250, g_loss 0.437933, d_loss 0.370621 accuracy 0.5\n",
      "Training in progress @ global_step 9300, g_loss 0.438061, d_loss 0.370476 accuracy 0.5\n",
      "Training in progress @ global_step 9350, g_loss 0.438459, d_loss 0.372719 accuracy 0.5\n",
      "Training in progress @ global_step 9400, g_loss 0.438579, d_loss 0.371018 accuracy 0.5\n",
      "Training in progress @ global_step 9450, g_loss 0.437248, d_loss 0.372847 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 9500, g_loss 0.437231, d_loss 0.374288 accuracy 0.5\n",
      "Training in progress @ global_step 9550, g_loss 0.437729, d_loss 0.372731 accuracy 0.5\n",
      "Training in progress @ global_step 9600, g_loss 0.438187, d_loss 0.373945 accuracy 0.5\n",
      "Training in progress @ global_step 9650, g_loss 0.437455, d_loss 0.376205 accuracy 0.5\n",
      "Training in progress @ global_step 9700, g_loss 0.437903, d_loss 0.374761 accuracy 0.5\n",
      "Training in progress @ global_step 9750, g_loss 0.437591, d_loss 0.374469 accuracy 0.5\n",
      "Training in progress @ global_step 9800, g_loss 0.437648, d_loss 0.375422 accuracy 0.5\n",
      "Training in progress @ global_step 9850, g_loss 0.437401, d_loss 0.376595 accuracy 0.5\n",
      "Training in progress @ global_step 9900, g_loss 0.437417, d_loss 0.378406 accuracy 0.5\n",
      "Training in progress @ global_step 9950, g_loss 0.437133, d_loss 0.378212 accuracy 0.5\n",
      "Training in progress @ global_step 10000, g_loss 0.437205, d_loss 0.378601 accuracy 0.5\n",
      "Training in progress @ global_step 10050, g_loss 0.437503, d_loss 0.378958 accuracy 0.5\n",
      "Training in progress @ global_step 10100, g_loss 0.437947, d_loss 0.38059 accuracy 0.5\n",
      "Training in progress @ global_step 10150, g_loss 0.438271, d_loss 0.38019 accuracy 0.5\n",
      "Training in progress @ global_step 10200, g_loss 0.437443, d_loss 0.379646 accuracy 0.5\n",
      "Training in progress @ global_step 10250, g_loss 0.438013, d_loss 0.381491 accuracy 0.5\n",
      "Training in progress @ global_step 10300, g_loss 0.437252, d_loss 0.381343 accuracy 0.5\n",
      "Training in progress @ global_step 10350, g_loss 0.438034, d_loss 0.382527 accuracy 0.5\n",
      "Training in progress @ global_step 10400, g_loss 0.438056, d_loss 0.381962 accuracy 0.5\n",
      "Training in progress @ global_step 10450, g_loss 0.438324, d_loss 0.383752 accuracy 0.5\n",
      "Training in progress @ global_step 10500, g_loss 0.438308, d_loss 0.383157 accuracy 0.5\n",
      "Training in progress @ global_step 10550, g_loss 0.438778, d_loss 0.382988 accuracy 0.5\n",
      "Training in progress @ global_step 10600, g_loss 0.438658, d_loss 0.384868 accuracy 0.5\n",
      "Training in progress @ global_step 10650, g_loss 0.43878, d_loss 0.384848 accuracy 0.5\n",
      "Training in progress @ global_step 10700, g_loss 0.438393, d_loss 0.385397 accuracy 0.5\n",
      "Training in progress @ global_step 10750, g_loss 0.438802, d_loss 0.385824 accuracy 0.5\n",
      "Training in progress @ global_step 10800, g_loss 0.439525, d_loss 0.385828 accuracy 0.5\n",
      "Training in progress @ global_step 10850, g_loss 0.439402, d_loss 0.388952 accuracy 0.5\n",
      "Training in progress @ global_step 10900, g_loss 0.439441, d_loss 0.388247 accuracy 0.5\n",
      "Training in progress @ global_step 10950, g_loss 0.439582, d_loss 0.38786 accuracy 0.5\n",
      "Training in progress @ global_step 11000, g_loss 0.439591, d_loss 0.390622 accuracy 0.5\n",
      "Training in progress @ global_step 11050, g_loss 0.43914, d_loss 0.388364 accuracy 0.5\n",
      "Training in progress @ global_step 11100, g_loss 0.439697, d_loss 0.389715 accuracy 0.5\n",
      "Training in progress @ global_step 11150, g_loss 0.439266, d_loss 0.391496 accuracy 0.5\n",
      "Training in progress @ global_step 11200, g_loss 0.439744, d_loss 0.390774 accuracy 0.5\n",
      "Training in progress @ global_step 11250, g_loss 0.440063, d_loss 0.393118 accuracy 0.5\n",
      "Training in progress @ global_step 11300, g_loss 0.439376, d_loss 0.3922 accuracy 0.5\n",
      "Training in progress @ global_step 11350, g_loss 0.440303, d_loss 0.392873 accuracy 0.5\n",
      "Training in progress @ global_step 11400, g_loss 0.440153, d_loss 0.390331 accuracy 0.5\n",
      "Training in progress @ global_step 11450, g_loss 0.440926, d_loss 0.393248 accuracy 0.5\n",
      "Training in progress @ global_step 11500, g_loss 0.441004, d_loss 0.394024 accuracy 0.5\n",
      "Training in progress @ global_step 11550, g_loss 0.441191, d_loss 0.395245 accuracy 0.5\n",
      "Training in progress @ global_step 11600, g_loss 0.441388, d_loss 0.394615 accuracy 0.5\n",
      "Training in progress @ global_step 11650, g_loss 0.441789, d_loss 0.394182 accuracy 0.5\n",
      "Training in progress @ global_step 11700, g_loss 0.441585, d_loss 0.395385 accuracy 0.5\n",
      "Training in progress @ global_step 11750, g_loss 0.442055, d_loss 0.397403 accuracy 0.5\n",
      "Training in progress @ global_step 11800, g_loss 0.442423, d_loss 0.397553 accuracy 0.5\n",
      "Training in progress @ global_step 11850, g_loss 0.441637, d_loss 0.39613 accuracy 0.5\n",
      "Training in progress @ global_step 11900, g_loss 0.442087, d_loss 0.398749 accuracy 0.5\n",
      "Training in progress @ global_step 11950, g_loss 0.442491, d_loss 0.399551 accuracy 0.5\n",
      "Training in progress @ global_step 12000, g_loss 0.442315, d_loss 0.397965 accuracy 0.5\n",
      "Training in progress @ global_step 12050, g_loss 0.44284, d_loss 0.400478 accuracy 0.5\n",
      "Training in progress @ global_step 12100, g_loss 0.443206, d_loss 0.399494 accuracy 0.5\n",
      "Training in progress @ global_step 12150, g_loss 0.443254, d_loss 0.401115 accuracy 0.5\n",
      "Training in progress @ global_step 12200, g_loss 0.443457, d_loss 0.400803 accuracy 0.5\n",
      "Training in progress @ global_step 12250, g_loss 0.443982, d_loss 0.402509 accuracy 0.5\n",
      "Training in progress @ global_step 12300, g_loss 0.44396, d_loss 0.403408 accuracy 0.5\n",
      "Training in progress @ global_step 12350, g_loss 0.444439, d_loss 0.400568 accuracy 0.5\n",
      "Training in progress @ global_step 12400, g_loss 0.445012, d_loss 0.405044 accuracy 0.5\n",
      "Training in progress @ global_step 12450, g_loss 0.444473, d_loss 0.404859 accuracy 0.5\n",
      "Training in progress @ global_step 12500, g_loss 0.445236, d_loss 0.405502 accuracy 0.5\n",
      "Training in progress @ global_step 12550, g_loss 0.445844, d_loss 0.404249 accuracy 0.5\n",
      "Training in progress @ global_step 12600, g_loss 0.446176, d_loss 0.407844 accuracy 0.5\n",
      "Training in progress @ global_step 12650, g_loss 0.446397, d_loss 0.409446 accuracy 0.5\n",
      "Training in progress @ global_step 12700, g_loss 0.44682, d_loss 0.40869 accuracy 0.5\n",
      "Training in progress @ global_step 12750, g_loss 0.447025, d_loss 0.411058 accuracy 0.5\n",
      "Training in progress @ global_step 12800, g_loss 0.447487, d_loss 0.408651 accuracy 0.5\n",
      "Training in progress @ global_step 12850, g_loss 0.448515, d_loss 0.411497 accuracy 0.5\n",
      "Training in progress @ global_step 12900, g_loss 0.448316, d_loss 0.410197 accuracy 0.5\n",
      "Training in progress @ global_step 12950, g_loss 0.449263, d_loss 0.412889 accuracy 0.5\n",
      "Training in progress @ global_step 13000, g_loss 0.449471, d_loss 0.411976 accuracy 0.5\n",
      "Training in progress @ global_step 13050, g_loss 0.45019, d_loss 0.4137 accuracy 0.5\n",
      "Training in progress @ global_step 13100, g_loss 0.450122, d_loss 0.412626 accuracy 0.5\n",
      "Training in progress @ global_step 13150, g_loss 0.450415, d_loss 0.413378 accuracy 0.5\n",
      "Training in progress @ global_step 13200, g_loss 0.450714, d_loss 0.413755 accuracy 0.5\n",
      "Training in progress @ global_step 13250, g_loss 0.450503, d_loss 0.415081 accuracy 0.5\n",
      "Training in progress @ global_step 13300, g_loss 0.451048, d_loss 0.414822 accuracy 0.5\n",
      "Training in progress @ global_step 13350, g_loss 0.451368, d_loss 0.41702 accuracy 0.5\n",
      "Training in progress @ global_step 13400, g_loss 0.451938, d_loss 0.417043 accuracy 0.5\n",
      "Training in progress @ global_step 13450, g_loss 0.451507, d_loss 0.417975 accuracy 0.5\n",
      "Training in progress @ global_step 13500, g_loss 0.451449, d_loss 0.41815 accuracy 0.5\n",
      "Training in progress @ global_step 13550, g_loss 0.451891, d_loss 0.417636 accuracy 0.5\n",
      "Training in progress @ global_step 13600, g_loss 0.45159, d_loss 0.418803 accuracy 0.5\n",
      "Training in progress @ global_step 13650, g_loss 0.452216, d_loss 0.418484 accuracy 0.5\n",
      "Training in progress @ global_step 13700, g_loss 0.45225, d_loss 0.419055 accuracy 0.5\n",
      "Training in progress @ global_step 13750, g_loss 0.452307, d_loss 0.419867 accuracy 0.5\n",
      "Training in progress @ global_step 13800, g_loss 0.452441, d_loss 0.420887 accuracy 0.5\n",
      "Training in progress @ global_step 13850, g_loss 0.452701, d_loss 0.421074 accuracy 0.5\n",
      "Training in progress @ global_step 13900, g_loss 0.452854, d_loss 0.421237 accuracy 0.5\n",
      "Training in progress @ global_step 13950, g_loss 0.452977, d_loss 0.420679 accuracy 0.5\n",
      "Training in progress @ global_step 14000, g_loss 0.453556, d_loss 0.421995 accuracy 0.5\n",
      "Training in progress @ global_step 14050, g_loss 0.453638, d_loss 0.422144 accuracy 0.5\n",
      "Training in progress @ global_step 14100, g_loss 0.454124, d_loss 0.421543 accuracy 0.5\n",
      "Training in progress @ global_step 14150, g_loss 0.454378, d_loss 0.422977 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 14200, g_loss 0.454661, d_loss 0.424019 accuracy 0.5\n",
      "Training in progress @ global_step 14250, g_loss 0.455096, d_loss 0.423088 accuracy 0.5\n",
      "Training in progress @ global_step 14300, g_loss 0.455858, d_loss 0.426434 accuracy 0.5\n",
      "Training in progress @ global_step 14350, g_loss 0.456182, d_loss 0.42668 accuracy 0.5\n",
      "Training in progress @ global_step 14400, g_loss 0.456312, d_loss 0.42725 accuracy 0.5\n",
      "Training in progress @ global_step 14450, g_loss 0.456804, d_loss 0.426644 accuracy 0.5\n",
      "Training in progress @ global_step 14500, g_loss 0.456993, d_loss 0.426888 accuracy 0.5\n",
      "Training in progress @ global_step 14550, g_loss 0.457344, d_loss 0.426909 accuracy 0.5\n",
      "Training in progress @ global_step 14600, g_loss 0.45756, d_loss 0.427904 accuracy 0.5\n",
      "Training in progress @ global_step 14650, g_loss 0.457923, d_loss 0.429113 accuracy 0.5\n",
      "Training in progress @ global_step 14700, g_loss 0.458293, d_loss 0.429068 accuracy 0.5\n",
      "Training in progress @ global_step 14750, g_loss 0.458565, d_loss 0.430832 accuracy 0.5\n",
      "Training in progress @ global_step 14800, g_loss 0.45891, d_loss 0.430483 accuracy 0.5\n",
      "Training in progress @ global_step 14850, g_loss 0.458978, d_loss 0.429882 accuracy 0.5\n",
      "Training in progress @ global_step 14900, g_loss 0.459087, d_loss 0.432527 accuracy 0.5\n",
      "Training in progress @ global_step 14950, g_loss 0.459129, d_loss 0.432522 accuracy 0.5\n",
      "Training in progress @ global_step 15000, g_loss 0.459736, d_loss 0.433342 accuracy 0.5\n",
      "Training in progress @ global_step 15050, g_loss 0.460071, d_loss 0.433518 accuracy 0.5\n",
      "Training in progress @ global_step 15100, g_loss 0.460339, d_loss 0.433261 accuracy 0.5\n",
      "Training in progress @ global_step 15150, g_loss 0.460869, d_loss 0.433565 accuracy 0.5\n",
      "Training in progress @ global_step 15200, g_loss 0.461213, d_loss 0.43391 accuracy 0.5\n",
      "Training in progress @ global_step 15250, g_loss 0.461729, d_loss 0.435246 accuracy 0.5\n",
      "Training in progress @ global_step 15300, g_loss 0.461962, d_loss 0.435143 accuracy 0.5\n",
      "Training in progress @ global_step 15350, g_loss 0.46206, d_loss 0.43399 accuracy 0.5\n",
      "Training in progress @ global_step 15400, g_loss 0.462644, d_loss 0.437839 accuracy 0.5\n",
      "Training in progress @ global_step 15450, g_loss 0.462601, d_loss 0.43734 accuracy 0.5\n",
      "Training in progress @ global_step 15500, g_loss 0.463131, d_loss 0.436495 accuracy 0.5\n",
      "Training in progress @ global_step 15550, g_loss 0.463395, d_loss 0.43893 accuracy 0.5\n",
      "Training in progress @ global_step 15600, g_loss 0.463473, d_loss 0.437908 accuracy 0.5\n",
      "Training in progress @ global_step 15650, g_loss 0.463714, d_loss 0.437457 accuracy 0.5\n",
      "Training in progress @ global_step 15700, g_loss 0.463807, d_loss 0.437716 accuracy 0.5\n",
      "Training in progress @ global_step 15750, g_loss 0.463906, d_loss 0.438259 accuracy 0.5\n",
      "Training in progress @ global_step 15800, g_loss 0.463891, d_loss 0.438357 accuracy 0.5\n",
      "Training in progress @ global_step 15850, g_loss 0.464379, d_loss 0.438208 accuracy 0.5\n",
      "Training in progress @ global_step 15900, g_loss 0.464294, d_loss 0.439608 accuracy 0.5\n",
      "Training in progress @ global_step 15950, g_loss 0.464709, d_loss 0.439853 accuracy 0.5\n",
      "Training in progress @ global_step 16000, g_loss 0.46506, d_loss 0.439016 accuracy 0.5\n",
      "Training in progress @ global_step 16050, g_loss 0.465312, d_loss 0.439509 accuracy 0.5\n",
      "Training in progress @ global_step 16100, g_loss 0.46552, d_loss 0.441042 accuracy 0.5\n",
      "Training in progress @ global_step 16150, g_loss 0.465859, d_loss 0.441081 accuracy 0.5\n",
      "Training in progress @ global_step 16200, g_loss 0.466047, d_loss 0.44136 accuracy 0.5\n",
      "Training in progress @ global_step 16250, g_loss 0.466371, d_loss 0.442103 accuracy 0.5\n",
      "Training in progress @ global_step 16300, g_loss 0.466811, d_loss 0.441588 accuracy 0.5\n",
      "Training in progress @ global_step 16350, g_loss 0.467264, d_loss 0.44253 accuracy 0.5\n",
      "Training in progress @ global_step 16400, g_loss 0.467587, d_loss 0.443976 accuracy 0.5\n",
      "Training in progress @ global_step 16450, g_loss 0.467636, d_loss 0.44163 accuracy 0.5\n",
      "Training in progress @ global_step 16500, g_loss 0.46786, d_loss 0.443745 accuracy 0.5\n",
      "Training in progress @ global_step 16550, g_loss 0.468112, d_loss 0.443663 accuracy 0.5\n",
      "Training in progress @ global_step 16600, g_loss 0.468328, d_loss 0.443683 accuracy 0.5\n",
      "Training in progress @ global_step 16650, g_loss 0.468706, d_loss 0.445981 accuracy 0.5\n",
      "Training in progress @ global_step 16700, g_loss 0.468886, d_loss 0.44403 accuracy 0.5\n",
      "Training in progress @ global_step 16750, g_loss 0.469037, d_loss 0.445495 accuracy 0.5\n",
      "Training in progress @ global_step 16800, g_loss 0.46932, d_loss 0.446237 accuracy 0.5\n",
      "Training in progress @ global_step 16850, g_loss 0.469432, d_loss 0.444402 accuracy 0.5\n",
      "Training in progress @ global_step 16900, g_loss 0.469496, d_loss 0.446653 accuracy 0.5\n",
      "Training in progress @ global_step 16950, g_loss 0.469609, d_loss 0.445457 accuracy 0.5\n",
      "Training in progress @ global_step 17000, g_loss 0.469908, d_loss 0.445843 accuracy 0.5\n",
      "Training in progress @ global_step 17050, g_loss 0.469989, d_loss 0.44531 accuracy 0.5\n",
      "Training in progress @ global_step 17100, g_loss 0.470203, d_loss 0.446465 accuracy 0.5\n",
      "Training in progress @ global_step 17150, g_loss 0.470237, d_loss 0.446014 accuracy 0.5\n",
      "Training in progress @ global_step 17200, g_loss 0.470472, d_loss 0.446559 accuracy 0.5\n",
      "Training in progress @ global_step 17250, g_loss 0.470668, d_loss 0.446605 accuracy 0.5\n",
      "Training in progress @ global_step 17300, g_loss 0.470837, d_loss 0.446259 accuracy 0.5\n",
      "Training in progress @ global_step 17350, g_loss 0.471086, d_loss 0.447147 accuracy 0.5\n",
      "Training in progress @ global_step 17400, g_loss 0.471257, d_loss 0.445621 accuracy 0.5\n",
      "Training in progress @ global_step 17450, g_loss 0.471474, d_loss 0.448197 accuracy 0.5\n",
      "Training in progress @ global_step 17500, g_loss 0.471699, d_loss 0.447427 accuracy 0.5\n",
      "Training in progress @ global_step 17550, g_loss 0.471898, d_loss 0.447956 accuracy 0.5\n",
      "Training in progress @ global_step 17600, g_loss 0.472084, d_loss 0.449122 accuracy 0.5\n",
      "Training in progress @ global_step 17650, g_loss 0.472275, d_loss 0.448221 accuracy 0.5\n",
      "Training in progress @ global_step 17700, g_loss 0.472474, d_loss 0.449525 accuracy 0.5\n",
      "Training in progress @ global_step 17750, g_loss 0.472645, d_loss 0.450339 accuracy 0.5\n",
      "Training in progress @ global_step 17800, g_loss 0.472803, d_loss 0.450802 accuracy 0.5\n",
      "Training in progress @ global_step 17850, g_loss 0.473007, d_loss 0.447873 accuracy 0.5\n",
      "Training in progress @ global_step 17900, g_loss 0.473106, d_loss 0.450688 accuracy 0.5\n",
      "Training in progress @ global_step 17950, g_loss 0.473304, d_loss 0.448415 accuracy 0.5\n",
      "Training in progress @ global_step 18000, g_loss 0.473422, d_loss 0.450593 accuracy 0.5\n",
      "Training in progress @ global_step 18050, g_loss 0.473672, d_loss 0.449652 accuracy 0.5\n",
      "Training in progress @ global_step 18100, g_loss 0.473852, d_loss 0.451125 accuracy 0.546875\n",
      "Training in progress @ global_step 18150, g_loss 0.47398, d_loss 0.451178 accuracy 0.640625\n",
      "Training in progress @ global_step 18200, g_loss 0.474204, d_loss 0.450073 accuracy 0.9375\n",
      "Training in progress @ global_step 18250, g_loss 0.474448, d_loss 0.451182 accuracy 1\n",
      "Training in progress @ global_step 18300, g_loss 0.474687, d_loss 0.451908 accuracy 1\n",
      "Training in progress @ global_step 18350, g_loss 0.474965, d_loss 0.450565 accuracy 1\n",
      "Training in progress @ global_step 18400, g_loss 0.475337, d_loss 0.453933 accuracy 1\n",
      "Training in progress @ global_step 18450, g_loss 0.475471, d_loss 0.451063 accuracy 1\n",
      "Training in progress @ global_step 18500, g_loss 0.475504, d_loss 0.452238 accuracy 1\n",
      "Training in progress @ global_step 18550, g_loss 0.475625, d_loss 0.451411 accuracy 1\n",
      "Training in progress @ global_step 18600, g_loss 0.475674, d_loss 0.452931 accuracy 1\n",
      "Training in progress @ global_step 18650, g_loss 0.475788, d_loss 0.453103 accuracy 1\n",
      "Training in progress @ global_step 18700, g_loss 0.475895, d_loss 0.451598 accuracy 1\n",
      "Training in progress @ global_step 18750, g_loss 0.476042, d_loss 0.452878 accuracy 1\n",
      "Training in progress @ global_step 18800, g_loss 0.476228, d_loss 0.45242 accuracy 1\n",
      "Training in progress @ global_step 18850, g_loss 0.47637, d_loss 0.451823 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 18900, g_loss 0.476568, d_loss 0.453356 accuracy 1\n",
      "Training in progress @ global_step 18950, g_loss 0.476883, d_loss 0.452631 accuracy 1\n",
      "Training in progress @ global_step 19000, g_loss 0.47679, d_loss 0.453056 accuracy 1\n",
      "Training in progress @ global_step 19050, g_loss 0.476828, d_loss 0.451795 accuracy 1\n",
      "Training in progress @ global_step 19100, g_loss 0.476805, d_loss 0.454191 accuracy 1\n",
      "Training in progress @ global_step 19150, g_loss 0.476916, d_loss 0.453349 accuracy 1\n",
      "Training in progress @ global_step 19200, g_loss 0.476981, d_loss 0.453843 accuracy 1\n",
      "Training in progress @ global_step 19250, g_loss 0.477032, d_loss 0.455076 accuracy 1\n",
      "Training in progress @ global_step 19300, g_loss 0.477079, d_loss 0.453483 accuracy 1\n",
      "Training in progress @ global_step 19350, g_loss 0.477147, d_loss 0.453033 accuracy 1\n",
      "Training in progress @ global_step 19400, g_loss 0.477309, d_loss 0.454762 accuracy 1\n",
      "Training in progress @ global_step 19450, g_loss 0.477487, d_loss 0.452517 accuracy 1\n",
      "Training in progress @ global_step 19500, g_loss 0.477586, d_loss 0.451634 accuracy 1\n",
      "Training in progress @ global_step 19550, g_loss 0.477572, d_loss 0.451301 accuracy 1\n",
      "Training in progress @ global_step 19600, g_loss 0.477567, d_loss 0.454342 accuracy 1\n",
      "Training in progress @ global_step 19650, g_loss 0.477489, d_loss 0.450865 accuracy 1\n",
      "Training in progress @ global_step 19700, g_loss 0.47752, d_loss 0.454036 accuracy 1\n",
      "Training in progress @ global_step 19750, g_loss 0.477545, d_loss 0.450985 accuracy 1\n",
      "Training in progress @ global_step 19800, g_loss 0.477497, d_loss 0.452258 accuracy 1\n",
      "Training in progress @ global_step 19850, g_loss 0.477449, d_loss 0.451864 accuracy 1\n",
      "Training in progress @ global_step 19900, g_loss 0.47729, d_loss 0.452392 accuracy 1\n",
      "Training in progress @ global_step 19950, g_loss 0.477327, d_loss 0.452974 accuracy 1\n",
      "Training in progress @ global_step 20000, g_loss 0.477325, d_loss 0.452323 accuracy 1\n",
      "Training in progress @ global_step 20050, g_loss 0.477399, d_loss 0.45231 accuracy 1\n",
      "Training in progress @ global_step 20100, g_loss 0.477439, d_loss 0.453645 accuracy 1\n",
      "Training in progress @ global_step 20150, g_loss 0.477446, d_loss 0.45203 accuracy 1\n",
      "Training in progress @ global_step 20200, g_loss 0.477501, d_loss 0.451634 accuracy 1\n",
      "Training in progress @ global_step 20250, g_loss 0.477569, d_loss 0.4492 accuracy 1\n",
      "Training in progress @ global_step 20300, g_loss 0.477725, d_loss 0.449823 accuracy 1\n",
      "Training in progress @ global_step 20350, g_loss 0.477754, d_loss 0.45245 accuracy 1\n",
      "Training in progress @ global_step 20400, g_loss 0.477778, d_loss 0.453031 accuracy 1\n",
      "Training in progress @ global_step 20450, g_loss 0.477921, d_loss 0.452294 accuracy 1\n",
      "Training in progress @ global_step 20500, g_loss 0.477913, d_loss 0.450107 accuracy 1\n",
      "Training in progress @ global_step 20550, g_loss 0.477975, d_loss 0.450876 accuracy 1\n",
      "Training in progress @ global_step 20600, g_loss 0.478095, d_loss 0.45081 accuracy 1\n",
      "Training in progress @ global_step 20650, g_loss 0.478088, d_loss 0.448821 accuracy 1\n",
      "Training in progress @ global_step 20700, g_loss 0.478033, d_loss 0.450693 accuracy 1\n",
      "Training in progress @ global_step 20750, g_loss 0.478078, d_loss 0.451029 accuracy 1\n",
      "Training in progress @ global_step 20800, g_loss 0.478192, d_loss 0.450414 accuracy 1\n",
      "Training in progress @ global_step 20850, g_loss 0.478324, d_loss 0.448705 accuracy 1\n",
      "Training in progress @ global_step 20900, g_loss 0.478387, d_loss 0.452979 accuracy 1\n",
      "Training in progress @ global_step 20950, g_loss 0.47838, d_loss 0.452394 accuracy 1\n",
      "Training in progress @ global_step 21000, g_loss 0.478527, d_loss 0.448109 accuracy 1\n",
      "Training in progress @ global_step 21050, g_loss 0.478531, d_loss 0.449937 accuracy 1\n",
      "Training in progress @ global_step 21100, g_loss 0.47853, d_loss 0.449468 accuracy 1\n",
      "Training in progress @ global_step 21150, g_loss 0.478639, d_loss 0.450199 accuracy 1\n",
      "Training in progress @ global_step 21200, g_loss 0.478612, d_loss 0.450322 accuracy 1\n",
      "Training in progress @ global_step 21250, g_loss 0.47878, d_loss 0.451372 accuracy 1\n",
      "Training in progress @ global_step 21300, g_loss 0.478934, d_loss 0.450609 accuracy 1\n",
      "Training in progress @ global_step 21350, g_loss 0.479062, d_loss 0.452853 accuracy 1\n",
      "Training in progress @ global_step 21400, g_loss 0.479091, d_loss 0.450457 accuracy 1\n",
      "Training in progress @ global_step 21450, g_loss 0.479195, d_loss 0.451465 accuracy 1\n",
      "Training in progress @ global_step 21500, g_loss 0.479333, d_loss 0.451152 accuracy 1\n",
      "Training in progress @ global_step 21550, g_loss 0.479424, d_loss 0.451205 accuracy 1\n",
      "Training in progress @ global_step 21600, g_loss 0.479591, d_loss 0.452707 accuracy 1\n",
      "Training in progress @ global_step 21650, g_loss 0.479707, d_loss 0.451479 accuracy 1\n",
      "Training in progress @ global_step 21700, g_loss 0.479892, d_loss 0.450144 accuracy 1\n",
      "Training in progress @ global_step 21750, g_loss 0.479897, d_loss 0.451411 accuracy 1\n",
      "Training in progress @ global_step 21800, g_loss 0.479932, d_loss 0.449436 accuracy 1\n",
      "Training in progress @ global_step 21850, g_loss 0.479891, d_loss 0.452533 accuracy 1\n",
      "Training in progress @ global_step 21900, g_loss 0.479941, d_loss 0.451821 accuracy 1\n",
      "Training in progress @ global_step 21950, g_loss 0.479891, d_loss 0.452093 accuracy 1\n",
      "Training in progress @ global_step 22000, g_loss 0.479864, d_loss 0.448307 accuracy 1\n",
      "Training in progress @ global_step 22050, g_loss 0.479877, d_loss 0.448853 accuracy 1\n",
      "Training in progress @ global_step 22100, g_loss 0.479976, d_loss 0.452567 accuracy 1\n",
      "Training in progress @ global_step 22150, g_loss 0.480033, d_loss 0.447401 accuracy 1\n",
      "Training in progress @ global_step 22200, g_loss 0.480081, d_loss 0.451652 accuracy 1\n",
      "Training in progress @ global_step 22250, g_loss 0.480271, d_loss 0.449446 accuracy 1\n",
      "Training in progress @ global_step 22300, g_loss 0.480341, d_loss 0.449478 accuracy 1\n",
      "Training in progress @ global_step 22350, g_loss 0.480378, d_loss 0.449287 accuracy 1\n",
      "Training in progress @ global_step 22400, g_loss 0.480067, d_loss 0.44887 accuracy 1\n",
      "Training in progress @ global_step 22450, g_loss 0.480012, d_loss 0.450744 accuracy 1\n",
      "Training in progress @ global_step 22500, g_loss 0.479934, d_loss 0.449717 accuracy 1\n",
      "Training in progress @ global_step 22550, g_loss 0.479774, d_loss 0.448871 accuracy 1\n",
      "Training in progress @ global_step 22600, g_loss 0.479714, d_loss 0.448782 accuracy 1\n",
      "Training in progress @ global_step 22650, g_loss 0.479593, d_loss 0.448413 accuracy 1\n",
      "Training in progress @ global_step 22700, g_loss 0.479444, d_loss 0.448014 accuracy 1\n",
      "Training in progress @ global_step 22750, g_loss 0.479402, d_loss 0.446647 accuracy 1\n",
      "Training in progress @ global_step 22800, g_loss 0.479296, d_loss 0.447403 accuracy 1\n",
      "Training in progress @ global_step 22850, g_loss 0.479228, d_loss 0.44586 accuracy 1\n",
      "Training in progress @ global_step 22900, g_loss 0.479252, d_loss 0.446702 accuracy 1\n",
      "Training in progress @ global_step 22950, g_loss 0.479214, d_loss 0.446754 accuracy 1\n",
      "Training in progress @ global_step 23000, g_loss 0.479244, d_loss 0.443641 accuracy 1\n",
      "Training in progress @ global_step 23050, g_loss 0.479229, d_loss 0.447233 accuracy 1\n",
      "Training in progress @ global_step 23100, g_loss 0.479041, d_loss 0.444524 accuracy 1\n",
      "Training in progress @ global_step 23150, g_loss 0.479104, d_loss 0.44514 accuracy 1\n",
      "Training in progress @ global_step 23200, g_loss 0.479025, d_loss 0.444141 accuracy 1\n",
      "Training in progress @ global_step 23250, g_loss 0.479078, d_loss 0.446885 accuracy 1\n",
      "Training in progress @ global_step 23300, g_loss 0.479071, d_loss 0.441626 accuracy 1\n",
      "Training in progress @ global_step 23350, g_loss 0.478948, d_loss 0.447421 accuracy 1\n",
      "Training in progress @ global_step 23400, g_loss 0.47906, d_loss 0.44298 accuracy 1\n",
      "Training in progress @ global_step 23450, g_loss 0.479095, d_loss 0.446762 accuracy 1\n",
      "Training in progress @ global_step 23500, g_loss 0.479113, d_loss 0.443849 accuracy 1\n",
      "Training in progress @ global_step 23550, g_loss 0.479119, d_loss 0.443419 accuracy 1\n",
      "Training in progress @ global_step 23600, g_loss 0.47914, d_loss 0.445818 accuracy 1\n",
      "Training in progress @ global_step 23650, g_loss 0.479253, d_loss 0.443801 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 23700, g_loss 0.479168, d_loss 0.446026 accuracy 1\n",
      "Training in progress @ global_step 23750, g_loss 0.4792, d_loss 0.44289 accuracy 1\n",
      "Training in progress @ global_step 23800, g_loss 0.479203, d_loss 0.443565 accuracy 1\n",
      "Training in progress @ global_step 23850, g_loss 0.479179, d_loss 0.438818 accuracy 1\n",
      "Training in progress @ global_step 23900, g_loss 0.479358, d_loss 0.441956 accuracy 1\n",
      "Training in progress @ global_step 23950, g_loss 0.479347, d_loss 0.443668 accuracy 1\n",
      "Training in progress @ global_step 24000, g_loss 0.47936, d_loss 0.443562 accuracy 1\n",
      "Training in progress @ global_step 24050, g_loss 0.479445, d_loss 0.444432 accuracy 1\n",
      "Training in progress @ global_step 24100, g_loss 0.479409, d_loss 0.443124 accuracy 1\n",
      "Training in progress @ global_step 24150, g_loss 0.479528, d_loss 0.441759 accuracy 1\n",
      "Training in progress @ global_step 24200, g_loss 0.479661, d_loss 0.442624 accuracy 1\n",
      "Training in progress @ global_step 24250, g_loss 0.479632, d_loss 0.442363 accuracy 1\n",
      "Training in progress @ global_step 24300, g_loss 0.47968, d_loss 0.444162 accuracy 1\n",
      "Training in progress @ global_step 24350, g_loss 0.479626, d_loss 0.44486 accuracy 1\n",
      "Training in progress @ global_step 24400, g_loss 0.479644, d_loss 0.44596 accuracy 1\n",
      "Training in progress @ global_step 24450, g_loss 0.479705, d_loss 0.445813 accuracy 1\n",
      "Training in progress @ global_step 24500, g_loss 0.479767, d_loss 0.446433 accuracy 1\n",
      "Training in progress @ global_step 24550, g_loss 0.479789, d_loss 0.442402 accuracy 1\n",
      "Training in progress @ global_step 24600, g_loss 0.479825, d_loss 0.444135 accuracy 1\n",
      "Training in progress @ global_step 24650, g_loss 0.479724, d_loss 0.442982 accuracy 1\n",
      "Training in progress @ global_step 24700, g_loss 0.479847, d_loss 0.444138 accuracy 1\n",
      "Training in progress @ global_step 24750, g_loss 0.479969, d_loss 0.444247 accuracy 1\n",
      "Training in progress @ global_step 24800, g_loss 0.479948, d_loss 0.444046 accuracy 1\n",
      "Training in progress @ global_step 24850, g_loss 0.480083, d_loss 0.444315 accuracy 1\n",
      "Training in progress @ global_step 24900, g_loss 0.480034, d_loss 0.442 accuracy 1\n",
      "Training in progress @ global_step 24950, g_loss 0.480134, d_loss 0.4426 accuracy 1\n",
      "Training in progress @ global_step 25000, g_loss 0.480138, d_loss 0.44068 accuracy 1\n",
      "Training in progress @ global_step 25050, g_loss 0.48017, d_loss 0.442042 accuracy 1\n",
      "Training in progress @ global_step 25100, g_loss 0.480095, d_loss 0.445487 accuracy 1\n",
      "Training in progress @ global_step 25150, g_loss 0.480205, d_loss 0.440775 accuracy 1\n",
      "Training in progress @ global_step 25200, g_loss 0.480143, d_loss 0.443157 accuracy 1\n",
      "Training in progress @ global_step 25250, g_loss 0.480089, d_loss 0.441226 accuracy 1\n",
      "Training in progress @ global_step 25300, g_loss 0.480023, d_loss 0.440806 accuracy 1\n",
      "Training in progress @ global_step 25350, g_loss 0.480201, d_loss 0.437732 accuracy 1\n",
      "Training in progress @ global_step 25400, g_loss 0.479912, d_loss 0.442491 accuracy 1\n",
      "Training in progress @ global_step 25450, g_loss 0.480158, d_loss 0.439325 accuracy 1\n",
      "Training in progress @ global_step 25500, g_loss 0.480118, d_loss 0.440892 accuracy 1\n",
      "Training in progress @ global_step 25550, g_loss 0.480191, d_loss 0.437421 accuracy 1\n",
      "Training in progress @ global_step 25600, g_loss 0.480169, d_loss 0.436998 accuracy 1\n",
      "Training in progress @ global_step 25650, g_loss 0.480086, d_loss 0.441751 accuracy 1\n",
      "Training in progress @ global_step 25700, g_loss 0.479997, d_loss 0.443173 accuracy 1\n",
      "Training in progress @ global_step 25750, g_loss 0.480114, d_loss 0.443213 accuracy 1\n",
      "Training in progress @ global_step 25800, g_loss 0.480093, d_loss 0.440359 accuracy 1\n",
      "Training in progress @ global_step 25850, g_loss 0.480334, d_loss 0.439751 accuracy 1\n",
      "Training in progress @ global_step 25900, g_loss 0.480204, d_loss 0.443496 accuracy 1\n",
      "Training in progress @ global_step 25950, g_loss 0.480238, d_loss 0.438635 accuracy 1\n",
      "Training in progress @ global_step 26000, g_loss 0.480198, d_loss 0.441216 accuracy 1\n",
      "Training in progress @ global_step 26050, g_loss 0.480262, d_loss 0.440303 accuracy 1\n",
      "Training in progress @ global_step 26100, g_loss 0.480512, d_loss 0.438606 accuracy 1\n",
      "Training in progress @ global_step 26150, g_loss 0.480349, d_loss 0.441982 accuracy 1\n",
      "Training in progress @ global_step 26200, g_loss 0.480229, d_loss 0.438604 accuracy 1\n",
      "Training in progress @ global_step 26250, g_loss 0.480424, d_loss 0.438735 accuracy 1\n",
      "Training in progress @ global_step 26300, g_loss 0.480385, d_loss 0.436739 accuracy 1\n",
      "Training in progress @ global_step 26350, g_loss 0.480645, d_loss 0.436617 accuracy 1\n",
      "Training in progress @ global_step 26400, g_loss 0.480501, d_loss 0.441892 accuracy 1\n",
      "Training in progress @ global_step 26450, g_loss 0.480491, d_loss 0.439314 accuracy 1\n",
      "Training in progress @ global_step 26500, g_loss 0.480479, d_loss 0.435587 accuracy 1\n",
      "Training in progress @ global_step 26550, g_loss 0.480495, d_loss 0.440548 accuracy 1\n",
      "Training in progress @ global_step 26600, g_loss 0.480678, d_loss 0.440061 accuracy 1\n",
      "Training in progress @ global_step 26650, g_loss 0.480564, d_loss 0.439812 accuracy 1\n",
      "Training in progress @ global_step 26700, g_loss 0.480531, d_loss 0.436793 accuracy 1\n",
      "Training in progress @ global_step 26750, g_loss 0.480377, d_loss 0.442116 accuracy 1\n",
      "Training in progress @ global_step 26800, g_loss 0.480545, d_loss 0.437282 accuracy 1\n",
      "Training in progress @ global_step 26850, g_loss 0.48063, d_loss 0.435733 accuracy 1\n",
      "Training in progress @ global_step 26900, g_loss 0.480421, d_loss 0.437846 accuracy 1\n",
      "Training in progress @ global_step 26950, g_loss 0.480578, d_loss 0.43669 accuracy 1\n",
      "Training in progress @ global_step 27000, g_loss 0.480621, d_loss 0.440837 accuracy 1\n",
      "Training in progress @ global_step 27050, g_loss 0.480518, d_loss 0.440862 accuracy 1\n",
      "Training in progress @ global_step 27100, g_loss 0.480631, d_loss 0.435188 accuracy 1\n",
      "Training in progress @ global_step 27150, g_loss 0.480561, d_loss 0.436629 accuracy 1\n",
      "Training in progress @ global_step 27200, g_loss 0.480637, d_loss 0.434716 accuracy 1\n",
      "Training in progress @ global_step 27250, g_loss 0.480702, d_loss 0.441672 accuracy 1\n",
      "Training in progress @ global_step 27300, g_loss 0.480749, d_loss 0.439009 accuracy 1\n",
      "Training in progress @ global_step 27350, g_loss 0.480965, d_loss 0.43948 accuracy 1\n",
      "Training in progress @ global_step 27400, g_loss 0.480811, d_loss 0.435154 accuracy 1\n",
      "Training in progress @ global_step 27450, g_loss 0.480679, d_loss 0.435719 accuracy 1\n",
      "Training in progress @ global_step 27500, g_loss 0.480774, d_loss 0.439715 accuracy 1\n",
      "Training in progress @ global_step 27550, g_loss 0.480628, d_loss 0.438743 accuracy 1\n",
      "Training in progress @ global_step 27600, g_loss 0.480932, d_loss 0.43596 accuracy 1\n",
      "Training in progress @ global_step 27650, g_loss 0.481158, d_loss 0.440055 accuracy 1\n",
      "Training in progress @ global_step 27700, g_loss 0.480839, d_loss 0.435128 accuracy 1\n",
      "Training in progress @ global_step 27750, g_loss 0.481159, d_loss 0.44303 accuracy 1\n",
      "Training in progress @ global_step 27800, g_loss 0.481156, d_loss 0.439789 accuracy 1\n",
      "Training in progress @ global_step 27850, g_loss 0.480919, d_loss 0.438043 accuracy 1\n",
      "Training in progress @ global_step 27900, g_loss 0.481005, d_loss 0.436651 accuracy 1\n",
      "Training in progress @ global_step 27950, g_loss 0.481252, d_loss 0.433925 accuracy 1\n",
      "Training in progress @ global_step 28000, g_loss 0.481095, d_loss 0.43698 accuracy 1\n",
      "Training in progress @ global_step 28050, g_loss 0.481195, d_loss 0.433541 accuracy 1\n",
      "Training in progress @ global_step 28100, g_loss 0.481176, d_loss 0.434893 accuracy 1\n",
      "Training in progress @ global_step 28150, g_loss 0.481058, d_loss 0.434679 accuracy 1\n",
      "Training in progress @ global_step 28200, g_loss 0.4811, d_loss 0.436284 accuracy 1\n",
      "Training in progress @ global_step 28250, g_loss 0.481057, d_loss 0.437573 accuracy 1\n",
      "Training in progress @ global_step 28300, g_loss 0.481174, d_loss 0.431508 accuracy 1\n",
      "Training in progress @ global_step 28350, g_loss 0.480772, d_loss 0.430712 accuracy 1\n",
      "Training in progress @ global_step 28400, g_loss 0.481251, d_loss 0.438071 accuracy 1\n",
      "Training in progress @ global_step 28450, g_loss 0.481072, d_loss 0.432331 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 28500, g_loss 0.480871, d_loss 0.432128 accuracy 1\n",
      "Training in progress @ global_step 28550, g_loss 0.48102, d_loss 0.436996 accuracy 1\n",
      "Training in progress @ global_step 28600, g_loss 0.480702, d_loss 0.435603 accuracy 1\n",
      "Training in progress @ global_step 28650, g_loss 0.48103, d_loss 0.435422 accuracy 1\n",
      "Training in progress @ global_step 28700, g_loss 0.480687, d_loss 0.430749 accuracy 1\n",
      "Training in progress @ global_step 28750, g_loss 0.48051, d_loss 0.436151 accuracy 1\n",
      "Training in progress @ global_step 28800, g_loss 0.480668, d_loss 0.435164 accuracy 1\n",
      "Training in progress @ global_step 28850, g_loss 0.480811, d_loss 0.43372 accuracy 1\n",
      "Training in progress @ global_step 28900, g_loss 0.480597, d_loss 0.435963 accuracy 1\n",
      "Training in progress @ global_step 28950, g_loss 0.481129, d_loss 0.431839 accuracy 1\n",
      "Training in progress @ global_step 29000, g_loss 0.481143, d_loss 0.435139 accuracy 1\n",
      "Training in progress @ global_step 29050, g_loss 0.480874, d_loss 0.436886 accuracy 1\n",
      "Training in progress @ global_step 29100, g_loss 0.480902, d_loss 0.436347 accuracy 1\n",
      "Training in progress @ global_step 29150, g_loss 0.481161, d_loss 0.432918 accuracy 1\n",
      "Training in progress @ global_step 29200, g_loss 0.480392, d_loss 0.430868 accuracy 1\n",
      "Training in progress @ global_step 29250, g_loss 0.480882, d_loss 0.436839 accuracy 1\n",
      "Training in progress @ global_step 29300, g_loss 0.480644, d_loss 0.436753 accuracy 1\n",
      "Training in progress @ global_step 29350, g_loss 0.480062, d_loss 0.433973 accuracy 1\n",
      "Training in progress @ global_step 29400, g_loss 0.480489, d_loss 0.433 accuracy 1\n",
      "Training in progress @ global_step 29450, g_loss 0.480873, d_loss 0.436522 accuracy 1\n",
      "Training in progress @ global_step 29500, g_loss 0.481066, d_loss 0.42956 accuracy 1\n",
      "Training in progress @ global_step 29550, g_loss 0.481042, d_loss 0.430277 accuracy 1\n",
      "Training in progress @ global_step 29600, g_loss 0.48067, d_loss 0.435855 accuracy 1\n",
      "Training in progress @ global_step 29650, g_loss 0.480535, d_loss 0.432931 accuracy 1\n",
      "Training in progress @ global_step 29700, g_loss 0.480141, d_loss 0.429831 accuracy 1\n",
      "Training in progress @ global_step 29750, g_loss 0.480158, d_loss 0.431359 accuracy 1\n",
      "Training in progress @ global_step 29800, g_loss 0.480889, d_loss 0.428231 accuracy 1\n",
      "Training in progress @ global_step 29850, g_loss 0.48004, d_loss 0.429539 accuracy 1\n",
      "Training in progress @ global_step 29900, g_loss 0.480054, d_loss 0.431972 accuracy 0.984375\n",
      "Training in progress @ global_step 29950, g_loss 0.480301, d_loss 0.430917 accuracy 1\n",
      "Training in progress @ global_step 30000, g_loss 0.480536, d_loss 0.434091 accuracy 1\n",
      "Training in progress @ global_step 30050, g_loss 0.480728, d_loss 0.437134 accuracy 1\n",
      "Training in progress @ global_step 30100, g_loss 0.480334, d_loss 0.433044 accuracy 0.984375\n",
      "Training in progress @ global_step 30150, g_loss 0.48071, d_loss 0.426621 accuracy 1\n",
      "Training in progress @ global_step 30200, g_loss 0.480521, d_loss 0.431617 accuracy 1\n",
      "Training in progress @ global_step 30250, g_loss 0.480413, d_loss 0.434525 accuracy 1\n",
      "Training in progress @ global_step 30300, g_loss 0.480776, d_loss 0.433584 accuracy 1\n",
      "Training in progress @ global_step 30350, g_loss 0.48035, d_loss 0.43402 accuracy 1\n",
      "Training in progress @ global_step 30400, g_loss 0.48007, d_loss 0.423611 accuracy 1\n",
      "Training in progress @ global_step 30450, g_loss 0.480047, d_loss 0.430847 accuracy 1\n",
      "Training in progress @ global_step 30500, g_loss 0.480177, d_loss 0.431223 accuracy 1\n",
      "Training in progress @ global_step 30550, g_loss 0.480078, d_loss 0.434407 accuracy 1\n",
      "Training in progress @ global_step 30600, g_loss 0.479257, d_loss 0.431586 accuracy 1\n",
      "Training in progress @ global_step 30650, g_loss 0.480542, d_loss 0.431694 accuracy 1\n",
      "Training in progress @ global_step 30700, g_loss 0.480367, d_loss 0.429619 accuracy 1\n",
      "Training in progress @ global_step 30750, g_loss 0.479613, d_loss 0.436331 accuracy 1\n",
      "Training in progress @ global_step 30800, g_loss 0.479435, d_loss 0.424474 accuracy 0.984375\n",
      "Training in progress @ global_step 30850, g_loss 0.479897, d_loss 0.427781 accuracy 1\n",
      "Training in progress @ global_step 30900, g_loss 0.479657, d_loss 0.428158 accuracy 1\n",
      "Training in progress @ global_step 30950, g_loss 0.480134, d_loss 0.431546 accuracy 1\n",
      "Training in progress @ global_step 31000, g_loss 0.479993, d_loss 0.432049 accuracy 0.984375\n",
      "Training in progress @ global_step 31050, g_loss 0.480187, d_loss 0.432842 accuracy 1\n",
      "Training in progress @ global_step 31100, g_loss 0.479803, d_loss 0.423976 accuracy 1\n",
      "Training in progress @ global_step 31150, g_loss 0.478987, d_loss 0.426379 accuracy 0.96875\n",
      "Training in progress @ global_step 31200, g_loss 0.479633, d_loss 0.430156 accuracy 1\n",
      "Training in progress @ global_step 31250, g_loss 0.479406, d_loss 0.433212 accuracy 0.953125\n",
      "Training in progress @ global_step 31300, g_loss 0.479477, d_loss 0.430661 accuracy 0.984375\n",
      "Training in progress @ global_step 31350, g_loss 0.479552, d_loss 0.426895 accuracy 0.96875\n",
      "Training in progress @ global_step 31400, g_loss 0.480122, d_loss 0.428998 accuracy 1\n",
      "Training in progress @ global_step 31450, g_loss 0.479381, d_loss 0.429206 accuracy 0.96875\n",
      "Training in progress @ global_step 31500, g_loss 0.479411, d_loss 0.426447 accuracy 0.984375\n",
      "Training in progress @ global_step 31550, g_loss 0.47933, d_loss 0.431313 accuracy 0.9375\n",
      "Training in progress @ global_step 31600, g_loss 0.478483, d_loss 0.430146 accuracy 0.9375\n",
      "Training in progress @ global_step 31650, g_loss 0.478891, d_loss 0.423167 accuracy 0.96875\n",
      "Training in progress @ global_step 31700, g_loss 0.478866, d_loss 0.422307 accuracy 0.953125\n",
      "Training in progress @ global_step 31750, g_loss 0.478226, d_loss 0.423895 accuracy 0.921875\n",
      "Training in progress @ global_step 31800, g_loss 0.477899, d_loss 0.426817 accuracy 0.953125\n",
      "Training in progress @ global_step 31850, g_loss 0.478773, d_loss 0.42818 accuracy 0.984375\n",
      "Training in progress @ global_step 31900, g_loss 0.477445, d_loss 0.427548 accuracy 0.9375\n",
      "Training in progress @ global_step 31950, g_loss 0.47877, d_loss 0.42734 accuracy 0.953125\n",
      "Training in progress @ global_step 32000, g_loss 0.479013, d_loss 0.425674 accuracy 0.9375\n",
      "Training in progress @ global_step 32050, g_loss 0.477163, d_loss 0.421928 accuracy 0.890625\n",
      "Training in progress @ global_step 32100, g_loss 0.479198, d_loss 0.428151 accuracy 0.984375\n",
      "Training in progress @ global_step 32150, g_loss 0.47864, d_loss 0.425117 accuracy 0.953125\n",
      "Training in progress @ global_step 32200, g_loss 0.477415, d_loss 0.426345 accuracy 0.9375\n",
      "Training in progress @ global_step 32250, g_loss 0.478487, d_loss 0.424546 accuracy 0.984375\n",
      "Training in progress @ global_step 32300, g_loss 0.477773, d_loss 0.430823 accuracy 0.921875\n",
      "Training in progress @ global_step 32350, g_loss 0.477663, d_loss 0.433127 accuracy 0.90625\n",
      "Training in progress @ global_step 32400, g_loss 0.477486, d_loss 0.424786 accuracy 0.90625\n",
      "Training in progress @ global_step 32450, g_loss 0.478449, d_loss 0.425644 accuracy 0.9375\n",
      "Training in progress @ global_step 32500, g_loss 0.478573, d_loss 0.426761 accuracy 0.9375\n",
      "Training in progress @ global_step 32550, g_loss 0.476119, d_loss 0.422942 accuracy 0.828125\n",
      "Training in progress @ global_step 32600, g_loss 0.477868, d_loss 0.427379 accuracy 0.9375\n",
      "Training in progress @ global_step 32650, g_loss 0.477681, d_loss 0.423932 accuracy 0.9375\n",
      "Training in progress @ global_step 32700, g_loss 0.477283, d_loss 0.426192 accuracy 0.875\n",
      "Training in progress @ global_step 32750, g_loss 0.476921, d_loss 0.422322 accuracy 0.859375\n",
      "Training in progress @ global_step 32800, g_loss 0.476322, d_loss 0.42897 accuracy 0.875\n",
      "Training in progress @ global_step 32850, g_loss 0.475733, d_loss 0.427638 accuracy 0.8125\n",
      "Training in progress @ global_step 32900, g_loss 0.476062, d_loss 0.426261 accuracy 0.796875\n",
      "Training in progress @ global_step 32950, g_loss 0.477057, d_loss 0.42568 accuracy 0.875\n",
      "Training in progress @ global_step 33000, g_loss 0.477381, d_loss 0.426962 accuracy 0.953125\n",
      "Training in progress @ global_step 33050, g_loss 0.477485, d_loss 0.42239 accuracy 0.890625\n",
      "Training in progress @ global_step 33100, g_loss 0.477347, d_loss 0.424637 accuracy 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 33150, g_loss 0.476366, d_loss 0.424585 accuracy 0.875\n",
      "Training in progress @ global_step 33200, g_loss 0.477818, d_loss 0.424003 accuracy 0.890625\n",
      "Training in progress @ global_step 33250, g_loss 0.476738, d_loss 0.426022 accuracy 0.875\n",
      "Training in progress @ global_step 33300, g_loss 0.477048, d_loss 0.421184 accuracy 0.890625\n",
      "Training in progress @ global_step 33350, g_loss 0.476647, d_loss 0.425456 accuracy 0.84375\n",
      "Training in progress @ global_step 33400, g_loss 0.476277, d_loss 0.417226 accuracy 0.875\n",
      "Training in progress @ global_step 33450, g_loss 0.476356, d_loss 0.424794 accuracy 0.875\n",
      "Training in progress @ global_step 33500, g_loss 0.478024, d_loss 0.426097 accuracy 0.9375\n",
      "Training in progress @ global_step 33550, g_loss 0.474159, d_loss 0.419482 accuracy 0.765625\n",
      "Training in progress @ global_step 33600, g_loss 0.474874, d_loss 0.426869 accuracy 0.8125\n",
      "Training in progress @ global_step 33650, g_loss 0.474768, d_loss 0.423448 accuracy 0.828125\n",
      "Training in progress @ global_step 33700, g_loss 0.47684, d_loss 0.419434 accuracy 0.875\n",
      "Training in progress @ global_step 33750, g_loss 0.475131, d_loss 0.430743 accuracy 0.796875\n",
      "Training in progress @ global_step 33800, g_loss 0.474769, d_loss 0.430191 accuracy 0.796875\n",
      "Training in progress @ global_step 33850, g_loss 0.475924, d_loss 0.422576 accuracy 0.84375\n",
      "Training in progress @ global_step 33900, g_loss 0.475261, d_loss 0.42261 accuracy 0.828125\n",
      "Training in progress @ global_step 33950, g_loss 0.474568, d_loss 0.418467 accuracy 0.796875\n",
      "Training in progress @ global_step 34000, g_loss 0.47573, d_loss 0.427605 accuracy 0.828125\n",
      "Training in progress @ global_step 34050, g_loss 0.474351, d_loss 0.42004 accuracy 0.796875\n",
      "Training in progress @ global_step 34100, g_loss 0.473459, d_loss 0.421015 accuracy 0.796875\n",
      "Training in progress @ global_step 34150, g_loss 0.474569, d_loss 0.424833 accuracy 0.796875\n",
      "Training in progress @ global_step 34200, g_loss 0.476342, d_loss 0.424652 accuracy 0.828125\n",
      "Training in progress @ global_step 34250, g_loss 0.474121, d_loss 0.42701 accuracy 0.796875\n",
      "Training in progress @ global_step 34300, g_loss 0.47442, d_loss 0.424451 accuracy 0.828125\n",
      "Training in progress @ global_step 34350, g_loss 0.474476, d_loss 0.42583 accuracy 0.8125\n",
      "Training in progress @ global_step 34400, g_loss 0.473589, d_loss 0.42327 accuracy 0.765625\n",
      "Training in progress @ global_step 34450, g_loss 0.475103, d_loss 0.419005 accuracy 0.8125\n",
      "Training in progress @ global_step 34500, g_loss 0.47382, d_loss 0.421042 accuracy 0.78125\n",
      "Training in progress @ global_step 34550, g_loss 0.473401, d_loss 0.418307 accuracy 0.765625\n",
      "Training in progress @ global_step 34600, g_loss 0.47343, d_loss 0.417918 accuracy 0.765625\n",
      "Training in progress @ global_step 34650, g_loss 0.475838, d_loss 0.424286 accuracy 0.84375\n",
      "Training in progress @ global_step 34700, g_loss 0.473872, d_loss 0.418271 accuracy 0.796875\n",
      "Training in progress @ global_step 34750, g_loss 0.473719, d_loss 0.421392 accuracy 0.78125\n",
      "Training in progress @ global_step 34800, g_loss 0.472586, d_loss 0.420423 accuracy 0.75\n",
      "Training in progress @ global_step 34850, g_loss 0.473302, d_loss 0.425851 accuracy 0.765625\n",
      "Training in progress @ global_step 34900, g_loss 0.474004, d_loss 0.420431 accuracy 0.796875\n",
      "Training in progress @ global_step 34950, g_loss 0.475345, d_loss 0.42001 accuracy 0.84375\n",
      "Training in progress @ global_step 35000, g_loss 0.4721, d_loss 0.421234 accuracy 0.71875\n",
      "Training in progress @ global_step 35050, g_loss 0.471851, d_loss 0.417154 accuracy 0.703125\n",
      "Training in progress @ global_step 35100, g_loss 0.473479, d_loss 0.422132 accuracy 0.78125\n",
      "Training in progress @ global_step 35150, g_loss 0.472093, d_loss 0.422829 accuracy 0.734375\n",
      "Training in progress @ global_step 35200, g_loss 0.472449, d_loss 0.417713 accuracy 0.6875\n",
      "Training in progress @ global_step 35250, g_loss 0.473142, d_loss 0.421093 accuracy 0.78125\n",
      "Training in progress @ global_step 35300, g_loss 0.473397, d_loss 0.41902 accuracy 0.8125\n",
      "Training in progress @ global_step 35350, g_loss 0.473004, d_loss 0.424763 accuracy 0.75\n",
      "Training in progress @ global_step 35400, g_loss 0.473519, d_loss 0.422821 accuracy 0.765625\n",
      "Training in progress @ global_step 35450, g_loss 0.472912, d_loss 0.417659 accuracy 0.765625\n",
      "Training in progress @ global_step 35500, g_loss 0.473503, d_loss 0.418103 accuracy 0.75\n",
      "Training in progress @ global_step 35550, g_loss 0.470359, d_loss 0.417844 accuracy 0.65625\n",
      "Training in progress @ global_step 35600, g_loss 0.469762, d_loss 0.424823 accuracy 0.703125\n",
      "Training in progress @ global_step 35650, g_loss 0.470938, d_loss 0.416403 accuracy 0.703125\n",
      "Training in progress @ global_step 35700, g_loss 0.469985, d_loss 0.414568 accuracy 0.671875\n",
      "Training in progress @ global_step 35750, g_loss 0.470146, d_loss 0.415775 accuracy 0.703125\n",
      "Training in progress @ global_step 35800, g_loss 0.469876, d_loss 0.41991 accuracy 0.703125\n",
      "Training in progress @ global_step 35850, g_loss 0.467893, d_loss 0.418632 accuracy 0.625\n",
      "Training in progress @ global_step 35900, g_loss 0.467771, d_loss 0.416364 accuracy 0.65625\n",
      "Training in progress @ global_step 35950, g_loss 0.472266, d_loss 0.419391 accuracy 0.734375\n",
      "Training in progress @ global_step 36000, g_loss 0.471868, d_loss 0.421346 accuracy 0.71875\n",
      "Training in progress @ global_step 36050, g_loss 0.468754, d_loss 0.422071 accuracy 0.65625\n",
      "Training in progress @ global_step 36100, g_loss 0.471225, d_loss 0.41966 accuracy 0.734375\n",
      "Training in progress @ global_step 36150, g_loss 0.473682, d_loss 0.4113 accuracy 0.765625\n",
      "Training in progress @ global_step 36200, g_loss 0.473224, d_loss 0.418105 accuracy 0.8125\n",
      "Training in progress @ global_step 36250, g_loss 0.470779, d_loss 0.421642 accuracy 0.75\n",
      "Training in progress @ global_step 36300, g_loss 0.471915, d_loss 0.417292 accuracy 0.796875\n",
      "Training in progress @ global_step 36350, g_loss 0.469745, d_loss 0.416511 accuracy 0.703125\n",
      "Training in progress @ global_step 36400, g_loss 0.471646, d_loss 0.41758 accuracy 0.734375\n",
      "Training in progress @ global_step 36450, g_loss 0.468572, d_loss 0.422287 accuracy 0.6875\n",
      "Training in progress @ global_step 36500, g_loss 0.471399, d_loss 0.422629 accuracy 0.78125\n",
      "Training in progress @ global_step 36550, g_loss 0.47288, d_loss 0.418714 accuracy 0.828125\n",
      "Training in progress @ global_step 36600, g_loss 0.467952, d_loss 0.420109 accuracy 0.671875\n",
      "Training in progress @ global_step 36650, g_loss 0.468541, d_loss 0.41668 accuracy 0.71875\n",
      "Training in progress @ global_step 36700, g_loss 0.470593, d_loss 0.416648 accuracy 0.734375\n",
      "Training in progress @ global_step 36750, g_loss 0.470229, d_loss 0.413385 accuracy 0.71875\n",
      "Training in progress @ global_step 36800, g_loss 0.465796, d_loss 0.420922 accuracy 0.59375\n",
      "Training in progress @ global_step 36850, g_loss 0.467492, d_loss 0.421056 accuracy 0.609375\n",
      "Training in progress @ global_step 36900, g_loss 0.469226, d_loss 0.419007 accuracy 0.71875\n",
      "Training in progress @ global_step 36950, g_loss 0.467875, d_loss 0.413393 accuracy 0.671875\n",
      "Training in progress @ global_step 37000, g_loss 0.470865, d_loss 0.417452 accuracy 0.71875\n",
      "Training in progress @ global_step 37050, g_loss 0.467912, d_loss 0.420908 accuracy 0.671875\n",
      "Training in progress @ global_step 37100, g_loss 0.467814, d_loss 0.410394 accuracy 0.6875\n",
      "Training in progress @ global_step 37150, g_loss 0.471176, d_loss 0.417336 accuracy 0.75\n",
      "Training in progress @ global_step 37200, g_loss 0.466942, d_loss 0.419034 accuracy 0.65625\n",
      "Training in progress @ global_step 37250, g_loss 0.468788, d_loss 0.421089 accuracy 0.75\n",
      "Training in progress @ global_step 37300, g_loss 0.466062, d_loss 0.417924 accuracy 0.65625\n",
      "Training in progress @ global_step 37350, g_loss 0.468199, d_loss 0.416432 accuracy 0.671875\n",
      "Training in progress @ global_step 37400, g_loss 0.46807, d_loss 0.414825 accuracy 0.6875\n",
      "Training in progress @ global_step 37450, g_loss 0.469987, d_loss 0.421672 accuracy 0.703125\n",
      "Training in progress @ global_step 37500, g_loss 0.467784, d_loss 0.420331 accuracy 0.671875\n",
      "Training in progress @ global_step 37550, g_loss 0.463661, d_loss 0.412379 accuracy 0.625\n",
      "Training in progress @ global_step 37600, g_loss 0.467664, d_loss 0.414014 accuracy 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 37650, g_loss 0.468, d_loss 0.417179 accuracy 0.671875\n",
      "Training in progress @ global_step 37700, g_loss 0.469376, d_loss 0.417281 accuracy 0.71875\n",
      "Training in progress @ global_step 37750, g_loss 0.463332, d_loss 0.413636 accuracy 0.59375\n",
      "Training in progress @ global_step 37800, g_loss 0.467657, d_loss 0.417635 accuracy 0.703125\n",
      "Training in progress @ global_step 37850, g_loss 0.466386, d_loss 0.415847 accuracy 0.671875\n",
      "Training in progress @ global_step 37900, g_loss 0.467459, d_loss 0.412988 accuracy 0.640625\n",
      "Training in progress @ global_step 37950, g_loss 0.461311, d_loss 0.4157 accuracy 0.5625\n",
      "Training in progress @ global_step 38000, g_loss 0.463819, d_loss 0.413943 accuracy 0.5625\n",
      "Training in progress @ global_step 38050, g_loss 0.465853, d_loss 0.417841 accuracy 0.640625\n",
      "Training in progress @ global_step 38100, g_loss 0.466485, d_loss 0.41604 accuracy 0.625\n",
      "Training in progress @ global_step 38150, g_loss 0.467465, d_loss 0.417025 accuracy 0.671875\n",
      "Training in progress @ global_step 38200, g_loss 0.463487, d_loss 0.419379 accuracy 0.578125\n",
      "Training in progress @ global_step 38250, g_loss 0.464989, d_loss 0.416303 accuracy 0.59375\n",
      "Training in progress @ global_step 38300, g_loss 0.465129, d_loss 0.415937 accuracy 0.609375\n",
      "Training in progress @ global_step 38350, g_loss 0.467959, d_loss 0.419111 accuracy 0.6875\n",
      "Training in progress @ global_step 38400, g_loss 0.46873, d_loss 0.412487 accuracy 0.65625\n",
      "Training in progress @ global_step 38450, g_loss 0.465591, d_loss 0.411385 accuracy 0.6875\n",
      "Training in progress @ global_step 38500, g_loss 0.466741, d_loss 0.415891 accuracy 0.65625\n",
      "Training in progress @ global_step 38550, g_loss 0.463712, d_loss 0.418721 accuracy 0.625\n",
      "Training in progress @ global_step 38600, g_loss 0.466091, d_loss 0.40918 accuracy 0.65625\n",
      "Training in progress @ global_step 38650, g_loss 0.465718, d_loss 0.417496 accuracy 0.65625\n",
      "Training in progress @ global_step 38700, g_loss 0.466287, d_loss 0.415982 accuracy 0.625\n",
      "Training in progress @ global_step 38750, g_loss 0.46125, d_loss 0.414642 accuracy 0.5625\n",
      "Training in progress @ global_step 38800, g_loss 0.466512, d_loss 0.416309 accuracy 0.640625\n",
      "Training in progress @ global_step 38850, g_loss 0.465437, d_loss 0.416756 accuracy 0.59375\n",
      "Training in progress @ global_step 38900, g_loss 0.468068, d_loss 0.417006 accuracy 0.65625\n",
      "Training in progress @ global_step 38950, g_loss 0.464041, d_loss 0.414813 accuracy 0.578125\n",
      "Training in progress @ global_step 39000, g_loss 0.464692, d_loss 0.410485 accuracy 0.578125\n",
      "Training in progress @ global_step 39050, g_loss 0.463665, d_loss 0.413206 accuracy 0.640625\n",
      "Training in progress @ global_step 39100, g_loss 0.461829, d_loss 0.414752 accuracy 0.5625\n",
      "Training in progress @ global_step 39150, g_loss 0.464449, d_loss 0.413484 accuracy 0.640625\n",
      "Training in progress @ global_step 39200, g_loss 0.465943, d_loss 0.416765 accuracy 0.65625\n",
      "Training in progress @ global_step 39250, g_loss 0.465576, d_loss 0.410938 accuracy 0.640625\n",
      "Training in progress @ global_step 39300, g_loss 0.464986, d_loss 0.418383 accuracy 0.625\n",
      "Training in progress @ global_step 39350, g_loss 0.464606, d_loss 0.415051 accuracy 0.625\n",
      "Training in progress @ global_step 39400, g_loss 0.468519, d_loss 0.415016 accuracy 0.6875\n",
      "Training in progress @ global_step 39450, g_loss 0.465323, d_loss 0.418633 accuracy 0.625\n",
      "Training in progress @ global_step 39500, g_loss 0.466777, d_loss 0.414703 accuracy 0.671875\n",
      "Training in progress @ global_step 39550, g_loss 0.467758, d_loss 0.412633 accuracy 0.625\n",
      "Training in progress @ global_step 39600, g_loss 0.466758, d_loss 0.418979 accuracy 0.65625\n",
      "Training in progress @ global_step 39650, g_loss 0.468746, d_loss 0.412983 accuracy 0.6875\n",
      "Training in progress @ global_step 39700, g_loss 0.463789, d_loss 0.417713 accuracy 0.59375\n",
      "Training in progress @ global_step 39750, g_loss 0.467944, d_loss 0.415144 accuracy 0.65625\n",
      "Training in progress @ global_step 39800, g_loss 0.467411, d_loss 0.415426 accuracy 0.65625\n",
      "Training in progress @ global_step 39850, g_loss 0.469052, d_loss 0.411422 accuracy 0.65625\n",
      "Training in progress @ global_step 39900, g_loss 0.464672, d_loss 0.414252 accuracy 0.5625\n",
      "Training in progress @ global_step 39950, g_loss 0.467076, d_loss 0.409411 accuracy 0.609375\n",
      "Training in progress @ global_step 40000, g_loss 0.466894, d_loss 0.412813 accuracy 0.640625\n",
      "Training in progress @ global_step 40050, g_loss 0.463453, d_loss 0.414291 accuracy 0.578125\n",
      "Training in progress @ global_step 40100, g_loss 0.464649, d_loss 0.417134 accuracy 0.609375\n",
      "Training in progress @ global_step 40150, g_loss 0.467746, d_loss 0.418113 accuracy 0.59375\n",
      "Training in progress @ global_step 40200, g_loss 0.468052, d_loss 0.414601 accuracy 0.640625\n",
      "Training in progress @ global_step 40250, g_loss 0.465297, d_loss 0.410843 accuracy 0.59375\n",
      "Training in progress @ global_step 40300, g_loss 0.466752, d_loss 0.414332 accuracy 0.609375\n",
      "Training in progress @ global_step 40350, g_loss 0.463497, d_loss 0.411184 accuracy 0.5625\n",
      "Training in progress @ global_step 40400, g_loss 0.466311, d_loss 0.414456 accuracy 0.609375\n",
      "Training in progress @ global_step 40450, g_loss 0.463813, d_loss 0.411257 accuracy 0.609375\n",
      "Training in progress @ global_step 40500, g_loss 0.467149, d_loss 0.414469 accuracy 0.59375\n",
      "Training in progress @ global_step 40550, g_loss 0.465475, d_loss 0.409293 accuracy 0.59375\n",
      "Training in progress @ global_step 40600, g_loss 0.463892, d_loss 0.413453 accuracy 0.5625\n",
      "Training in progress @ global_step 40650, g_loss 0.467564, d_loss 0.413866 accuracy 0.640625\n",
      "Training in progress @ global_step 40700, g_loss 0.467265, d_loss 0.411321 accuracy 0.609375\n",
      "Training in progress @ global_step 40750, g_loss 0.465594, d_loss 0.417075 accuracy 0.578125\n",
      "Training in progress @ global_step 40800, g_loss 0.469001, d_loss 0.409781 accuracy 0.65625\n",
      "Training in progress @ global_step 40850, g_loss 0.467122, d_loss 0.412955 accuracy 0.625\n",
      "Training in progress @ global_step 40900, g_loss 0.469785, d_loss 0.415544 accuracy 0.609375\n",
      "Training in progress @ global_step 40950, g_loss 0.466427, d_loss 0.401239 accuracy 0.5625\n",
      "Training in progress @ global_step 41000, g_loss 0.464479, d_loss 0.413253 accuracy 0.5625\n",
      "Training in progress @ global_step 41050, g_loss 0.464783, d_loss 0.417078 accuracy 0.625\n",
      "Training in progress @ global_step 41100, g_loss 0.464244, d_loss 0.414371 accuracy 0.5625\n",
      "Training in progress @ global_step 41150, g_loss 0.467794, d_loss 0.416359 accuracy 0.65625\n",
      "Training in progress @ global_step 41200, g_loss 0.465984, d_loss 0.411273 accuracy 0.546875\n",
      "Training in progress @ global_step 41250, g_loss 0.469331, d_loss 0.414013 accuracy 0.640625\n",
      "Training in progress @ global_step 41300, g_loss 0.467771, d_loss 0.412519 accuracy 0.546875\n",
      "Training in progress @ global_step 41350, g_loss 0.46732, d_loss 0.410602 accuracy 0.609375\n",
      "Training in progress @ global_step 41400, g_loss 0.466929, d_loss 0.413898 accuracy 0.59375\n",
      "Training in progress @ global_step 41450, g_loss 0.468161, d_loss 0.40819 accuracy 0.625\n",
      "Training in progress @ global_step 41500, g_loss 0.467287, d_loss 0.413302 accuracy 0.578125\n",
      "Training in progress @ global_step 41550, g_loss 0.468691, d_loss 0.413903 accuracy 0.609375\n",
      "Training in progress @ global_step 41600, g_loss 0.467907, d_loss 0.412427 accuracy 0.59375\n",
      "Training in progress @ global_step 41650, g_loss 0.46616, d_loss 0.410023 accuracy 0.5625\n",
      "Training in progress @ global_step 41700, g_loss 0.466418, d_loss 0.405944 accuracy 0.546875\n",
      "Training in progress @ global_step 41750, g_loss 0.468772, d_loss 0.412151 accuracy 0.59375\n",
      "Training in progress @ global_step 41800, g_loss 0.468851, d_loss 0.414444 accuracy 0.625\n",
      "Training in progress @ global_step 41850, g_loss 0.46628, d_loss 0.415095 accuracy 0.53125\n",
      "Training in progress @ global_step 41900, g_loss 0.469895, d_loss 0.412655 accuracy 0.671875\n",
      "Training in progress @ global_step 41950, g_loss 0.467146, d_loss 0.420794 accuracy 0.625\n",
      "Training in progress @ global_step 42000, g_loss 0.469019, d_loss 0.410138 accuracy 0.625\n",
      "Training in progress @ global_step 42050, g_loss 0.470129, d_loss 0.416344 accuracy 0.59375\n",
      "Training in progress @ global_step 42100, g_loss 0.469525, d_loss 0.408148 accuracy 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 42150, g_loss 0.469637, d_loss 0.416419 accuracy 0.6875\n",
      "Training in progress @ global_step 42200, g_loss 0.468208, d_loss 0.412382 accuracy 0.625\n",
      "Training in progress @ global_step 42250, g_loss 0.468283, d_loss 0.412915 accuracy 0.578125\n",
      "Training in progress @ global_step 42300, g_loss 0.468972, d_loss 0.413718 accuracy 0.59375\n",
      "Training in progress @ global_step 42350, g_loss 0.469406, d_loss 0.410397 accuracy 0.609375\n",
      "Training in progress @ global_step 42400, g_loss 0.467189, d_loss 0.413406 accuracy 0.609375\n",
      "Training in progress @ global_step 42450, g_loss 0.46775, d_loss 0.411295 accuracy 0.59375\n",
      "Training in progress @ global_step 42500, g_loss 0.46996, d_loss 0.405822 accuracy 0.640625\n",
      "Training in progress @ global_step 42550, g_loss 0.468393, d_loss 0.41405 accuracy 0.625\n",
      "Training in progress @ global_step 42600, g_loss 0.46831, d_loss 0.414771 accuracy 0.609375\n",
      "Training in progress @ global_step 42650, g_loss 0.470622, d_loss 0.419561 accuracy 0.640625\n",
      "Training in progress @ global_step 42700, g_loss 0.468651, d_loss 0.409953 accuracy 0.578125\n",
      "Training in progress @ global_step 42750, g_loss 0.469103, d_loss 0.410472 accuracy 0.640625\n",
      "Training in progress @ global_step 42800, g_loss 0.469915, d_loss 0.414048 accuracy 0.625\n",
      "Training in progress @ global_step 42850, g_loss 0.470035, d_loss 0.41721 accuracy 0.625\n",
      "Training in progress @ global_step 42900, g_loss 0.469063, d_loss 0.408174 accuracy 0.578125\n",
      "Training in progress @ global_step 42950, g_loss 0.470906, d_loss 0.416048 accuracy 0.703125\n",
      "Training in progress @ global_step 43000, g_loss 0.47099, d_loss 0.411932 accuracy 0.703125\n",
      "Training in progress @ global_step 43050, g_loss 0.467589, d_loss 0.40798 accuracy 0.578125\n",
      "Training in progress @ global_step 43100, g_loss 0.469899, d_loss 0.411476 accuracy 0.625\n",
      "Training in progress @ global_step 43150, g_loss 0.467374, d_loss 0.415504 accuracy 0.625\n",
      "Training in progress @ global_step 43200, g_loss 0.471068, d_loss 0.417773 accuracy 0.640625\n",
      "Training in progress @ global_step 43250, g_loss 0.469076, d_loss 0.415081 accuracy 0.640625\n",
      "Training in progress @ global_step 43300, g_loss 0.46918, d_loss 0.413885 accuracy 0.640625\n",
      "Training in progress @ global_step 43350, g_loss 0.470957, d_loss 0.410051 accuracy 0.71875\n",
      "Training in progress @ global_step 43400, g_loss 0.470933, d_loss 0.413022 accuracy 0.703125\n",
      "Training in progress @ global_step 43450, g_loss 0.471031, d_loss 0.417911 accuracy 0.6875\n",
      "Training in progress @ global_step 43500, g_loss 0.47194, d_loss 0.413 accuracy 0.71875\n",
      "Training in progress @ global_step 43550, g_loss 0.471027, d_loss 0.419587 accuracy 0.71875\n",
      "Training in progress @ global_step 43600, g_loss 0.470653, d_loss 0.413974 accuracy 0.734375\n",
      "Training in progress @ global_step 43650, g_loss 0.470912, d_loss 0.413915 accuracy 0.671875\n",
      "Training in progress @ global_step 43700, g_loss 0.472369, d_loss 0.417399 accuracy 0.75\n",
      "Training in progress @ global_step 43750, g_loss 0.470266, d_loss 0.414417 accuracy 0.671875\n",
      "Training in progress @ global_step 43800, g_loss 0.469783, d_loss 0.412527 accuracy 0.75\n",
      "Training in progress @ global_step 43850, g_loss 0.469561, d_loss 0.413199 accuracy 0.640625\n",
      "Training in progress @ global_step 43900, g_loss 0.468215, d_loss 0.409477 accuracy 0.65625\n",
      "Training in progress @ global_step 43950, g_loss 0.471051, d_loss 0.410699 accuracy 0.734375\n",
      "Training in progress @ global_step 44000, g_loss 0.471093, d_loss 0.414757 accuracy 0.75\n",
      "Training in progress @ global_step 44050, g_loss 0.470519, d_loss 0.415814 accuracy 0.71875\n",
      "Training in progress @ global_step 44100, g_loss 0.470829, d_loss 0.415903 accuracy 0.71875\n",
      "Training in progress @ global_step 44150, g_loss 0.468391, d_loss 0.419205 accuracy 0.640625\n",
      "Training in progress @ global_step 44200, g_loss 0.469796, d_loss 0.419413 accuracy 0.65625\n",
      "Training in progress @ global_step 44250, g_loss 0.467996, d_loss 0.411371 accuracy 0.671875\n",
      "Training in progress @ global_step 44300, g_loss 0.469608, d_loss 0.411832 accuracy 0.65625\n",
      "Training in progress @ global_step 44350, g_loss 0.470163, d_loss 0.417024 accuracy 0.6875\n",
      "Training in progress @ global_step 44400, g_loss 0.470752, d_loss 0.413448 accuracy 0.6875\n",
      "Training in progress @ global_step 44450, g_loss 0.470112, d_loss 0.411497 accuracy 0.671875\n",
      "Training in progress @ global_step 44500, g_loss 0.469828, d_loss 0.412645 accuracy 0.6875\n",
      "Training in progress @ global_step 44550, g_loss 0.470248, d_loss 0.41135 accuracy 0.71875\n",
      "Training in progress @ global_step 44600, g_loss 0.47069, d_loss 0.413948 accuracy 0.703125\n",
      "Training in progress @ global_step 44650, g_loss 0.473275, d_loss 0.417378 accuracy 0.78125\n",
      "Training in progress @ global_step 44700, g_loss 0.471541, d_loss 0.411588 accuracy 0.75\n",
      "Training in progress @ global_step 44750, g_loss 0.471428, d_loss 0.415123 accuracy 0.71875\n",
      "Training in progress @ global_step 44800, g_loss 0.469756, d_loss 0.414275 accuracy 0.765625\n",
      "Training in progress @ global_step 44850, g_loss 0.471919, d_loss 0.410949 accuracy 0.71875\n",
      "Training in progress @ global_step 44900, g_loss 0.471999, d_loss 0.409512 accuracy 0.75\n",
      "Training in progress @ global_step 44950, g_loss 0.470686, d_loss 0.407883 accuracy 0.703125\n",
      "Training in progress @ global_step 45000, g_loss 0.470683, d_loss 0.413746 accuracy 0.734375\n",
      "Training in progress @ global_step 45050, g_loss 0.47254, d_loss 0.419187 accuracy 0.734375\n",
      "Training in progress @ global_step 45100, g_loss 0.469279, d_loss 0.409003 accuracy 0.6875\n",
      "Training in progress @ global_step 45150, g_loss 0.470327, d_loss 0.422036 accuracy 0.734375\n",
      "Training in progress @ global_step 45200, g_loss 0.468368, d_loss 0.407331 accuracy 0.6875\n",
      "Training in progress @ global_step 45250, g_loss 0.468752, d_loss 0.418947 accuracy 0.671875\n",
      "Training in progress @ global_step 45300, g_loss 0.471732, d_loss 0.420906 accuracy 0.765625\n",
      "Training in progress @ global_step 45350, g_loss 0.467463, d_loss 0.417031 accuracy 0.640625\n",
      "Training in progress @ global_step 45400, g_loss 0.468684, d_loss 0.41863 accuracy 0.671875\n",
      "Training in progress @ global_step 45450, g_loss 0.47523, d_loss 0.417374 accuracy 0.8125\n",
      "Training in progress @ global_step 45500, g_loss 0.465881, d_loss 0.414187 accuracy 0.703125\n",
      "Training in progress @ global_step 45550, g_loss 0.469392, d_loss 0.417477 accuracy 0.71875\n",
      "Training in progress @ global_step 45600, g_loss 0.470421, d_loss 0.413556 accuracy 0.71875\n",
      "Training in progress @ global_step 45650, g_loss 0.469067, d_loss 0.412198 accuracy 0.734375\n",
      "Training in progress @ global_step 45700, g_loss 0.470321, d_loss 0.4196 accuracy 0.75\n",
      "Training in progress @ global_step 45750, g_loss 0.469516, d_loss 0.417036 accuracy 0.703125\n",
      "Training in progress @ global_step 45800, g_loss 0.470201, d_loss 0.415989 accuracy 0.703125\n",
      "Training in progress @ global_step 45850, g_loss 0.467246, d_loss 0.418367 accuracy 0.71875\n",
      "Training in progress @ global_step 45900, g_loss 0.470191, d_loss 0.422375 accuracy 0.703125\n",
      "Training in progress @ global_step 45950, g_loss 0.46714, d_loss 0.414126 accuracy 0.625\n",
      "Training in progress @ global_step 46000, g_loss 0.468947, d_loss 0.41269 accuracy 0.734375\n",
      "Training in progress @ global_step 46050, g_loss 0.469674, d_loss 0.40887 accuracy 0.71875\n",
      "Training in progress @ global_step 46100, g_loss 0.470698, d_loss 0.418766 accuracy 0.78125\n",
      "Training in progress @ global_step 46150, g_loss 0.470178, d_loss 0.421013 accuracy 0.78125\n",
      "Training in progress @ global_step 46200, g_loss 0.467027, d_loss 0.416682 accuracy 0.6875\n",
      "Training in progress @ global_step 46250, g_loss 0.467815, d_loss 0.414162 accuracy 0.671875\n",
      "Training in progress @ global_step 46300, g_loss 0.465648, d_loss 0.415939 accuracy 0.671875\n",
      "Training in progress @ global_step 46350, g_loss 0.466248, d_loss 0.415782 accuracy 0.6875\n",
      "Training in progress @ global_step 46400, g_loss 0.468327, d_loss 0.413851 accuracy 0.71875\n",
      "Training in progress @ global_step 46450, g_loss 0.470421, d_loss 0.415088 accuracy 0.71875\n",
      "Training in progress @ global_step 46500, g_loss 0.465144, d_loss 0.418269 accuracy 0.6875\n",
      "Training in progress @ global_step 46550, g_loss 0.467544, d_loss 0.4185 accuracy 0.75\n",
      "Training in progress @ global_step 46600, g_loss 0.471436, d_loss 0.415254 accuracy 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 46650, g_loss 0.466128, d_loss 0.408987 accuracy 0.703125\n",
      "Training in progress @ global_step 46700, g_loss 0.462884, d_loss 0.411634 accuracy 0.640625\n",
      "Training in progress @ global_step 46750, g_loss 0.469025, d_loss 0.409515 accuracy 0.703125\n",
      "Training in progress @ global_step 46800, g_loss 0.46875, d_loss 0.409715 accuracy 0.65625\n",
      "Training in progress @ global_step 46850, g_loss 0.467719, d_loss 0.41166 accuracy 0.625\n",
      "Training in progress @ global_step 46900, g_loss 0.466013, d_loss 0.407436 accuracy 0.671875\n",
      "Training in progress @ global_step 46950, g_loss 0.465911, d_loss 0.412051 accuracy 0.6875\n",
      "Training in progress @ global_step 47000, g_loss 0.468367, d_loss 0.409456 accuracy 0.6875\n",
      "Training in progress @ global_step 47050, g_loss 0.467346, d_loss 0.418328 accuracy 0.71875\n",
      "Training in progress @ global_step 47100, g_loss 0.467367, d_loss 0.417791 accuracy 0.734375\n",
      "Training in progress @ global_step 47150, g_loss 0.467799, d_loss 0.403907 accuracy 0.703125\n",
      "Training in progress @ global_step 47200, g_loss 0.462589, d_loss 0.41691 accuracy 0.6875\n",
      "Training in progress @ global_step 47250, g_loss 0.462876, d_loss 0.415143 accuracy 0.625\n",
      "Training in progress @ global_step 47300, g_loss 0.463, d_loss 0.409513 accuracy 0.59375\n",
      "Training in progress @ global_step 47350, g_loss 0.462906, d_loss 0.410078 accuracy 0.703125\n",
      "Training in progress @ global_step 47400, g_loss 0.467879, d_loss 0.412758 accuracy 0.71875\n",
      "Training in progress @ global_step 47450, g_loss 0.465075, d_loss 0.411716 accuracy 0.671875\n",
      "Training in progress @ global_step 47500, g_loss 0.460317, d_loss 0.40806 accuracy 0.640625\n",
      "Training in progress @ global_step 47550, g_loss 0.463596, d_loss 0.409142 accuracy 0.609375\n",
      "Training in progress @ global_step 47600, g_loss 0.465238, d_loss 0.412321 accuracy 0.65625\n",
      "Training in progress @ global_step 47650, g_loss 0.461295, d_loss 0.416089 accuracy 0.59375\n",
      "Training in progress @ global_step 47700, g_loss 0.459301, d_loss 0.412216 accuracy 0.640625\n",
      "Training in progress @ global_step 47750, g_loss 0.465189, d_loss 0.412699 accuracy 0.671875\n",
      "Training in progress @ global_step 47800, g_loss 0.466118, d_loss 0.407676 accuracy 0.6875\n",
      "Training in progress @ global_step 47850, g_loss 0.465366, d_loss 0.416145 accuracy 0.71875\n",
      "Training in progress @ global_step 47900, g_loss 0.462352, d_loss 0.408813 accuracy 0.640625\n",
      "Training in progress @ global_step 47950, g_loss 0.463869, d_loss 0.413782 accuracy 0.65625\n",
      "Training in progress @ global_step 48000, g_loss 0.458796, d_loss 0.412426 accuracy 0.578125\n",
      "Training in progress @ global_step 48050, g_loss 0.468203, d_loss 0.414722 accuracy 0.734375\n",
      "Training in progress @ global_step 48100, g_loss 0.465749, d_loss 0.411585 accuracy 0.625\n",
      "Training in progress @ global_step 48150, g_loss 0.46386, d_loss 0.408253 accuracy 0.625\n",
      "Training in progress @ global_step 48200, g_loss 0.459341, d_loss 0.408014 accuracy 0.59375\n",
      "Training in progress @ global_step 48250, g_loss 0.460818, d_loss 0.411612 accuracy 0.640625\n",
      "Training in progress @ global_step 48300, g_loss 0.462142, d_loss 0.413609 accuracy 0.671875\n",
      "Training in progress @ global_step 48350, g_loss 0.455166, d_loss 0.405966 accuracy 0.53125\n",
      "Training in progress @ global_step 48400, g_loss 0.463401, d_loss 0.406926 accuracy 0.65625\n",
      "Training in progress @ global_step 48450, g_loss 0.466582, d_loss 0.412987 accuracy 0.609375\n",
      "Training in progress @ global_step 48500, g_loss 0.456461, d_loss 0.407374 accuracy 0.59375\n",
      "Training in progress @ global_step 48550, g_loss 0.463467, d_loss 0.406418 accuracy 0.625\n",
      "Training in progress @ global_step 48600, g_loss 0.460699, d_loss 0.408332 accuracy 0.546875\n",
      "Training in progress @ global_step 48650, g_loss 0.461702, d_loss 0.406255 accuracy 0.609375\n",
      "Training in progress @ global_step 48700, g_loss 0.462319, d_loss 0.411562 accuracy 0.578125\n",
      "Training in progress @ global_step 48750, g_loss 0.459697, d_loss 0.416976 accuracy 0.578125\n",
      "Training in progress @ global_step 48800, g_loss 0.459787, d_loss 0.409232 accuracy 0.53125\n",
      "Training in progress @ global_step 48850, g_loss 0.461709, d_loss 0.411874 accuracy 0.625\n",
      "Training in progress @ global_step 48900, g_loss 0.461481, d_loss 0.414317 accuracy 0.59375\n",
      "Training in progress @ global_step 48950, g_loss 0.461613, d_loss 0.407501 accuracy 0.59375\n",
      "Training in progress @ global_step 49000, g_loss 0.464388, d_loss 0.411043 accuracy 0.640625\n",
      "Training in progress @ global_step 49050, g_loss 0.46317, d_loss 0.408634 accuracy 0.609375\n",
      "Training in progress @ global_step 49100, g_loss 0.457818, d_loss 0.410445 accuracy 0.546875\n",
      "Training in progress @ global_step 49150, g_loss 0.464701, d_loss 0.412179 accuracy 0.609375\n",
      "Training in progress @ global_step 49200, g_loss 0.465255, d_loss 0.410325 accuracy 0.640625\n",
      "Training in progress @ global_step 49250, g_loss 0.459377, d_loss 0.406976 accuracy 0.5625\n",
      "Training in progress @ global_step 49300, g_loss 0.456899, d_loss 0.400617 accuracy 0.59375\n",
      "Training in progress @ global_step 49350, g_loss 0.465844, d_loss 0.411393 accuracy 0.625\n",
      "Training in progress @ global_step 49400, g_loss 0.464624, d_loss 0.406359 accuracy 0.609375\n",
      "Training in progress @ global_step 49450, g_loss 0.457578, d_loss 0.411994 accuracy 0.625\n",
      "Training in progress @ global_step 49500, g_loss 0.463176, d_loss 0.414657 accuracy 0.609375\n",
      "Training in progress @ global_step 49550, g_loss 0.461283, d_loss 0.413511 accuracy 0.609375\n",
      "Training in progress @ global_step 49600, g_loss 0.457875, d_loss 0.409821 accuracy 0.578125\n",
      "Training in progress @ global_step 49650, g_loss 0.46517, d_loss 0.41117 accuracy 0.734375\n",
      "Training in progress @ global_step 49700, g_loss 0.462906, d_loss 0.408598 accuracy 0.59375\n",
      "Training in progress @ global_step 49750, g_loss 0.463741, d_loss 0.408157 accuracy 0.640625\n",
      "Training in progress @ global_step 49800, g_loss 0.458512, d_loss 0.40848 accuracy 0.640625\n",
      "Training in progress @ global_step 49850, g_loss 0.457584, d_loss 0.4101 accuracy 0.5625\n",
      "Training in progress @ global_step 49900, g_loss 0.456361, d_loss 0.406834 accuracy 0.59375\n",
      "Training in progress @ global_step 49950, g_loss 0.457456, d_loss 0.408402 accuracy 0.578125\n",
      "Training in progress @ global_step 50000, g_loss 0.462459, d_loss 0.408066 accuracy 0.59375\n",
      "Training in progress @ global_step 50050, g_loss 0.463463, d_loss 0.409859 accuracy 0.671875\n",
      "Training in progress @ global_step 50100, g_loss 0.459431, d_loss 0.413942 accuracy 0.546875\n",
      "Training in progress @ global_step 50150, g_loss 0.456375, d_loss 0.406126 accuracy 0.640625\n",
      "Training in progress @ global_step 50200, g_loss 0.459451, d_loss 0.41557 accuracy 0.578125\n",
      "Training in progress @ global_step 50250, g_loss 0.45719, d_loss 0.414369 accuracy 0.59375\n",
      "Training in progress @ global_step 50300, g_loss 0.457969, d_loss 0.41284 accuracy 0.578125\n",
      "Training in progress @ global_step 50350, g_loss 0.457531, d_loss 0.408541 accuracy 0.640625\n",
      "Training in progress @ global_step 50400, g_loss 0.459769, d_loss 0.408253 accuracy 0.578125\n",
      "Training in progress @ global_step 50450, g_loss 0.45735, d_loss 0.412506 accuracy 0.59375\n",
      "Training in progress @ global_step 50500, g_loss 0.452649, d_loss 0.402473 accuracy 0.546875\n",
      "Training in progress @ global_step 50550, g_loss 0.458843, d_loss 0.411211 accuracy 0.625\n",
      "Training in progress @ global_step 50600, g_loss 0.456614, d_loss 0.40668 accuracy 0.640625\n",
      "Training in progress @ global_step 50650, g_loss 0.456931, d_loss 0.414015 accuracy 0.546875\n",
      "Training in progress @ global_step 50700, g_loss 0.454318, d_loss 0.412219 accuracy 0.5\n",
      "Training in progress @ global_step 50750, g_loss 0.458519, d_loss 0.414549 accuracy 0.5625\n",
      "Training in progress @ global_step 50800, g_loss 0.4551, d_loss 0.411196 accuracy 0.5625\n",
      "Training in progress @ global_step 50850, g_loss 0.456645, d_loss 0.411613 accuracy 0.609375\n",
      "Training in progress @ global_step 50900, g_loss 0.458718, d_loss 0.414187 accuracy 0.578125\n",
      "Training in progress @ global_step 50950, g_loss 0.456916, d_loss 0.412547 accuracy 0.53125\n",
      "Training in progress @ global_step 51000, g_loss 0.459031, d_loss 0.413765 accuracy 0.53125\n",
      "Training in progress @ global_step 51050, g_loss 0.459797, d_loss 0.410212 accuracy 0.53125\n",
      "Training in progress @ global_step 51100, g_loss 0.456276, d_loss 0.411413 accuracy 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 51150, g_loss 0.456556, d_loss 0.412377 accuracy 0.515625\n",
      "Training in progress @ global_step 51200, g_loss 0.45933, d_loss 0.414807 accuracy 0.5625\n",
      "Training in progress @ global_step 51250, g_loss 0.463512, d_loss 0.410592 accuracy 0.546875\n",
      "Training in progress @ global_step 51300, g_loss 0.454988, d_loss 0.404775 accuracy 0.53125\n",
      "Training in progress @ global_step 51350, g_loss 0.456967, d_loss 0.411651 accuracy 0.515625\n",
      "Training in progress @ global_step 51400, g_loss 0.455697, d_loss 0.414588 accuracy 0.515625\n",
      "Training in progress @ global_step 51450, g_loss 0.461221, d_loss 0.41 accuracy 0.515625\n",
      "Training in progress @ global_step 51500, g_loss 0.454829, d_loss 0.412263 accuracy 0.515625\n",
      "Training in progress @ global_step 51550, g_loss 0.458796, d_loss 0.414457 accuracy 0.53125\n",
      "Training in progress @ global_step 51600, g_loss 0.454596, d_loss 0.407375 accuracy 0.5\n",
      "Training in progress @ global_step 51650, g_loss 0.457755, d_loss 0.411353 accuracy 0.515625\n",
      "Training in progress @ global_step 51700, g_loss 0.454754, d_loss 0.414307 accuracy 0.5\n",
      "Training in progress @ global_step 51750, g_loss 0.461483, d_loss 0.409103 accuracy 0.53125\n",
      "Training in progress @ global_step 51800, g_loss 0.46352, d_loss 0.410988 accuracy 0.578125\n",
      "Training in progress @ global_step 51850, g_loss 0.456457, d_loss 0.40958 accuracy 0.546875\n",
      "Training in progress @ global_step 51900, g_loss 0.458228, d_loss 0.406223 accuracy 0.515625\n",
      "Training in progress @ global_step 51950, g_loss 0.458274, d_loss 0.412407 accuracy 0.53125\n",
      "Training in progress @ global_step 52000, g_loss 0.456734, d_loss 0.410309 accuracy 0.546875\n",
      "Training in progress @ global_step 52050, g_loss 0.460242, d_loss 0.410617 accuracy 0.5\n",
      "Training in progress @ global_step 52100, g_loss 0.460843, d_loss 0.402873 accuracy 0.53125\n",
      "Training in progress @ global_step 52150, g_loss 0.460789, d_loss 0.411878 accuracy 0.546875\n",
      "Training in progress @ global_step 52200, g_loss 0.461103, d_loss 0.413615 accuracy 0.59375\n",
      "Training in progress @ global_step 52250, g_loss 0.461945, d_loss 0.412351 accuracy 0.5625\n",
      "Training in progress @ global_step 52300, g_loss 0.460869, d_loss 0.411005 accuracy 0.578125\n",
      "Training in progress @ global_step 52350, g_loss 0.462717, d_loss 0.412457 accuracy 0.578125\n",
      "Training in progress @ global_step 52400, g_loss 0.461215, d_loss 0.410907 accuracy 0.59375\n",
      "Training in progress @ global_step 52450, g_loss 0.453617, d_loss 0.411139 accuracy 0.515625\n",
      "Training in progress @ global_step 52500, g_loss 0.454742, d_loss 0.408203 accuracy 0.546875\n",
      "Training in progress @ global_step 52550, g_loss 0.457308, d_loss 0.413111 accuracy 0.5625\n",
      "Training in progress @ global_step 52600, g_loss 0.458134, d_loss 0.409104 accuracy 0.515625\n",
      "Training in progress @ global_step 52650, g_loss 0.461403, d_loss 0.407221 accuracy 0.5625\n",
      "Training in progress @ global_step 52700, g_loss 0.460893, d_loss 0.413828 accuracy 0.609375\n",
      "Training in progress @ global_step 52750, g_loss 0.457794, d_loss 0.413749 accuracy 0.53125\n",
      "Training in progress @ global_step 52800, g_loss 0.461097, d_loss 0.41386 accuracy 0.59375\n",
      "Training in progress @ global_step 52850, g_loss 0.456547, d_loss 0.41044 accuracy 0.5625\n",
      "Training in progress @ global_step 52900, g_loss 0.458702, d_loss 0.412851 accuracy 0.609375\n",
      "Training in progress @ global_step 52950, g_loss 0.461734, d_loss 0.411362 accuracy 0.578125\n",
      "Training in progress @ global_step 53000, g_loss 0.458204, d_loss 0.414022 accuracy 0.609375\n",
      "Training in progress @ global_step 53050, g_loss 0.456302, d_loss 0.406234 accuracy 0.578125\n",
      "Training in progress @ global_step 53100, g_loss 0.463183, d_loss 0.40932 accuracy 0.59375\n",
      "Training in progress @ global_step 53150, g_loss 0.457442, d_loss 0.419787 accuracy 0.609375\n",
      "Training in progress @ global_step 53200, g_loss 0.458292, d_loss 0.413187 accuracy 0.59375\n",
      "Training in progress @ global_step 53250, g_loss 0.461527, d_loss 0.413626 accuracy 0.5625\n",
      "Training in progress @ global_step 53300, g_loss 0.455186, d_loss 0.413639 accuracy 0.515625\n",
      "Training in progress @ global_step 53350, g_loss 0.459154, d_loss 0.405649 accuracy 0.5625\n",
      "Training in progress @ global_step 53400, g_loss 0.457085, d_loss 0.40846 accuracy 0.515625\n",
      "Training in progress @ global_step 53450, g_loss 0.461281, d_loss 0.409111 accuracy 0.546875\n",
      "Training in progress @ global_step 53500, g_loss 0.455207, d_loss 0.410233 accuracy 0.546875\n",
      "Training in progress @ global_step 53550, g_loss 0.462823, d_loss 0.415594 accuracy 0.5625\n",
      "Training in progress @ global_step 53600, g_loss 0.460158, d_loss 0.411479 accuracy 0.546875\n",
      "Training in progress @ global_step 53650, g_loss 0.46244, d_loss 0.409787 accuracy 0.546875\n",
      "Training in progress @ global_step 53700, g_loss 0.463275, d_loss 0.409375 accuracy 0.59375\n",
      "Training in progress @ global_step 53750, g_loss 0.456944, d_loss 0.411695 accuracy 0.53125\n",
      "Training in progress @ global_step 53800, g_loss 0.458477, d_loss 0.410531 accuracy 0.515625\n",
      "Training in progress @ global_step 53850, g_loss 0.458385, d_loss 0.412762 accuracy 0.5\n",
      "Training in progress @ global_step 53900, g_loss 0.459363, d_loss 0.407655 accuracy 0.546875\n",
      "Training in progress @ global_step 53950, g_loss 0.459704, d_loss 0.41329 accuracy 0.578125\n",
      "Training in progress @ global_step 54000, g_loss 0.459453, d_loss 0.412657 accuracy 0.53125\n",
      "Training in progress @ global_step 54050, g_loss 0.457023, d_loss 0.416347 accuracy 0.53125\n",
      "Training in progress @ global_step 54100, g_loss 0.45758, d_loss 0.414061 accuracy 0.53125\n",
      "Training in progress @ global_step 54150, g_loss 0.457383, d_loss 0.411165 accuracy 0.53125\n",
      "Training in progress @ global_step 54200, g_loss 0.463671, d_loss 0.415823 accuracy 0.578125\n",
      "Training in progress @ global_step 54250, g_loss 0.462008, d_loss 0.416597 accuracy 0.5625\n",
      "Training in progress @ global_step 54300, g_loss 0.461647, d_loss 0.408472 accuracy 0.5625\n",
      "Training in progress @ global_step 54350, g_loss 0.461248, d_loss 0.411547 accuracy 0.546875\n",
      "Training in progress @ global_step 54400, g_loss 0.458849, d_loss 0.411218 accuracy 0.53125\n",
      "Training in progress @ global_step 54450, g_loss 0.463395, d_loss 0.412774 accuracy 0.5625\n",
      "Training in progress @ global_step 54500, g_loss 0.462637, d_loss 0.408546 accuracy 0.59375\n",
      "Training in progress @ global_step 54550, g_loss 0.461505, d_loss 0.410637 accuracy 0.515625\n",
      "Training in progress @ global_step 54600, g_loss 0.461858, d_loss 0.415054 accuracy 0.515625\n",
      "Training in progress @ global_step 54650, g_loss 0.46018, d_loss 0.408087 accuracy 0.546875\n",
      "Training in progress @ global_step 54700, g_loss 0.467658, d_loss 0.415177 accuracy 0.609375\n",
      "Training in progress @ global_step 54750, g_loss 0.463239, d_loss 0.416766 accuracy 0.5625\n",
      "Training in progress @ global_step 54800, g_loss 0.467667, d_loss 0.41627 accuracy 0.65625\n",
      "Training in progress @ global_step 54850, g_loss 0.463526, d_loss 0.415905 accuracy 0.546875\n",
      "Training in progress @ global_step 54900, g_loss 0.464066, d_loss 0.41615 accuracy 0.5625\n",
      "Training in progress @ global_step 54950, g_loss 0.462984, d_loss 0.408448 accuracy 0.578125\n",
      "Training in progress @ global_step 55000, g_loss 0.462682, d_loss 0.412731 accuracy 0.53125\n",
      "Training in progress @ global_step 55050, g_loss 0.466003, d_loss 0.411885 accuracy 0.609375\n",
      "Training in progress @ global_step 55100, g_loss 0.46537, d_loss 0.413785 accuracy 0.5625\n",
      "Training in progress @ global_step 55150, g_loss 0.464143, d_loss 0.417566 accuracy 0.53125\n",
      "Training in progress @ global_step 55200, g_loss 0.466292, d_loss 0.417679 accuracy 0.59375\n",
      "Training in progress @ global_step 55250, g_loss 0.466775, d_loss 0.415908 accuracy 0.59375\n",
      "Training in progress @ global_step 55300, g_loss 0.466166, d_loss 0.410381 accuracy 0.578125\n",
      "Training in progress @ global_step 55350, g_loss 0.466034, d_loss 0.412399 accuracy 0.640625\n",
      "Training in progress @ global_step 55400, g_loss 0.469225, d_loss 0.415451 accuracy 0.6875\n",
      "Training in progress @ global_step 55450, g_loss 0.463387, d_loss 0.415131 accuracy 0.546875\n",
      "Training in progress @ global_step 55500, g_loss 0.466425, d_loss 0.40987 accuracy 0.546875\n",
      "Training in progress @ global_step 55550, g_loss 0.464997, d_loss 0.415631 accuracy 0.59375\n",
      "Training in progress @ global_step 55600, g_loss 0.464182, d_loss 0.411354 accuracy 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 55650, g_loss 0.469593, d_loss 0.419283 accuracy 0.703125\n",
      "Training in progress @ global_step 55700, g_loss 0.466578, d_loss 0.411963 accuracy 0.59375\n",
      "Training in progress @ global_step 55750, g_loss 0.466524, d_loss 0.412376 accuracy 0.5625\n",
      "Training in progress @ global_step 55800, g_loss 0.469171, d_loss 0.419139 accuracy 0.703125\n",
      "Training in progress @ global_step 55850, g_loss 0.471476, d_loss 0.416193 accuracy 0.71875\n",
      "Training in progress @ global_step 55900, g_loss 0.466365, d_loss 0.418682 accuracy 0.609375\n",
      "Training in progress @ global_step 55950, g_loss 0.467094, d_loss 0.420674 accuracy 0.578125\n",
      "Training in progress @ global_step 56000, g_loss 0.46472, d_loss 0.420563 accuracy 0.640625\n",
      "Training in progress @ global_step 56050, g_loss 0.470816, d_loss 0.414267 accuracy 0.703125\n",
      "Training in progress @ global_step 56100, g_loss 0.469424, d_loss 0.414695 accuracy 0.671875\n",
      "Training in progress @ global_step 56150, g_loss 0.4649, d_loss 0.415547 accuracy 0.625\n",
      "Training in progress @ global_step 56200, g_loss 0.467326, d_loss 0.41259 accuracy 0.6875\n",
      "Training in progress @ global_step 56250, g_loss 0.466867, d_loss 0.417461 accuracy 0.6875\n",
      "Training in progress @ global_step 56300, g_loss 0.46904, d_loss 0.41715 accuracy 0.734375\n",
      "Training in progress @ global_step 56350, g_loss 0.468184, d_loss 0.416545 accuracy 0.671875\n",
      "Training in progress @ global_step 56400, g_loss 0.470699, d_loss 0.420858 accuracy 0.703125\n",
      "Training in progress @ global_step 56450, g_loss 0.468274, d_loss 0.417938 accuracy 0.78125\n",
      "Training in progress @ global_step 56500, g_loss 0.466166, d_loss 0.421566 accuracy 0.609375\n",
      "Training in progress @ global_step 56550, g_loss 0.46698, d_loss 0.425289 accuracy 0.640625\n",
      "Training in progress @ global_step 56600, g_loss 0.467877, d_loss 0.424293 accuracy 0.703125\n",
      "Training in progress @ global_step 56650, g_loss 0.46572, d_loss 0.419008 accuracy 0.65625\n",
      "Training in progress @ global_step 56700, g_loss 0.466934, d_loss 0.421195 accuracy 0.671875\n",
      "Training in progress @ global_step 56750, g_loss 0.471489, d_loss 0.417073 accuracy 0.796875\n",
      "Training in progress @ global_step 56800, g_loss 0.466962, d_loss 0.418371 accuracy 0.640625\n",
      "Training in progress @ global_step 56850, g_loss 0.471423, d_loss 0.425145 accuracy 0.734375\n",
      "Training in progress @ global_step 56900, g_loss 0.46985, d_loss 0.421733 accuracy 0.65625\n",
      "Training in progress @ global_step 56950, g_loss 0.468556, d_loss 0.420655 accuracy 0.71875\n",
      "Training in progress @ global_step 57000, g_loss 0.46948, d_loss 0.425928 accuracy 0.625\n",
      "Training in progress @ global_step 57050, g_loss 0.468772, d_loss 0.425055 accuracy 0.671875\n",
      "Training in progress @ global_step 57100, g_loss 0.470396, d_loss 0.418887 accuracy 0.6875\n",
      "Training in progress @ global_step 57150, g_loss 0.470367, d_loss 0.424611 accuracy 0.6875\n",
      "Training in progress @ global_step 57200, g_loss 0.469889, d_loss 0.422548 accuracy 0.640625\n",
      "Training in progress @ global_step 57250, g_loss 0.469695, d_loss 0.423045 accuracy 0.6875\n",
      "Training in progress @ global_step 57300, g_loss 0.470102, d_loss 0.42239 accuracy 0.671875\n",
      "Training in progress @ global_step 57350, g_loss 0.472484, d_loss 0.422648 accuracy 0.765625\n",
      "Training in progress @ global_step 57400, g_loss 0.469191, d_loss 0.422542 accuracy 0.640625\n",
      "Training in progress @ global_step 57450, g_loss 0.470617, d_loss 0.426214 accuracy 0.703125\n",
      "Training in progress @ global_step 57500, g_loss 0.47139, d_loss 0.420308 accuracy 0.703125\n",
      "Training in progress @ global_step 57550, g_loss 0.470441, d_loss 0.421852 accuracy 0.703125\n",
      "Training in progress @ global_step 57600, g_loss 0.470202, d_loss 0.428144 accuracy 0.6875\n",
      "Training in progress @ global_step 57650, g_loss 0.472225, d_loss 0.419589 accuracy 0.71875\n",
      "Training in progress @ global_step 57700, g_loss 0.472272, d_loss 0.425029 accuracy 0.734375\n",
      "Training in progress @ global_step 57750, g_loss 0.470705, d_loss 0.421934 accuracy 0.71875\n",
      "Training in progress @ global_step 57800, g_loss 0.470773, d_loss 0.423643 accuracy 0.625\n",
      "Training in progress @ global_step 57850, g_loss 0.470064, d_loss 0.421742 accuracy 0.671875\n",
      "Training in progress @ global_step 57900, g_loss 0.471419, d_loss 0.427425 accuracy 0.703125\n",
      "Training in progress @ global_step 57950, g_loss 0.47026, d_loss 0.42749 accuracy 0.734375\n",
      "Training in progress @ global_step 58000, g_loss 0.471279, d_loss 0.422607 accuracy 0.703125\n",
      "Training in progress @ global_step 58050, g_loss 0.472222, d_loss 0.425038 accuracy 0.734375\n",
      "Training in progress @ global_step 58100, g_loss 0.473316, d_loss 0.424519 accuracy 0.796875\n",
      "Training in progress @ global_step 58150, g_loss 0.471757, d_loss 0.432663 accuracy 0.6875\n",
      "Training in progress @ global_step 58200, g_loss 0.474573, d_loss 0.422966 accuracy 0.8125\n",
      "Training in progress @ global_step 58250, g_loss 0.471413, d_loss 0.429102 accuracy 0.703125\n",
      "Training in progress @ global_step 58300, g_loss 0.474912, d_loss 0.426284 accuracy 0.8125\n",
      "Training in progress @ global_step 58350, g_loss 0.473387, d_loss 0.421008 accuracy 0.765625\n",
      "Training in progress @ global_step 58400, g_loss 0.469737, d_loss 0.427381 accuracy 0.734375\n",
      "Training in progress @ global_step 58450, g_loss 0.471534, d_loss 0.4164 accuracy 0.6875\n",
      "Training in progress @ global_step 58500, g_loss 0.47281, d_loss 0.42716 accuracy 0.78125\n",
      "Training in progress @ global_step 58550, g_loss 0.47232, d_loss 0.428514 accuracy 0.71875\n",
      "Training in progress @ global_step 58600, g_loss 0.474709, d_loss 0.42889 accuracy 0.828125\n",
      "Training in progress @ global_step 58650, g_loss 0.47411, d_loss 0.431364 accuracy 0.765625\n",
      "Training in progress @ global_step 58700, g_loss 0.473913, d_loss 0.420472 accuracy 0.8125\n",
      "Training in progress @ global_step 58750, g_loss 0.472566, d_loss 0.423794 accuracy 0.75\n",
      "Training in progress @ global_step 58800, g_loss 0.474639, d_loss 0.427095 accuracy 0.796875\n",
      "Training in progress @ global_step 58850, g_loss 0.471688, d_loss 0.430444 accuracy 0.734375\n",
      "Training in progress @ global_step 58900, g_loss 0.472426, d_loss 0.424857 accuracy 0.734375\n",
      "Training in progress @ global_step 58950, g_loss 0.4758, d_loss 0.428776 accuracy 0.875\n",
      "Training in progress @ global_step 59000, g_loss 0.474039, d_loss 0.423413 accuracy 0.796875\n",
      "Training in progress @ global_step 59050, g_loss 0.473024, d_loss 0.433397 accuracy 0.75\n",
      "Training in progress @ global_step 59100, g_loss 0.47538, d_loss 0.42322 accuracy 0.796875\n",
      "Training in progress @ global_step 59150, g_loss 0.473994, d_loss 0.430884 accuracy 0.734375\n",
      "Training in progress @ global_step 59200, g_loss 0.475239, d_loss 0.432273 accuracy 0.765625\n",
      "Training in progress @ global_step 59250, g_loss 0.475427, d_loss 0.42115 accuracy 0.796875\n",
      "Training in progress @ global_step 59300, g_loss 0.469771, d_loss 0.428984 accuracy 0.671875\n",
      "Training in progress @ global_step 59350, g_loss 0.472361, d_loss 0.428059 accuracy 0.765625\n",
      "Training in progress @ global_step 59400, g_loss 0.475123, d_loss 0.424164 accuracy 0.8125\n",
      "Training in progress @ global_step 59450, g_loss 0.471002, d_loss 0.426451 accuracy 0.671875\n",
      "Training in progress @ global_step 59500, g_loss 0.475326, d_loss 0.424111 accuracy 0.78125\n",
      "Training in progress @ global_step 59550, g_loss 0.470078, d_loss 0.429815 accuracy 0.734375\n",
      "Training in progress @ global_step 59600, g_loss 0.472424, d_loss 0.425668 accuracy 0.765625\n",
      "Training in progress @ global_step 59650, g_loss 0.47036, d_loss 0.417253 accuracy 0.71875\n",
      "Training in progress @ global_step 59700, g_loss 0.472406, d_loss 0.428548 accuracy 0.765625\n",
      "Training in progress @ global_step 59750, g_loss 0.475809, d_loss 0.427786 accuracy 0.78125\n",
      "Training in progress @ global_step 59800, g_loss 0.472934, d_loss 0.432124 accuracy 0.765625\n",
      "Training in progress @ global_step 59850, g_loss 0.470932, d_loss 0.428948 accuracy 0.734375\n",
      "Training in progress @ global_step 59900, g_loss 0.473167, d_loss 0.43164 accuracy 0.765625\n",
      "Training in progress @ global_step 59950, g_loss 0.470555, d_loss 0.431883 accuracy 0.6875\n",
      "Training in progress @ global_step 60000, g_loss 0.473227, d_loss 0.430917 accuracy 0.78125\n",
      "Training in progress @ global_step 60050, g_loss 0.474893, d_loss 0.428432 accuracy 0.78125\n",
      "Training in progress @ global_step 60100, g_loss 0.469785, d_loss 0.436921 accuracy 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 60150, g_loss 0.47144, d_loss 0.430205 accuracy 0.703125\n",
      "Training in progress @ global_step 60200, g_loss 0.472878, d_loss 0.438228 accuracy 0.75\n",
      "Training in progress @ global_step 60250, g_loss 0.471575, d_loss 0.425794 accuracy 0.734375\n",
      "Training in progress @ global_step 60300, g_loss 0.470522, d_loss 0.42881 accuracy 0.6875\n",
      "Training in progress @ global_step 60350, g_loss 0.47241, d_loss 0.429103 accuracy 0.78125\n",
      "Training in progress @ global_step 60400, g_loss 0.472612, d_loss 0.432498 accuracy 0.796875\n",
      "Training in progress @ global_step 60450, g_loss 0.469964, d_loss 0.427884 accuracy 0.703125\n",
      "Training in progress @ global_step 60500, g_loss 0.472064, d_loss 0.423565 accuracy 0.734375\n",
      "Training in progress @ global_step 60550, g_loss 0.470438, d_loss 0.423302 accuracy 0.671875\n",
      "Training in progress @ global_step 60600, g_loss 0.474149, d_loss 0.430026 accuracy 0.796875\n",
      "Training in progress @ global_step 60650, g_loss 0.471755, d_loss 0.42778 accuracy 0.78125\n",
      "Training in progress @ global_step 60700, g_loss 0.47064, d_loss 0.418061 accuracy 0.75\n",
      "Training in progress @ global_step 60750, g_loss 0.469748, d_loss 0.420235 accuracy 0.671875\n",
      "Training in progress @ global_step 60800, g_loss 0.472946, d_loss 0.433022 accuracy 0.765625\n",
      "Training in progress @ global_step 60850, g_loss 0.476797, d_loss 0.421767 accuracy 0.875\n",
      "Training in progress @ global_step 60900, g_loss 0.471273, d_loss 0.432928 accuracy 0.703125\n",
      "Training in progress @ global_step 60950, g_loss 0.475175, d_loss 0.425967 accuracy 0.828125\n",
      "Training in progress @ global_step 61000, g_loss 0.473512, d_loss 0.434804 accuracy 0.8125\n",
      "Training in progress @ global_step 61050, g_loss 0.472757, d_loss 0.427134 accuracy 0.734375\n",
      "Training in progress @ global_step 61100, g_loss 0.472507, d_loss 0.425182 accuracy 0.765625\n",
      "Training in progress @ global_step 61150, g_loss 0.473444, d_loss 0.42845 accuracy 0.78125\n",
      "Training in progress @ global_step 61200, g_loss 0.472531, d_loss 0.422601 accuracy 0.75\n",
      "Training in progress @ global_step 61250, g_loss 0.472924, d_loss 0.42513 accuracy 0.8125\n",
      "Training in progress @ global_step 61300, g_loss 0.467576, d_loss 0.420317 accuracy 0.703125\n",
      "Training in progress @ global_step 61350, g_loss 0.470173, d_loss 0.424555 accuracy 0.71875\n",
      "Training in progress @ global_step 61400, g_loss 0.473802, d_loss 0.42383 accuracy 0.828125\n",
      "Training in progress @ global_step 61450, g_loss 0.472947, d_loss 0.42515 accuracy 0.796875\n",
      "Training in progress @ global_step 61500, g_loss 0.476562, d_loss 0.427277 accuracy 0.84375\n",
      "Training in progress @ global_step 61550, g_loss 0.467762, d_loss 0.428744 accuracy 0.734375\n",
      "Training in progress @ global_step 61600, g_loss 0.467422, d_loss 0.421448 accuracy 0.71875\n",
      "Training in progress @ global_step 61650, g_loss 0.47696, d_loss 0.424672 accuracy 0.875\n",
      "Training in progress @ global_step 61700, g_loss 0.468429, d_loss 0.42262 accuracy 0.734375\n",
      "Training in progress @ global_step 61750, g_loss 0.468392, d_loss 0.423789 accuracy 0.6875\n",
      "Training in progress @ global_step 61800, g_loss 0.469004, d_loss 0.426814 accuracy 0.6875\n",
      "Training in progress @ global_step 61850, g_loss 0.472433, d_loss 0.425625 accuracy 0.71875\n",
      "Training in progress @ global_step 61900, g_loss 0.465179, d_loss 0.419839 accuracy 0.625\n",
      "Training in progress @ global_step 61950, g_loss 0.468539, d_loss 0.424903 accuracy 0.734375\n",
      "Training in progress @ global_step 62000, g_loss 0.469643, d_loss 0.425549 accuracy 0.75\n",
      "Training in progress @ global_step 62050, g_loss 0.462844, d_loss 0.431031 accuracy 0.609375\n",
      "Training in progress @ global_step 62100, g_loss 0.468955, d_loss 0.425148 accuracy 0.71875\n",
      "Training in progress @ global_step 62150, g_loss 0.46801, d_loss 0.425792 accuracy 0.703125\n",
      "Training in progress @ global_step 62200, g_loss 0.467288, d_loss 0.422168 accuracy 0.640625\n",
      "Training in progress @ global_step 62250, g_loss 0.469351, d_loss 0.424685 accuracy 0.734375\n",
      "Training in progress @ global_step 62300, g_loss 0.469973, d_loss 0.425156 accuracy 0.734375\n",
      "Training in progress @ global_step 62350, g_loss 0.467538, d_loss 0.426878 accuracy 0.671875\n",
      "Training in progress @ global_step 62400, g_loss 0.470391, d_loss 0.424782 accuracy 0.75\n",
      "Training in progress @ global_step 62450, g_loss 0.468533, d_loss 0.428062 accuracy 0.671875\n",
      "Training in progress @ global_step 62500, g_loss 0.466945, d_loss 0.427856 accuracy 0.6875\n",
      "Training in progress @ global_step 62550, g_loss 0.464075, d_loss 0.429669 accuracy 0.625\n",
      "Training in progress @ global_step 62600, g_loss 0.469075, d_loss 0.428868 accuracy 0.640625\n",
      "Training in progress @ global_step 62650, g_loss 0.467028, d_loss 0.42939 accuracy 0.65625\n",
      "Training in progress @ global_step 62700, g_loss 0.468715, d_loss 0.427795 accuracy 0.71875\n",
      "Training in progress @ global_step 62750, g_loss 0.468547, d_loss 0.429635 accuracy 0.71875\n",
      "Training in progress @ global_step 62800, g_loss 0.468234, d_loss 0.415023 accuracy 0.671875\n",
      "Training in progress @ global_step 62850, g_loss 0.471863, d_loss 0.425943 accuracy 0.765625\n",
      "Training in progress @ global_step 62900, g_loss 0.472291, d_loss 0.434005 accuracy 0.71875\n",
      "Training in progress @ global_step 62950, g_loss 0.471005, d_loss 0.428813 accuracy 0.765625\n",
      "Training in progress @ global_step 63000, g_loss 0.469268, d_loss 0.429304 accuracy 0.6875\n",
      "Training in progress @ global_step 63050, g_loss 0.470069, d_loss 0.416377 accuracy 0.75\n",
      "Training in progress @ global_step 63100, g_loss 0.473131, d_loss 0.433027 accuracy 0.828125\n",
      "Training in progress @ global_step 63150, g_loss 0.470787, d_loss 0.421928 accuracy 0.78125\n",
      "Training in progress @ global_step 63200, g_loss 0.471593, d_loss 0.426414 accuracy 0.78125\n",
      "Training in progress @ global_step 63250, g_loss 0.473428, d_loss 0.4204 accuracy 0.828125\n",
      "Training in progress @ global_step 63300, g_loss 0.475964, d_loss 0.42899 accuracy 0.90625\n",
      "Training in progress @ global_step 63350, g_loss 0.475135, d_loss 0.425876 accuracy 0.78125\n",
      "Training in progress @ global_step 63400, g_loss 0.473557, d_loss 0.417728 accuracy 0.828125\n",
      "Training in progress @ global_step 63450, g_loss 0.472255, d_loss 0.428698 accuracy 0.78125\n",
      "Training in progress @ global_step 63500, g_loss 0.475088, d_loss 0.435147 accuracy 0.84375\n",
      "Training in progress @ global_step 63550, g_loss 0.47191, d_loss 0.419143 accuracy 0.734375\n",
      "Training in progress @ global_step 63600, g_loss 0.473813, d_loss 0.430687 accuracy 0.828125\n",
      "Training in progress @ global_step 63650, g_loss 0.470977, d_loss 0.416053 accuracy 0.796875\n",
      "Training in progress @ global_step 63700, g_loss 0.469932, d_loss 0.426391 accuracy 0.828125\n",
      "Training in progress @ global_step 63750, g_loss 0.474338, d_loss 0.434619 accuracy 0.84375\n",
      "Training in progress @ global_step 63800, g_loss 0.462305, d_loss 0.426528 accuracy 0.6875\n",
      "Training in progress @ global_step 63850, g_loss 0.474317, d_loss 0.425983 accuracy 0.8125\n",
      "Training in progress @ global_step 63900, g_loss 0.471957, d_loss 0.424277 accuracy 0.84375\n",
      "Training in progress @ global_step 63950, g_loss 0.467532, d_loss 0.432012 accuracy 0.71875\n",
      "Training in progress @ global_step 64000, g_loss 0.465876, d_loss 0.43091 accuracy 0.78125\n",
      "Training in progress @ global_step 64050, g_loss 0.474057, d_loss 0.424417 accuracy 0.796875\n",
      "Training in progress @ global_step 64100, g_loss 0.469343, d_loss 0.42164 accuracy 0.78125\n",
      "Training in progress @ global_step 64150, g_loss 0.467118, d_loss 0.434664 accuracy 0.734375\n",
      "Training in progress @ global_step 64200, g_loss 0.460145, d_loss 0.418609 accuracy 0.703125\n",
      "Training in progress @ global_step 64250, g_loss 0.46916, d_loss 0.416172 accuracy 0.75\n",
      "Training in progress @ global_step 64300, g_loss 0.465185, d_loss 0.434727 accuracy 0.734375\n",
      "Training in progress @ global_step 64350, g_loss 0.46354, d_loss 0.421776 accuracy 0.734375\n",
      "Training in progress @ global_step 64400, g_loss 0.465294, d_loss 0.421393 accuracy 0.75\n",
      "Training in progress @ global_step 64450, g_loss 0.461758, d_loss 0.42622 accuracy 0.6875\n",
      "Training in progress @ global_step 64500, g_loss 0.468482, d_loss 0.42811 accuracy 0.703125\n",
      "Training in progress @ global_step 64550, g_loss 0.466262, d_loss 0.431309 accuracy 0.734375\n",
      "Training in progress @ global_step 64600, g_loss 0.463809, d_loss 0.429999 accuracy 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 64650, g_loss 0.467655, d_loss 0.436708 accuracy 0.734375\n",
      "Training in progress @ global_step 64700, g_loss 0.46882, d_loss 0.432733 accuracy 0.75\n",
      "Training in progress @ global_step 64750, g_loss 0.46209, d_loss 0.425656 accuracy 0.6875\n",
      "Training in progress @ global_step 64800, g_loss 0.451971, d_loss 0.425067 accuracy 0.609375\n",
      "Training in progress @ global_step 64850, g_loss 0.465913, d_loss 0.428636 accuracy 0.703125\n",
      "Training in progress @ global_step 64900, g_loss 0.465738, d_loss 0.426103 accuracy 0.671875\n",
      "Training in progress @ global_step 64950, g_loss 0.462386, d_loss 0.428202 accuracy 0.625\n",
      "Training in progress @ global_step 65000, g_loss 0.464644, d_loss 0.424183 accuracy 0.671875\n",
      "Training in progress @ global_step 65050, g_loss 0.457149, d_loss 0.420293 accuracy 0.65625\n",
      "Training in progress @ global_step 65100, g_loss 0.458445, d_loss 0.423372 accuracy 0.671875\n",
      "Training in progress @ global_step 65150, g_loss 0.464802, d_loss 0.421373 accuracy 0.703125\n",
      "Training in progress @ global_step 65200, g_loss 0.463008, d_loss 0.421269 accuracy 0.671875\n",
      "Training in progress @ global_step 65250, g_loss 0.465113, d_loss 0.422458 accuracy 0.703125\n",
      "Training in progress @ global_step 65300, g_loss 0.467455, d_loss 0.422875 accuracy 0.71875\n",
      "Training in progress @ global_step 65350, g_loss 0.468459, d_loss 0.419063 accuracy 0.75\n",
      "Training in progress @ global_step 65400, g_loss 0.46688, d_loss 0.424972 accuracy 0.6875\n",
      "Training in progress @ global_step 65450, g_loss 0.461715, d_loss 0.424474 accuracy 0.640625\n",
      "Training in progress @ global_step 65500, g_loss 0.467463, d_loss 0.420134 accuracy 0.703125\n",
      "Training in progress @ global_step 65550, g_loss 0.470752, d_loss 0.425762 accuracy 0.71875\n",
      "Training in progress @ global_step 65600, g_loss 0.464492, d_loss 0.424481 accuracy 0.6875\n",
      "Training in progress @ global_step 65650, g_loss 0.466697, d_loss 0.419741 accuracy 0.703125\n",
      "Training in progress @ global_step 65700, g_loss 0.471213, d_loss 0.419889 accuracy 0.78125\n",
      "Training in progress @ global_step 65750, g_loss 0.468167, d_loss 0.41565 accuracy 0.703125\n",
      "Training in progress @ global_step 65800, g_loss 0.464364, d_loss 0.419276 accuracy 0.703125\n",
      "Training in progress @ global_step 65850, g_loss 0.470326, d_loss 0.419222 accuracy 0.71875\n",
      "Training in progress @ global_step 65900, g_loss 0.466872, d_loss 0.420195 accuracy 0.65625\n",
      "Training in progress @ global_step 65950, g_loss 0.469711, d_loss 0.417331 accuracy 0.78125\n",
      "Training in progress @ global_step 66000, g_loss 0.468677, d_loss 0.421947 accuracy 0.734375\n",
      "Training in progress @ global_step 66050, g_loss 0.468897, d_loss 0.415826 accuracy 0.71875\n",
      "Training in progress @ global_step 66100, g_loss 0.464276, d_loss 0.41923 accuracy 0.703125\n",
      "Training in progress @ global_step 66150, g_loss 0.465344, d_loss 0.415586 accuracy 0.703125\n",
      "Training in progress @ global_step 66200, g_loss 0.466148, d_loss 0.414894 accuracy 0.703125\n",
      "Training in progress @ global_step 66250, g_loss 0.468015, d_loss 0.414518 accuracy 0.765625\n",
      "Training in progress @ global_step 66300, g_loss 0.469801, d_loss 0.420122 accuracy 0.765625\n",
      "Training in progress @ global_step 66350, g_loss 0.465307, d_loss 0.419217 accuracy 0.671875\n",
      "Training in progress @ global_step 66400, g_loss 0.467235, d_loss 0.422646 accuracy 0.703125\n",
      "Training in progress @ global_step 66450, g_loss 0.467257, d_loss 0.415638 accuracy 0.75\n",
      "Training in progress @ global_step 66500, g_loss 0.465493, d_loss 0.419591 accuracy 0.734375\n",
      "Training in progress @ global_step 66550, g_loss 0.474278, d_loss 0.422131 accuracy 0.859375\n",
      "Training in progress @ global_step 66600, g_loss 0.468352, d_loss 0.417354 accuracy 0.71875\n",
      "Training in progress @ global_step 66650, g_loss 0.46286, d_loss 0.416737 accuracy 0.71875\n",
      "Training in progress @ global_step 66700, g_loss 0.469905, d_loss 0.422465 accuracy 0.75\n",
      "Training in progress @ global_step 66750, g_loss 0.463467, d_loss 0.415769 accuracy 0.703125\n",
      "Training in progress @ global_step 66800, g_loss 0.464833, d_loss 0.419247 accuracy 0.6875\n",
      "Training in progress @ global_step 66850, g_loss 0.466659, d_loss 0.423259 accuracy 0.734375\n",
      "Training in progress @ global_step 66900, g_loss 0.460271, d_loss 0.420504 accuracy 0.6875\n",
      "Training in progress @ global_step 66950, g_loss 0.460339, d_loss 0.424093 accuracy 0.671875\n",
      "Training in progress @ global_step 67000, g_loss 0.46502, d_loss 0.422968 accuracy 0.703125\n",
      "Training in progress @ global_step 67050, g_loss 0.467577, d_loss 0.419248 accuracy 0.734375\n",
      "Training in progress @ global_step 67100, g_loss 0.457654, d_loss 0.417723 accuracy 0.609375\n",
      "Training in progress @ global_step 67150, g_loss 0.466119, d_loss 0.421915 accuracy 0.75\n",
      "Training in progress @ global_step 67200, g_loss 0.458939, d_loss 0.431584 accuracy 0.59375\n",
      "Training in progress @ global_step 67250, g_loss 0.462305, d_loss 0.416893 accuracy 0.75\n",
      "Training in progress @ global_step 67300, g_loss 0.461137, d_loss 0.421589 accuracy 0.640625\n",
      "Training in progress @ global_step 67350, g_loss 0.460721, d_loss 0.419732 accuracy 0.75\n",
      "Training in progress @ global_step 67400, g_loss 0.461515, d_loss 0.4232 accuracy 0.640625\n",
      "Training in progress @ global_step 67450, g_loss 0.453475, d_loss 0.424984 accuracy 0.625\n",
      "Training in progress @ global_step 67500, g_loss 0.452382, d_loss 0.418541 accuracy 0.609375\n",
      "Training in progress @ global_step 67550, g_loss 0.461165, d_loss 0.422628 accuracy 0.703125\n",
      "Training in progress @ global_step 67600, g_loss 0.457776, d_loss 0.420213 accuracy 0.640625\n",
      "Training in progress @ global_step 67650, g_loss 0.451044, d_loss 0.424667 accuracy 0.578125\n",
      "Training in progress @ global_step 67700, g_loss 0.458904, d_loss 0.425817 accuracy 0.65625\n",
      "Training in progress @ global_step 67750, g_loss 0.467815, d_loss 0.419791 accuracy 0.734375\n",
      "Training in progress @ global_step 67800, g_loss 0.460631, d_loss 0.431336 accuracy 0.625\n",
      "Training in progress @ global_step 67850, g_loss 0.457843, d_loss 0.422075 accuracy 0.65625\n",
      "Training in progress @ global_step 67900, g_loss 0.459337, d_loss 0.425349 accuracy 0.6875\n",
      "Training in progress @ global_step 67950, g_loss 0.451382, d_loss 0.423367 accuracy 0.578125\n",
      "Training in progress @ global_step 68000, g_loss 0.457411, d_loss 0.423301 accuracy 0.625\n",
      "Training in progress @ global_step 68050, g_loss 0.45636, d_loss 0.420017 accuracy 0.5625\n",
      "Training in progress @ global_step 68100, g_loss 0.459604, d_loss 0.422475 accuracy 0.671875\n",
      "Training in progress @ global_step 68150, g_loss 0.46313, d_loss 0.423831 accuracy 0.6875\n",
      "Training in progress @ global_step 68200, g_loss 0.461301, d_loss 0.428342 accuracy 0.625\n",
      "Training in progress @ global_step 68250, g_loss 0.460398, d_loss 0.425083 accuracy 0.625\n",
      "Training in progress @ global_step 68300, g_loss 0.46431, d_loss 0.423827 accuracy 0.65625\n",
      "Training in progress @ global_step 68350, g_loss 0.461929, d_loss 0.424668 accuracy 0.640625\n",
      "Training in progress @ global_step 68400, g_loss 0.464033, d_loss 0.424515 accuracy 0.609375\n",
      "Training in progress @ global_step 68450, g_loss 0.459759, d_loss 0.421706 accuracy 0.546875\n",
      "Training in progress @ global_step 68500, g_loss 0.4637, d_loss 0.427614 accuracy 0.609375\n",
      "Training in progress @ global_step 68550, g_loss 0.464818, d_loss 0.429056 accuracy 0.671875\n",
      "Training in progress @ global_step 68600, g_loss 0.46279, d_loss 0.430924 accuracy 0.59375\n",
      "Training in progress @ global_step 68650, g_loss 0.460787, d_loss 0.430331 accuracy 0.625\n",
      "Training in progress @ global_step 68700, g_loss 0.468024, d_loss 0.424413 accuracy 0.65625\n",
      "Training in progress @ global_step 68750, g_loss 0.462949, d_loss 0.419604 accuracy 0.59375\n",
      "Training in progress @ global_step 68800, g_loss 0.465029, d_loss 0.42284 accuracy 0.625\n",
      "Training in progress @ global_step 68850, g_loss 0.467142, d_loss 0.424731 accuracy 0.625\n",
      "Training in progress @ global_step 68900, g_loss 0.46619, d_loss 0.424187 accuracy 0.640625\n",
      "Training in progress @ global_step 68950, g_loss 0.468182, d_loss 0.427995 accuracy 0.625\n",
      "Training in progress @ global_step 69000, g_loss 0.468197, d_loss 0.429943 accuracy 0.65625\n",
      "Training in progress @ global_step 69050, g_loss 0.468746, d_loss 0.431019 accuracy 0.578125\n",
      "Training in progress @ global_step 69100, g_loss 0.467891, d_loss 0.427051 accuracy 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 69150, g_loss 0.471255, d_loss 0.427877 accuracy 0.703125\n",
      "Training in progress @ global_step 69200, g_loss 0.471174, d_loss 0.419479 accuracy 0.78125\n",
      "Training in progress @ global_step 69250, g_loss 0.472137, d_loss 0.425996 accuracy 0.765625\n",
      "Training in progress @ global_step 69300, g_loss 0.472424, d_loss 0.423481 accuracy 0.75\n",
      "Training in progress @ global_step 69350, g_loss 0.470906, d_loss 0.424983 accuracy 0.765625\n",
      "Training in progress @ global_step 69400, g_loss 0.472488, d_loss 0.431572 accuracy 0.765625\n",
      "Training in progress @ global_step 69450, g_loss 0.472552, d_loss 0.431889 accuracy 0.765625\n",
      "Training in progress @ global_step 69500, g_loss 0.476994, d_loss 0.430141 accuracy 0.890625\n",
      "Training in progress @ global_step 69550, g_loss 0.471413, d_loss 0.428067 accuracy 0.8125\n",
      "Training in progress @ global_step 69600, g_loss 0.474307, d_loss 0.422526 accuracy 0.890625\n",
      "Training in progress @ global_step 69650, g_loss 0.473115, d_loss 0.425924 accuracy 0.890625\n",
      "Training in progress @ global_step 69700, g_loss 0.474031, d_loss 0.434115 accuracy 0.859375\n",
      "Training in progress @ global_step 69750, g_loss 0.477725, d_loss 0.432535 accuracy 0.890625\n",
      "Training in progress @ global_step 69800, g_loss 0.477924, d_loss 0.434839 accuracy 0.90625\n",
      "Training in progress @ global_step 69850, g_loss 0.477838, d_loss 0.432606 accuracy 0.859375\n",
      "Training in progress @ global_step 69900, g_loss 0.476344, d_loss 0.4372 accuracy 0.890625\n",
      "Training in progress @ global_step 69950, g_loss 0.476036, d_loss 0.423849 accuracy 0.8125\n",
      "Training in progress @ global_step 70000, g_loss 0.47597, d_loss 0.420727 accuracy 0.828125\n",
      "Training in progress @ global_step 70050, g_loss 0.482686, d_loss 0.430233 accuracy 0.90625\n",
      "Training in progress @ global_step 70100, g_loss 0.475116, d_loss 0.422496 accuracy 0.859375\n",
      "Training in progress @ global_step 70150, g_loss 0.479288, d_loss 0.446054 accuracy 0.859375\n",
      "Training in progress @ global_step 70200, g_loss 0.474971, d_loss 0.431458 accuracy 0.796875\n",
      "Training in progress @ global_step 70250, g_loss 0.477606, d_loss 0.426179 accuracy 0.84375\n",
      "Training in progress @ global_step 70300, g_loss 0.475583, d_loss 0.426338 accuracy 0.765625\n",
      "Training in progress @ global_step 70350, g_loss 0.478777, d_loss 0.430857 accuracy 0.859375\n",
      "Training in progress @ global_step 70400, g_loss 0.47901, d_loss 0.43884 accuracy 0.8125\n",
      "Training in progress @ global_step 70450, g_loss 0.479708, d_loss 0.435509 accuracy 0.859375\n",
      "Training in progress @ global_step 70500, g_loss 0.481085, d_loss 0.434166 accuracy 0.84375\n",
      "Training in progress @ global_step 70550, g_loss 0.476907, d_loss 0.42649 accuracy 0.84375\n",
      "Training in progress @ global_step 70600, g_loss 0.476413, d_loss 0.425367 accuracy 0.796875\n",
      "Training in progress @ global_step 70650, g_loss 0.473778, d_loss 0.424795 accuracy 0.75\n",
      "Training in progress @ global_step 70700, g_loss 0.471357, d_loss 0.432091 accuracy 0.78125\n",
      "Training in progress @ global_step 70750, g_loss 0.478122, d_loss 0.421557 accuracy 0.84375\n",
      "Training in progress @ global_step 70800, g_loss 0.467832, d_loss 0.426086 accuracy 0.734375\n",
      "Training in progress @ global_step 70850, g_loss 0.466767, d_loss 0.431573 accuracy 0.6875\n",
      "Training in progress @ global_step 70900, g_loss 0.465737, d_loss 0.439195 accuracy 0.765625\n",
      "Training in progress @ global_step 70950, g_loss 0.47444, d_loss 0.429949 accuracy 0.765625\n",
      "Training in progress @ global_step 71000, g_loss 0.467772, d_loss 0.428653 accuracy 0.75\n",
      "Training in progress @ global_step 71050, g_loss 0.465726, d_loss 0.427516 accuracy 0.75\n",
      "Training in progress @ global_step 71100, g_loss 0.473599, d_loss 0.429323 accuracy 0.796875\n",
      "Training in progress @ global_step 71150, g_loss 0.471366, d_loss 0.428842 accuracy 0.796875\n",
      "Training in progress @ global_step 71200, g_loss 0.473204, d_loss 0.429831 accuracy 0.765625\n",
      "Training in progress @ global_step 71250, g_loss 0.475563, d_loss 0.430702 accuracy 0.78125\n",
      "Training in progress @ global_step 71300, g_loss 0.459834, d_loss 0.435362 accuracy 0.71875\n",
      "Training in progress @ global_step 71350, g_loss 0.467071, d_loss 0.432781 accuracy 0.734375\n",
      "Training in progress @ global_step 71400, g_loss 0.473865, d_loss 0.428378 accuracy 0.828125\n",
      "Training in progress @ global_step 71450, g_loss 0.465583, d_loss 0.431803 accuracy 0.78125\n",
      "Training in progress @ global_step 71500, g_loss 0.469773, d_loss 0.426025 accuracy 0.78125\n",
      "Training in progress @ global_step 71550, g_loss 0.463674, d_loss 0.438231 accuracy 0.6875\n",
      "Training in progress @ global_step 71600, g_loss 0.46732, d_loss 0.429827 accuracy 0.734375\n",
      "Training in progress @ global_step 71650, g_loss 0.468883, d_loss 0.436993 accuracy 0.8125\n",
      "Training in progress @ global_step 71700, g_loss 0.465441, d_loss 0.429434 accuracy 0.6875\n",
      "Training in progress @ global_step 71750, g_loss 0.469478, d_loss 0.427054 accuracy 0.78125\n",
      "Training in progress @ global_step 71800, g_loss 0.469116, d_loss 0.43029 accuracy 0.765625\n",
      "Training in progress @ global_step 71850, g_loss 0.473891, d_loss 0.437606 accuracy 0.796875\n",
      "Training in progress @ global_step 71900, g_loss 0.471132, d_loss 0.430304 accuracy 0.84375\n",
      "Training in progress @ global_step 71950, g_loss 0.463156, d_loss 0.434849 accuracy 0.71875\n",
      "Training in progress @ global_step 72000, g_loss 0.462654, d_loss 0.432517 accuracy 0.78125\n",
      "Training in progress @ global_step 72050, g_loss 0.470894, d_loss 0.434045 accuracy 0.78125\n",
      "Training in progress @ global_step 72100, g_loss 0.469452, d_loss 0.441801 accuracy 0.8125\n",
      "Training in progress @ global_step 72150, g_loss 0.477362, d_loss 0.431746 accuracy 0.90625\n",
      "Training in progress @ global_step 72200, g_loss 0.470232, d_loss 0.429849 accuracy 0.796875\n",
      "Training in progress @ global_step 72250, g_loss 0.46995, d_loss 0.431689 accuracy 0.796875\n",
      "Training in progress @ global_step 72300, g_loss 0.472058, d_loss 0.432986 accuracy 0.8125\n",
      "Training in progress @ global_step 72350, g_loss 0.467138, d_loss 0.427449 accuracy 0.765625\n",
      "Training in progress @ global_step 72400, g_loss 0.472338, d_loss 0.43236 accuracy 0.75\n",
      "Training in progress @ global_step 72450, g_loss 0.469012, d_loss 0.429617 accuracy 0.71875\n",
      "Training in progress @ global_step 72500, g_loss 0.465775, d_loss 0.431551 accuracy 0.6875\n",
      "Training in progress @ global_step 72550, g_loss 0.47666, d_loss 0.431126 accuracy 0.890625\n",
      "Training in progress @ global_step 72600, g_loss 0.470707, d_loss 0.433069 accuracy 0.71875\n",
      "Training in progress @ global_step 72650, g_loss 0.472634, d_loss 0.43261 accuracy 0.796875\n",
      "Training in progress @ global_step 72700, g_loss 0.472065, d_loss 0.427128 accuracy 0.78125\n",
      "Training in progress @ global_step 72750, g_loss 0.474082, d_loss 0.431238 accuracy 0.796875\n",
      "Training in progress @ global_step 72800, g_loss 0.475048, d_loss 0.427849 accuracy 0.859375\n",
      "Training in progress @ global_step 72850, g_loss 0.472855, d_loss 0.432916 accuracy 0.75\n",
      "Training in progress @ global_step 72900, g_loss 0.471694, d_loss 0.428469 accuracy 0.734375\n",
      "Training in progress @ global_step 72950, g_loss 0.474294, d_loss 0.424318 accuracy 0.765625\n",
      "Training in progress @ global_step 73000, g_loss 0.475395, d_loss 0.429333 accuracy 0.78125\n",
      "Training in progress @ global_step 73050, g_loss 0.468732, d_loss 0.426892 accuracy 0.703125\n",
      "Training in progress @ global_step 73100, g_loss 0.474777, d_loss 0.42799 accuracy 0.78125\n",
      "Training in progress @ global_step 73150, g_loss 0.475132, d_loss 0.426822 accuracy 0.8125\n",
      "Training in progress @ global_step 73200, g_loss 0.475204, d_loss 0.424584 accuracy 0.8125\n",
      "Training in progress @ global_step 73250, g_loss 0.474624, d_loss 0.434343 accuracy 0.84375\n",
      "Training in progress @ global_step 73300, g_loss 0.476106, d_loss 0.426789 accuracy 0.828125\n",
      "Training in progress @ global_step 73350, g_loss 0.47279, d_loss 0.424538 accuracy 0.8125\n",
      "Training in progress @ global_step 73400, g_loss 0.474316, d_loss 0.425438 accuracy 0.78125\n",
      "Training in progress @ global_step 73450, g_loss 0.480107, d_loss 0.426663 accuracy 0.875\n",
      "Training in progress @ global_step 73500, g_loss 0.47094, d_loss 0.420095 accuracy 0.734375\n",
      "Training in progress @ global_step 73550, g_loss 0.479368, d_loss 0.427999 accuracy 0.84375\n",
      "Training in progress @ global_step 73600, g_loss 0.474327, d_loss 0.427245 accuracy 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 73650, g_loss 0.480202, d_loss 0.422169 accuracy 0.828125\n",
      "Training in progress @ global_step 73700, g_loss 0.47677, d_loss 0.425587 accuracy 0.8125\n",
      "Training in progress @ global_step 73750, g_loss 0.481478, d_loss 0.425613 accuracy 0.875\n",
      "Training in progress @ global_step 73800, g_loss 0.485301, d_loss 0.430734 accuracy 0.890625\n",
      "Training in progress @ global_step 73850, g_loss 0.473038, d_loss 0.420355 accuracy 0.796875\n",
      "Training in progress @ global_step 73900, g_loss 0.477887, d_loss 0.423348 accuracy 0.84375\n",
      "Training in progress @ global_step 73950, g_loss 0.471204, d_loss 0.429905 accuracy 0.734375\n",
      "Training in progress @ global_step 74000, g_loss 0.479198, d_loss 0.423016 accuracy 0.8125\n",
      "Training in progress @ global_step 74050, g_loss 0.479408, d_loss 0.42464 accuracy 0.8125\n",
      "Training in progress @ global_step 74100, g_loss 0.481401, d_loss 0.424765 accuracy 0.8125\n",
      "Training in progress @ global_step 74150, g_loss 0.479702, d_loss 0.419684 accuracy 0.78125\n",
      "Training in progress @ global_step 74200, g_loss 0.482438, d_loss 0.424798 accuracy 0.859375\n",
      "Training in progress @ global_step 74250, g_loss 0.477455, d_loss 0.428763 accuracy 0.796875\n",
      "Training in progress @ global_step 74300, g_loss 0.475835, d_loss 0.42535 accuracy 0.765625\n",
      "Training in progress @ global_step 74350, g_loss 0.480559, d_loss 0.426011 accuracy 0.8125\n",
      "Training in progress @ global_step 74400, g_loss 0.473325, d_loss 0.428426 accuracy 0.75\n",
      "Training in progress @ global_step 74450, g_loss 0.475455, d_loss 0.424992 accuracy 0.8125\n",
      "Training in progress @ global_step 74500, g_loss 0.470984, d_loss 0.425801 accuracy 0.765625\n",
      "Training in progress @ global_step 74550, g_loss 0.474358, d_loss 0.429719 accuracy 0.75\n",
      "Training in progress @ global_step 74600, g_loss 0.470925, d_loss 0.423291 accuracy 0.71875\n",
      "Training in progress @ global_step 74650, g_loss 0.472574, d_loss 0.428067 accuracy 0.765625\n",
      "Training in progress @ global_step 74700, g_loss 0.473439, d_loss 0.423485 accuracy 0.765625\n",
      "Training in progress @ global_step 74750, g_loss 0.475774, d_loss 0.424434 accuracy 0.765625\n",
      "Training in progress @ global_step 74800, g_loss 0.468706, d_loss 0.423738 accuracy 0.65625\n",
      "Training in progress @ global_step 74850, g_loss 0.470898, d_loss 0.432293 accuracy 0.75\n",
      "Training in progress @ global_step 74900, g_loss 0.479263, d_loss 0.421127 accuracy 0.78125\n",
      "Training in progress @ global_step 74950, g_loss 0.476562, d_loss 0.43325 accuracy 0.78125\n",
      "Training in progress @ global_step 75000, g_loss 0.473988, d_loss 0.431719 accuracy 0.75\n",
      "Training in progress @ global_step 75050, g_loss 0.470378, d_loss 0.430681 accuracy 0.6875\n",
      "Training in progress @ global_step 75100, g_loss 0.476994, d_loss 0.425887 accuracy 0.796875\n",
      "Training in progress @ global_step 75150, g_loss 0.470894, d_loss 0.432602 accuracy 0.703125\n",
      "Training in progress @ global_step 75200, g_loss 0.470379, d_loss 0.428878 accuracy 0.6875\n",
      "Training in progress @ global_step 75250, g_loss 0.473322, d_loss 0.429813 accuracy 0.71875\n",
      "Training in progress @ global_step 75300, g_loss 0.473799, d_loss 0.428337 accuracy 0.765625\n",
      "Training in progress @ global_step 75350, g_loss 0.47604, d_loss 0.427438 accuracy 0.765625\n",
      "Training in progress @ global_step 75400, g_loss 0.476362, d_loss 0.429906 accuracy 0.765625\n",
      "Training in progress @ global_step 75450, g_loss 0.478124, d_loss 0.430833 accuracy 0.796875\n",
      "Training in progress @ global_step 75500, g_loss 0.479104, d_loss 0.428578 accuracy 0.796875\n",
      "Training in progress @ global_step 75550, g_loss 0.474084, d_loss 0.432165 accuracy 0.734375\n",
      "Training in progress @ global_step 75600, g_loss 0.476674, d_loss 0.434994 accuracy 0.8125\n",
      "Training in progress @ global_step 75650, g_loss 0.479294, d_loss 0.433934 accuracy 0.859375\n",
      "Training in progress @ global_step 75700, g_loss 0.479144, d_loss 0.430336 accuracy 0.796875\n",
      "Training in progress @ global_step 75750, g_loss 0.474185, d_loss 0.433593 accuracy 0.734375\n",
      "Training in progress @ global_step 75800, g_loss 0.475458, d_loss 0.427218 accuracy 0.71875\n",
      "Training in progress @ global_step 75850, g_loss 0.475557, d_loss 0.435479 accuracy 0.75\n",
      "Training in progress @ global_step 75900, g_loss 0.47604, d_loss 0.429575 accuracy 0.78125\n",
      "Training in progress @ global_step 75950, g_loss 0.473707, d_loss 0.434651 accuracy 0.75\n",
      "Training in progress @ global_step 76000, g_loss 0.478856, d_loss 0.432838 accuracy 0.828125\n",
      "Training in progress @ global_step 76050, g_loss 0.478675, d_loss 0.434995 accuracy 0.8125\n",
      "Training in progress @ global_step 76100, g_loss 0.474622, d_loss 0.435739 accuracy 0.765625\n",
      "Training in progress @ global_step 76150, g_loss 0.477773, d_loss 0.439329 accuracy 0.859375\n",
      "Training in progress @ global_step 76200, g_loss 0.479121, d_loss 0.434765 accuracy 0.84375\n",
      "Training in progress @ global_step 76250, g_loss 0.478061, d_loss 0.43784 accuracy 0.828125\n",
      "Training in progress @ global_step 76300, g_loss 0.478142, d_loss 0.432446 accuracy 0.84375\n",
      "Training in progress @ global_step 76350, g_loss 0.479617, d_loss 0.429538 accuracy 0.875\n",
      "Training in progress @ global_step 76400, g_loss 0.479017, d_loss 0.436818 accuracy 0.859375\n",
      "Training in progress @ global_step 76450, g_loss 0.478345, d_loss 0.432057 accuracy 0.828125\n",
      "Training in progress @ global_step 76500, g_loss 0.477992, d_loss 0.42815 accuracy 0.921875\n",
      "Training in progress @ global_step 76550, g_loss 0.478349, d_loss 0.433492 accuracy 0.859375\n",
      "Training in progress @ global_step 76600, g_loss 0.479628, d_loss 0.435883 accuracy 0.9375\n",
      "Training in progress @ global_step 76650, g_loss 0.481314, d_loss 0.437438 accuracy 0.96875\n",
      "Training in progress @ global_step 76700, g_loss 0.480324, d_loss 0.438011 accuracy 0.9375\n",
      "Training in progress @ global_step 76750, g_loss 0.479946, d_loss 0.434706 accuracy 0.953125\n",
      "Training in progress @ global_step 76800, g_loss 0.480737, d_loss 0.433727 accuracy 0.96875\n",
      "Training in progress @ global_step 76850, g_loss 0.481967, d_loss 0.438973 accuracy 0.984375\n",
      "Training in progress @ global_step 76900, g_loss 0.481065, d_loss 0.436572 accuracy 0.96875\n",
      "Training in progress @ global_step 76950, g_loss 0.481707, d_loss 0.429262 accuracy 0.953125\n",
      "Training in progress @ global_step 77000, g_loss 0.480814, d_loss 0.430391 accuracy 0.9375\n",
      "Training in progress @ global_step 77050, g_loss 0.482854, d_loss 0.442105 accuracy 0.984375\n",
      "Training in progress @ global_step 77100, g_loss 0.482493, d_loss 0.436541 accuracy 0.984375\n",
      "Training in progress @ global_step 77150, g_loss 0.481854, d_loss 0.429568 accuracy 0.953125\n",
      "Training in progress @ global_step 77200, g_loss 0.483172, d_loss 0.435502 accuracy 0.96875\n",
      "Training in progress @ global_step 77250, g_loss 0.482332, d_loss 0.431081 accuracy 0.953125\n",
      "Training in progress @ global_step 77300, g_loss 0.482567, d_loss 0.433472 accuracy 0.921875\n",
      "Training in progress @ global_step 77350, g_loss 0.481473, d_loss 0.435246 accuracy 0.890625\n",
      "Training in progress @ global_step 77400, g_loss 0.482194, d_loss 0.434112 accuracy 0.90625\n",
      "Training in progress @ global_step 77450, g_loss 0.481504, d_loss 0.436861 accuracy 0.890625\n",
      "Training in progress @ global_step 77500, g_loss 0.483921, d_loss 0.436628 accuracy 0.921875\n",
      "Training in progress @ global_step 77550, g_loss 0.486521, d_loss 0.442528 accuracy 0.96875\n",
      "Training in progress @ global_step 77600, g_loss 0.485776, d_loss 0.432251 accuracy 0.96875\n",
      "Training in progress @ global_step 77650, g_loss 0.4828, d_loss 0.432719 accuracy 0.890625\n",
      "Training in progress @ global_step 77700, g_loss 0.483519, d_loss 0.442391 accuracy 0.921875\n",
      "Training in progress @ global_step 77750, g_loss 0.481921, d_loss 0.443557 accuracy 0.890625\n",
      "Training in progress @ global_step 77800, g_loss 0.487776, d_loss 0.437472 accuracy 0.96875\n",
      "Training in progress @ global_step 77850, g_loss 0.485008, d_loss 0.444784 accuracy 0.90625\n",
      "Training in progress @ global_step 77900, g_loss 0.483935, d_loss 0.43942 accuracy 0.90625\n",
      "Training in progress @ global_step 77950, g_loss 0.484523, d_loss 0.434492 accuracy 0.9375\n",
      "Training in progress @ global_step 78000, g_loss 0.48662, d_loss 0.439237 accuracy 0.90625\n",
      "Training in progress @ global_step 78050, g_loss 0.48386, d_loss 0.445546 accuracy 0.890625\n",
      "Training in progress @ global_step 78100, g_loss 0.485994, d_loss 0.438489 accuracy 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 78150, g_loss 0.482966, d_loss 0.436304 accuracy 0.890625\n",
      "Training in progress @ global_step 78200, g_loss 0.485372, d_loss 0.432638 accuracy 0.921875\n",
      "Training in progress @ global_step 78250, g_loss 0.485284, d_loss 0.437572 accuracy 0.953125\n",
      "Training in progress @ global_step 78300, g_loss 0.480076, d_loss 0.443876 accuracy 0.828125\n",
      "Training in progress @ global_step 78350, g_loss 0.480335, d_loss 0.442312 accuracy 0.859375\n",
      "Training in progress @ global_step 78400, g_loss 0.479806, d_loss 0.435576 accuracy 0.875\n",
      "Training in progress @ global_step 78450, g_loss 0.483142, d_loss 0.443739 accuracy 0.875\n",
      "Training in progress @ global_step 78500, g_loss 0.486607, d_loss 0.444701 accuracy 0.953125\n",
      "Training in progress @ global_step 78550, g_loss 0.487405, d_loss 0.441932 accuracy 0.953125\n",
      "Training in progress @ global_step 78600, g_loss 0.481435, d_loss 0.439531 accuracy 0.859375\n",
      "Training in progress @ global_step 78650, g_loss 0.48183, d_loss 0.429638 accuracy 0.859375\n",
      "Training in progress @ global_step 78700, g_loss 0.483612, d_loss 0.433893 accuracy 0.921875\n",
      "Training in progress @ global_step 78750, g_loss 0.483284, d_loss 0.437166 accuracy 0.90625\n",
      "Training in progress @ global_step 78800, g_loss 0.481924, d_loss 0.440817 accuracy 0.84375\n",
      "Training in progress @ global_step 78850, g_loss 0.486359, d_loss 0.436659 accuracy 0.96875\n",
      "Training in progress @ global_step 78900, g_loss 0.481613, d_loss 0.438509 accuracy 0.875\n",
      "Training in progress @ global_step 78950, g_loss 0.483348, d_loss 0.440005 accuracy 0.890625\n",
      "Training in progress @ global_step 79000, g_loss 0.481272, d_loss 0.436787 accuracy 0.84375\n",
      "Training in progress @ global_step 79050, g_loss 0.482058, d_loss 0.439124 accuracy 0.875\n",
      "Training in progress @ global_step 79100, g_loss 0.482461, d_loss 0.441413 accuracy 0.9375\n",
      "Training in progress @ global_step 79150, g_loss 0.483572, d_loss 0.445224 accuracy 0.90625\n",
      "Training in progress @ global_step 79200, g_loss 0.48254, d_loss 0.441833 accuracy 0.921875\n",
      "Training in progress @ global_step 79250, g_loss 0.481226, d_loss 0.435339 accuracy 0.890625\n",
      "Training in progress @ global_step 79300, g_loss 0.482865, d_loss 0.435697 accuracy 0.96875\n",
      "Training in progress @ global_step 79350, g_loss 0.484842, d_loss 0.438401 accuracy 0.90625\n",
      "Training in progress @ global_step 79400, g_loss 0.480528, d_loss 0.437717 accuracy 0.921875\n",
      "Training in progress @ global_step 79450, g_loss 0.483209, d_loss 0.436731 accuracy 0.9375\n",
      "Training in progress @ global_step 79500, g_loss 0.483494, d_loss 0.441849 accuracy 0.953125\n",
      "Training in progress @ global_step 79550, g_loss 0.480786, d_loss 0.439081 accuracy 0.875\n",
      "Training in progress @ global_step 79600, g_loss 0.481867, d_loss 0.435507 accuracy 0.9375\n",
      "Training in progress @ global_step 79650, g_loss 0.483168, d_loss 0.439242 accuracy 0.96875\n",
      "Training in progress @ global_step 79700, g_loss 0.482179, d_loss 0.436721 accuracy 0.9375\n",
      "Training in progress @ global_step 79750, g_loss 0.480849, d_loss 0.439413 accuracy 0.921875\n",
      "Training in progress @ global_step 79800, g_loss 0.48054, d_loss 0.438439 accuracy 0.921875\n",
      "Training in progress @ global_step 79850, g_loss 0.480312, d_loss 0.431993 accuracy 0.90625\n",
      "Training in progress @ global_step 79900, g_loss 0.482707, d_loss 0.433512 accuracy 0.984375\n",
      "Training in progress @ global_step 79950, g_loss 0.482792, d_loss 0.433522 accuracy 0.96875\n",
      "Training in progress @ global_step 80000, g_loss 0.481734, d_loss 0.437754 accuracy 0.984375\n",
      "Training in progress @ global_step 80050, g_loss 0.48107, d_loss 0.438841 accuracy 0.953125\n",
      "Training in progress @ global_step 80100, g_loss 0.481478, d_loss 0.439618 accuracy 0.921875\n",
      "Training in progress @ global_step 80150, g_loss 0.481354, d_loss 0.442183 accuracy 0.921875\n",
      "Training in progress @ global_step 80200, g_loss 0.481006, d_loss 0.435045 accuracy 0.9375\n",
      "Training in progress @ global_step 80250, g_loss 0.481491, d_loss 0.429723 accuracy 0.953125\n",
      "Training in progress @ global_step 80300, g_loss 0.481649, d_loss 0.440395 accuracy 0.96875\n",
      "Training in progress @ global_step 80350, g_loss 0.481919, d_loss 0.428153 accuracy 0.9375\n",
      "Training in progress @ global_step 80400, g_loss 0.481816, d_loss 0.431684 accuracy 0.9375\n",
      "Training in progress @ global_step 80450, g_loss 0.481997, d_loss 0.435926 accuracy 0.9375\n",
      "Training in progress @ global_step 80500, g_loss 0.484008, d_loss 0.433637 accuracy 0.96875\n",
      "Training in progress @ global_step 80550, g_loss 0.484704, d_loss 0.437597 accuracy 0.9375\n",
      "Training in progress @ global_step 80600, g_loss 0.480356, d_loss 0.434813 accuracy 0.890625\n",
      "Training in progress @ global_step 80650, g_loss 0.480771, d_loss 0.435947 accuracy 0.890625\n",
      "Training in progress @ global_step 80700, g_loss 0.486076, d_loss 0.432018 accuracy 0.96875\n",
      "Training in progress @ global_step 80750, g_loss 0.482967, d_loss 0.429412 accuracy 0.953125\n",
      "Training in progress @ global_step 80800, g_loss 0.484068, d_loss 0.431322 accuracy 0.9375\n",
      "Training in progress @ global_step 80850, g_loss 0.484164, d_loss 0.442363 accuracy 0.9375\n",
      "Training in progress @ global_step 80900, g_loss 0.482068, d_loss 0.439786 accuracy 0.890625\n",
      "Training in progress @ global_step 80950, g_loss 0.482545, d_loss 0.440191 accuracy 0.890625\n",
      "Training in progress @ global_step 81000, g_loss 0.484297, d_loss 0.437327 accuracy 0.9375\n",
      "Training in progress @ global_step 81050, g_loss 0.482347, d_loss 0.432047 accuracy 0.921875\n",
      "Training in progress @ global_step 81100, g_loss 0.483277, d_loss 0.43651 accuracy 0.9375\n",
      "Training in progress @ global_step 81150, g_loss 0.479758, d_loss 0.44197 accuracy 0.859375\n",
      "Training in progress @ global_step 81200, g_loss 0.483716, d_loss 0.440128 accuracy 0.953125\n",
      "Training in progress @ global_step 81250, g_loss 0.483205, d_loss 0.432289 accuracy 0.96875\n",
      "Training in progress @ global_step 81300, g_loss 0.483179, d_loss 0.43732 accuracy 0.9375\n",
      "Training in progress @ global_step 81350, g_loss 0.483439, d_loss 0.443952 accuracy 0.984375\n",
      "Training in progress @ global_step 81400, g_loss 0.485463, d_loss 0.439108 accuracy 0.953125\n",
      "Training in progress @ global_step 81450, g_loss 0.483502, d_loss 0.437014 accuracy 0.90625\n",
      "Training in progress @ global_step 81500, g_loss 0.48121, d_loss 0.436447 accuracy 0.921875\n",
      "Training in progress @ global_step 81550, g_loss 0.483666, d_loss 0.43481 accuracy 0.921875\n",
      "Training in progress @ global_step 81600, g_loss 0.483114, d_loss 0.436795 accuracy 0.9375\n",
      "Training in progress @ global_step 81650, g_loss 0.486765, d_loss 0.439543 accuracy 0.9375\n",
      "Training in progress @ global_step 81700, g_loss 0.480895, d_loss 0.438617 accuracy 0.921875\n",
      "Training in progress @ global_step 81750, g_loss 0.483877, d_loss 0.439671 accuracy 0.9375\n",
      "Training in progress @ global_step 81800, g_loss 0.482852, d_loss 0.440234 accuracy 0.921875\n",
      "Training in progress @ global_step 81850, g_loss 0.481744, d_loss 0.435522 accuracy 0.90625\n",
      "Training in progress @ global_step 81900, g_loss 0.483879, d_loss 0.436647 accuracy 0.9375\n",
      "Training in progress @ global_step 81950, g_loss 0.486522, d_loss 0.437498 accuracy 0.96875\n",
      "Training in progress @ global_step 82000, g_loss 0.483826, d_loss 0.436783 accuracy 0.9375\n",
      "Training in progress @ global_step 82050, g_loss 0.480129, d_loss 0.435749 accuracy 0.90625\n",
      "Training in progress @ global_step 82100, g_loss 0.486624, d_loss 0.438204 accuracy 0.984375\n",
      "Training in progress @ global_step 82150, g_loss 0.482226, d_loss 0.441019 accuracy 0.921875\n",
      "Training in progress @ global_step 82200, g_loss 0.484123, d_loss 0.435614 accuracy 0.953125\n",
      "Training in progress @ global_step 82250, g_loss 0.483986, d_loss 0.439148 accuracy 0.921875\n",
      "Training in progress @ global_step 82300, g_loss 0.484826, d_loss 0.437948 accuracy 0.953125\n",
      "Training in progress @ global_step 82350, g_loss 0.484183, d_loss 0.432355 accuracy 0.9375\n",
      "Training in progress @ global_step 82400, g_loss 0.484369, d_loss 0.438968 accuracy 0.9375\n",
      "Training in progress @ global_step 82450, g_loss 0.483343, d_loss 0.446906 accuracy 0.890625\n",
      "Training in progress @ global_step 82500, g_loss 0.481297, d_loss 0.437107 accuracy 0.875\n",
      "Training in progress @ global_step 82550, g_loss 0.481978, d_loss 0.444321 accuracy 0.953125\n",
      "Training in progress @ global_step 82600, g_loss 0.484021, d_loss 0.43852 accuracy 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 82650, g_loss 0.482863, d_loss 0.444233 accuracy 0.90625\n",
      "Training in progress @ global_step 82700, g_loss 0.484222, d_loss 0.437595 accuracy 0.890625\n",
      "Training in progress @ global_step 82750, g_loss 0.483897, d_loss 0.437877 accuracy 0.96875\n",
      "Training in progress @ global_step 82800, g_loss 0.483812, d_loss 0.444605 accuracy 0.953125\n",
      "Training in progress @ global_step 82850, g_loss 0.483606, d_loss 0.438346 accuracy 0.9375\n",
      "Training in progress @ global_step 82900, g_loss 0.482051, d_loss 0.440017 accuracy 0.875\n",
      "Training in progress @ global_step 82950, g_loss 0.485461, d_loss 0.435191 accuracy 0.984375\n",
      "Training in progress @ global_step 83000, g_loss 0.482268, d_loss 0.435646 accuracy 0.875\n",
      "Training in progress @ global_step 83050, g_loss 0.483154, d_loss 0.442019 accuracy 0.90625\n",
      "Training in progress @ global_step 83100, g_loss 0.485385, d_loss 0.439037 accuracy 0.96875\n",
      "Training in progress @ global_step 83150, g_loss 0.485495, d_loss 0.441437 accuracy 0.96875\n",
      "Training in progress @ global_step 83200, g_loss 0.482864, d_loss 0.440133 accuracy 0.9375\n",
      "Training in progress @ global_step 83250, g_loss 0.484263, d_loss 0.443203 accuracy 0.921875\n",
      "Training in progress @ global_step 83300, g_loss 0.48541, d_loss 0.439984 accuracy 0.90625\n",
      "Training in progress @ global_step 83350, g_loss 0.487029, d_loss 0.436074 accuracy 0.984375\n",
      "Training in progress @ global_step 83400, g_loss 0.484762, d_loss 0.441429 accuracy 0.9375\n",
      "Training in progress @ global_step 83450, g_loss 0.485924, d_loss 0.436688 accuracy 0.921875\n",
      "Training in progress @ global_step 83500, g_loss 0.485473, d_loss 0.44289 accuracy 0.9375\n",
      "Training in progress @ global_step 83550, g_loss 0.483348, d_loss 0.440191 accuracy 0.890625\n",
      "Training in progress @ global_step 83600, g_loss 0.483701, d_loss 0.440483 accuracy 0.953125\n",
      "Training in progress @ global_step 83650, g_loss 0.485156, d_loss 0.44517 accuracy 0.921875\n",
      "Training in progress @ global_step 83700, g_loss 0.482791, d_loss 0.432145 accuracy 0.9375\n",
      "Training in progress @ global_step 83750, g_loss 0.484931, d_loss 0.443544 accuracy 0.953125\n",
      "Training in progress @ global_step 83800, g_loss 0.482463, d_loss 0.440169 accuracy 0.90625\n",
      "Training in progress @ global_step 83850, g_loss 0.486948, d_loss 0.435288 accuracy 1\n",
      "Training in progress @ global_step 83900, g_loss 0.484416, d_loss 0.43694 accuracy 0.9375\n",
      "Training in progress @ global_step 83950, g_loss 0.48547, d_loss 0.441212 accuracy 0.953125\n",
      "Training in progress @ global_step 84000, g_loss 0.485203, d_loss 0.445328 accuracy 0.9375\n",
      "Training in progress @ global_step 84050, g_loss 0.484677, d_loss 0.437519 accuracy 0.96875\n",
      "Training in progress @ global_step 84100, g_loss 0.485393, d_loss 0.439991 accuracy 0.96875\n",
      "Training in progress @ global_step 84150, g_loss 0.482602, d_loss 0.438182 accuracy 0.875\n",
      "Training in progress @ global_step 84200, g_loss 0.483897, d_loss 0.435521 accuracy 0.90625\n",
      "Training in progress @ global_step 84250, g_loss 0.485655, d_loss 0.434552 accuracy 0.96875\n",
      "Training in progress @ global_step 84300, g_loss 0.482145, d_loss 0.435528 accuracy 0.890625\n",
      "Training in progress @ global_step 84350, g_loss 0.486775, d_loss 0.439565 accuracy 0.984375\n",
      "Training in progress @ global_step 84400, g_loss 0.484808, d_loss 0.441132 accuracy 0.953125\n",
      "Training in progress @ global_step 84450, g_loss 0.483182, d_loss 0.437022 accuracy 0.90625\n",
      "Training in progress @ global_step 84500, g_loss 0.483432, d_loss 0.432959 accuracy 0.96875\n",
      "Training in progress @ global_step 84550, g_loss 0.482335, d_loss 0.44088 accuracy 0.90625\n",
      "Training in progress @ global_step 84600, g_loss 0.484823, d_loss 0.435325 accuracy 0.96875\n",
      "Training in progress @ global_step 84650, g_loss 0.485645, d_loss 0.441483 accuracy 0.984375\n",
      "Training in progress @ global_step 84700, g_loss 0.482163, d_loss 0.441156 accuracy 0.890625\n",
      "Training in progress @ global_step 84750, g_loss 0.48391, d_loss 0.437769 accuracy 0.953125\n",
      "Training in progress @ global_step 84800, g_loss 0.481141, d_loss 0.436957 accuracy 0.875\n",
      "Training in progress @ global_step 84850, g_loss 0.483393, d_loss 0.430604 accuracy 0.96875\n",
      "Training in progress @ global_step 84900, g_loss 0.48215, d_loss 0.439031 accuracy 0.921875\n",
      "Training in progress @ global_step 84950, g_loss 0.484033, d_loss 0.445929 accuracy 0.921875\n",
      "Training in progress @ global_step 85000, g_loss 0.483636, d_loss 0.444623 accuracy 1\n",
      "Training in progress @ global_step 85050, g_loss 0.483151, d_loss 0.439484 accuracy 0.921875\n",
      "Training in progress @ global_step 85100, g_loss 0.481722, d_loss 0.435518 accuracy 0.90625\n",
      "Training in progress @ global_step 85150, g_loss 0.483887, d_loss 0.432507 accuracy 0.921875\n",
      "Training in progress @ global_step 85200, g_loss 0.484392, d_loss 0.43748 accuracy 0.9375\n",
      "Training in progress @ global_step 85250, g_loss 0.482402, d_loss 0.439409 accuracy 0.953125\n",
      "Training in progress @ global_step 85300, g_loss 0.484873, d_loss 0.440521 accuracy 0.953125\n",
      "Training in progress @ global_step 85350, g_loss 0.481768, d_loss 0.445243 accuracy 0.90625\n",
      "Training in progress @ global_step 85400, g_loss 0.481803, d_loss 0.439022 accuracy 0.890625\n",
      "Training in progress @ global_step 85450, g_loss 0.48394, d_loss 0.439358 accuracy 0.9375\n",
      "Training in progress @ global_step 85500, g_loss 0.481125, d_loss 0.438532 accuracy 0.875\n",
      "Training in progress @ global_step 85550, g_loss 0.478083, d_loss 0.437014 accuracy 0.859375\n",
      "Training in progress @ global_step 85600, g_loss 0.483373, d_loss 0.438619 accuracy 0.953125\n",
      "Training in progress @ global_step 85650, g_loss 0.478997, d_loss 0.440942 accuracy 0.828125\n",
      "Training in progress @ global_step 85700, g_loss 0.479559, d_loss 0.442497 accuracy 0.828125\n",
      "Training in progress @ global_step 85750, g_loss 0.480192, d_loss 0.441572 accuracy 0.859375\n",
      "Training in progress @ global_step 85800, g_loss 0.479549, d_loss 0.437183 accuracy 0.875\n",
      "Training in progress @ global_step 85850, g_loss 0.481587, d_loss 0.437176 accuracy 0.90625\n",
      "Training in progress @ global_step 85900, g_loss 0.480022, d_loss 0.436321 accuracy 0.84375\n",
      "Training in progress @ global_step 85950, g_loss 0.482576, d_loss 0.439122 accuracy 0.890625\n",
      "Training in progress @ global_step 86000, g_loss 0.481492, d_loss 0.436115 accuracy 0.90625\n",
      "Training in progress @ global_step 86050, g_loss 0.481455, d_loss 0.444583 accuracy 0.890625\n",
      "Training in progress @ global_step 86100, g_loss 0.484538, d_loss 0.438847 accuracy 0.953125\n",
      "Training in progress @ global_step 86150, g_loss 0.480837, d_loss 0.431411 accuracy 0.890625\n",
      "Training in progress @ global_step 86200, g_loss 0.47895, d_loss 0.442499 accuracy 0.828125\n",
      "Training in progress @ global_step 86250, g_loss 0.482591, d_loss 0.443878 accuracy 0.9375\n",
      "Training in progress @ global_step 86300, g_loss 0.48301, d_loss 0.441399 accuracy 0.921875\n",
      "Training in progress @ global_step 86350, g_loss 0.483198, d_loss 0.438268 accuracy 0.9375\n",
      "Training in progress @ global_step 86400, g_loss 0.482006, d_loss 0.437658 accuracy 0.875\n",
      "Training in progress @ global_step 86450, g_loss 0.481279, d_loss 0.437071 accuracy 0.84375\n",
      "Training in progress @ global_step 86500, g_loss 0.482735, d_loss 0.439082 accuracy 0.90625\n",
      "Training in progress @ global_step 86550, g_loss 0.482765, d_loss 0.441261 accuracy 0.921875\n",
      "Training in progress @ global_step 86600, g_loss 0.480655, d_loss 0.436108 accuracy 0.859375\n",
      "Training in progress @ global_step 86650, g_loss 0.480621, d_loss 0.440843 accuracy 0.84375\n",
      "Training in progress @ global_step 86700, g_loss 0.482536, d_loss 0.440808 accuracy 0.9375\n",
      "Training in progress @ global_step 86750, g_loss 0.482859, d_loss 0.441271 accuracy 0.9375\n",
      "Training in progress @ global_step 86800, g_loss 0.481698, d_loss 0.431913 accuracy 0.90625\n",
      "Training in progress @ global_step 86850, g_loss 0.481556, d_loss 0.441336 accuracy 0.859375\n",
      "Training in progress @ global_step 86900, g_loss 0.481276, d_loss 0.439494 accuracy 0.859375\n",
      "Training in progress @ global_step 86950, g_loss 0.479884, d_loss 0.43975 accuracy 0.890625\n",
      "Training in progress @ global_step 87000, g_loss 0.482508, d_loss 0.433211 accuracy 0.921875\n",
      "Training in progress @ global_step 87050, g_loss 0.481733, d_loss 0.43877 accuracy 0.921875\n",
      "Training in progress @ global_step 87100, g_loss 0.478266, d_loss 0.436586 accuracy 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 87150, g_loss 0.480061, d_loss 0.43718 accuracy 0.90625\n",
      "Training in progress @ global_step 87200, g_loss 0.482457, d_loss 0.442884 accuracy 0.859375\n",
      "Training in progress @ global_step 87250, g_loss 0.481765, d_loss 0.442297 accuracy 0.890625\n",
      "Training in progress @ global_step 87300, g_loss 0.483044, d_loss 0.438287 accuracy 0.921875\n",
      "Training in progress @ global_step 87350, g_loss 0.480194, d_loss 0.435487 accuracy 0.859375\n",
      "Training in progress @ global_step 87400, g_loss 0.482097, d_loss 0.446708 accuracy 0.859375\n",
      "Training in progress @ global_step 87450, g_loss 0.484076, d_loss 0.443126 accuracy 0.921875\n",
      "Training in progress @ global_step 87500, g_loss 0.482104, d_loss 0.442733 accuracy 0.90625\n",
      "Training in progress @ global_step 87550, g_loss 0.482546, d_loss 0.441103 accuracy 0.859375\n",
      "Training in progress @ global_step 87600, g_loss 0.48315, d_loss 0.435816 accuracy 0.90625\n",
      "Training in progress @ global_step 87650, g_loss 0.484567, d_loss 0.443829 accuracy 0.890625\n",
      "Training in progress @ global_step 87700, g_loss 0.480919, d_loss 0.439518 accuracy 0.859375\n",
      "Training in progress @ global_step 87750, g_loss 0.480657, d_loss 0.439988 accuracy 0.8125\n",
      "Training in progress @ global_step 87800, g_loss 0.480664, d_loss 0.434169 accuracy 0.90625\n",
      "Training in progress @ global_step 87850, g_loss 0.482353, d_loss 0.439023 accuracy 0.921875\n",
      "Training in progress @ global_step 87900, g_loss 0.481052, d_loss 0.444551 accuracy 0.8125\n",
      "Training in progress @ global_step 87950, g_loss 0.481118, d_loss 0.440601 accuracy 0.828125\n",
      "Training in progress @ global_step 88000, g_loss 0.480024, d_loss 0.440795 accuracy 0.875\n",
      "Training in progress @ global_step 88050, g_loss 0.480969, d_loss 0.441702 accuracy 0.875\n",
      "Training in progress @ global_step 88100, g_loss 0.478779, d_loss 0.445341 accuracy 0.796875\n",
      "Training in progress @ global_step 88150, g_loss 0.482013, d_loss 0.440063 accuracy 0.875\n",
      "Training in progress @ global_step 88200, g_loss 0.483709, d_loss 0.438964 accuracy 0.921875\n",
      "Training in progress @ global_step 88250, g_loss 0.482163, d_loss 0.449554 accuracy 0.859375\n",
      "Training in progress @ global_step 88300, g_loss 0.482965, d_loss 0.441271 accuracy 0.8125\n",
      "Training in progress @ global_step 88350, g_loss 0.477794, d_loss 0.442406 accuracy 0.796875\n",
      "Training in progress @ global_step 88400, g_loss 0.480959, d_loss 0.437308 accuracy 0.90625\n",
      "Training in progress @ global_step 88450, g_loss 0.480674, d_loss 0.437044 accuracy 0.859375\n",
      "Training in progress @ global_step 88500, g_loss 0.481202, d_loss 0.440777 accuracy 0.859375\n",
      "Training in progress @ global_step 88550, g_loss 0.478186, d_loss 0.443517 accuracy 0.828125\n",
      "Training in progress @ global_step 88600, g_loss 0.476269, d_loss 0.434828 accuracy 0.796875\n",
      "Training in progress @ global_step 88650, g_loss 0.483037, d_loss 0.442099 accuracy 0.84375\n",
      "Training in progress @ global_step 88700, g_loss 0.478667, d_loss 0.431589 accuracy 0.859375\n",
      "Training in progress @ global_step 88750, g_loss 0.47981, d_loss 0.441375 accuracy 0.828125\n",
      "Training in progress @ global_step 88800, g_loss 0.481604, d_loss 0.438635 accuracy 0.890625\n",
      "Training in progress @ global_step 88850, g_loss 0.477216, d_loss 0.435344 accuracy 0.875\n",
      "Training in progress @ global_step 88900, g_loss 0.478, d_loss 0.443218 accuracy 0.859375\n",
      "Training in progress @ global_step 88950, g_loss 0.47628, d_loss 0.444816 accuracy 0.78125\n",
      "Training in progress @ global_step 89000, g_loss 0.474976, d_loss 0.433488 accuracy 0.8125\n",
      "Training in progress @ global_step 89050, g_loss 0.477411, d_loss 0.441048 accuracy 0.765625\n",
      "Training in progress @ global_step 89100, g_loss 0.475134, d_loss 0.438464 accuracy 0.78125\n",
      "Training in progress @ global_step 89150, g_loss 0.474361, d_loss 0.43536 accuracy 0.75\n",
      "Training in progress @ global_step 89200, g_loss 0.478878, d_loss 0.43499 accuracy 0.875\n",
      "Training in progress @ global_step 89250, g_loss 0.47796, d_loss 0.440625 accuracy 0.78125\n",
      "Training in progress @ global_step 89300, g_loss 0.4774, d_loss 0.431802 accuracy 0.828125\n",
      "Training in progress @ global_step 89350, g_loss 0.475711, d_loss 0.434057 accuracy 0.765625\n",
      "Training in progress @ global_step 89400, g_loss 0.480107, d_loss 0.435218 accuracy 0.875\n",
      "Training in progress @ global_step 89450, g_loss 0.478199, d_loss 0.432263 accuracy 0.78125\n",
      "Training in progress @ global_step 89500, g_loss 0.48054, d_loss 0.438905 accuracy 0.890625\n",
      "Training in progress @ global_step 89550, g_loss 0.480158, d_loss 0.437608 accuracy 0.890625\n",
      "Training in progress @ global_step 89600, g_loss 0.474141, d_loss 0.433059 accuracy 0.765625\n",
      "Training in progress @ global_step 89650, g_loss 0.47878, d_loss 0.43173 accuracy 0.828125\n",
      "Training in progress @ global_step 89700, g_loss 0.477674, d_loss 0.442164 accuracy 0.78125\n",
      "Training in progress @ global_step 89750, g_loss 0.479572, d_loss 0.440409 accuracy 0.828125\n",
      "Training in progress @ global_step 89800, g_loss 0.478373, d_loss 0.43672 accuracy 0.78125\n",
      "Training in progress @ global_step 89850, g_loss 0.477775, d_loss 0.432894 accuracy 0.8125\n",
      "Training in progress @ global_step 89900, g_loss 0.479758, d_loss 0.433988 accuracy 0.875\n",
      "Training in progress @ global_step 89950, g_loss 0.477409, d_loss 0.439743 accuracy 0.84375\n",
      "Training in progress @ global_step 90000, g_loss 0.47536, d_loss 0.434661 accuracy 0.75\n",
      "Training in progress @ global_step 90050, g_loss 0.479556, d_loss 0.434426 accuracy 0.84375\n",
      "Training in progress @ global_step 90100, g_loss 0.477179, d_loss 0.437606 accuracy 0.796875\n",
      "Training in progress @ global_step 90150, g_loss 0.478044, d_loss 0.432179 accuracy 0.84375\n",
      "Training in progress @ global_step 90200, g_loss 0.477366, d_loss 0.438163 accuracy 0.796875\n",
      "Training in progress @ global_step 90250, g_loss 0.474135, d_loss 0.446868 accuracy 0.75\n",
      "Training in progress @ global_step 90300, g_loss 0.477196, d_loss 0.441268 accuracy 0.78125\n",
      "Training in progress @ global_step 90350, g_loss 0.479514, d_loss 0.433321 accuracy 0.875\n",
      "Training in progress @ global_step 90400, g_loss 0.477073, d_loss 0.439694 accuracy 0.828125\n",
      "Training in progress @ global_step 90450, g_loss 0.47722, d_loss 0.436925 accuracy 0.8125\n",
      "Training in progress @ global_step 90500, g_loss 0.477237, d_loss 0.437418 accuracy 0.8125\n",
      "Training in progress @ global_step 90550, g_loss 0.477102, d_loss 0.440962 accuracy 0.828125\n",
      "Training in progress @ global_step 90600, g_loss 0.478111, d_loss 0.433579 accuracy 0.84375\n",
      "Training in progress @ global_step 90650, g_loss 0.476076, d_loss 0.428812 accuracy 0.78125\n",
      "Training in progress @ global_step 90700, g_loss 0.475726, d_loss 0.434766 accuracy 0.8125\n",
      "Training in progress @ global_step 90750, g_loss 0.480002, d_loss 0.435311 accuracy 0.875\n",
      "Training in progress @ global_step 90800, g_loss 0.475742, d_loss 0.437205 accuracy 0.78125\n",
      "Training in progress @ global_step 90850, g_loss 0.47576, d_loss 0.43671 accuracy 0.84375\n",
      "Training in progress @ global_step 90900, g_loss 0.47528, d_loss 0.43917 accuracy 0.796875\n",
      "Training in progress @ global_step 90950, g_loss 0.476068, d_loss 0.447311 accuracy 0.84375\n",
      "Training in progress @ global_step 91000, g_loss 0.475365, d_loss 0.431647 accuracy 0.765625\n",
      "Training in progress @ global_step 91050, g_loss 0.473712, d_loss 0.441276 accuracy 0.8125\n",
      "Training in progress @ global_step 91100, g_loss 0.476864, d_loss 0.429899 accuracy 0.859375\n",
      "Training in progress @ global_step 91150, g_loss 0.475563, d_loss 0.44036 accuracy 0.765625\n",
      "Training in progress @ global_step 91200, g_loss 0.477885, d_loss 0.437412 accuracy 0.890625\n",
      "Training in progress @ global_step 91250, g_loss 0.471692, d_loss 0.43801 accuracy 0.796875\n",
      "Training in progress @ global_step 91300, g_loss 0.478025, d_loss 0.437835 accuracy 0.875\n",
      "Training in progress @ global_step 91350, g_loss 0.478531, d_loss 0.437257 accuracy 0.859375\n",
      "Training in progress @ global_step 91400, g_loss 0.477437, d_loss 0.435524 accuracy 0.875\n",
      "Training in progress @ global_step 91450, g_loss 0.477605, d_loss 0.433168 accuracy 0.890625\n",
      "Training in progress @ global_step 91500, g_loss 0.477713, d_loss 0.434648 accuracy 0.84375\n",
      "Training in progress @ global_step 91550, g_loss 0.473798, d_loss 0.443069 accuracy 0.84375\n",
      "Training in progress @ global_step 91600, g_loss 0.475604, d_loss 0.442113 accuracy 0.828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 91650, g_loss 0.476844, d_loss 0.429234 accuracy 0.84375\n",
      "Training in progress @ global_step 91700, g_loss 0.47903, d_loss 0.437126 accuracy 0.828125\n",
      "Training in progress @ global_step 91750, g_loss 0.477264, d_loss 0.434583 accuracy 0.828125\n",
      "Training in progress @ global_step 91800, g_loss 0.474607, d_loss 0.435477 accuracy 0.796875\n",
      "Training in progress @ global_step 91850, g_loss 0.477296, d_loss 0.438173 accuracy 0.796875\n",
      "Training in progress @ global_step 91900, g_loss 0.475243, d_loss 0.439723 accuracy 0.734375\n",
      "Training in progress @ global_step 91950, g_loss 0.472044, d_loss 0.436638 accuracy 0.75\n",
      "Training in progress @ global_step 92000, g_loss 0.476028, d_loss 0.431908 accuracy 0.8125\n",
      "Training in progress @ global_step 92050, g_loss 0.472636, d_loss 0.438834 accuracy 0.75\n",
      "Training in progress @ global_step 92100, g_loss 0.481741, d_loss 0.43766 accuracy 0.859375\n",
      "Training in progress @ global_step 92150, g_loss 0.479093, d_loss 0.439286 accuracy 0.859375\n",
      "Training in progress @ global_step 92200, g_loss 0.476988, d_loss 0.432957 accuracy 0.84375\n",
      "Training in progress @ global_step 92250, g_loss 0.477643, d_loss 0.44263 accuracy 0.796875\n",
      "Training in progress @ global_step 92300, g_loss 0.479811, d_loss 0.440567 accuracy 0.859375\n",
      "Training in progress @ global_step 92350, g_loss 0.478233, d_loss 0.437504 accuracy 0.796875\n",
      "Training in progress @ global_step 92400, g_loss 0.475713, d_loss 0.441556 accuracy 0.78125\n",
      "Training in progress @ global_step 92450, g_loss 0.480133, d_loss 0.440228 accuracy 0.796875\n",
      "Training in progress @ global_step 92500, g_loss 0.47834, d_loss 0.442095 accuracy 0.796875\n",
      "Training in progress @ global_step 92550, g_loss 0.477166, d_loss 0.441628 accuracy 0.75\n",
      "Training in progress @ global_step 92600, g_loss 0.48497, d_loss 0.439767 accuracy 0.875\n",
      "Training in progress @ global_step 92650, g_loss 0.479122, d_loss 0.44389 accuracy 0.78125\n",
      "Training in progress @ global_step 92700, g_loss 0.47131, d_loss 0.441125 accuracy 0.65625\n",
      "Training in progress @ global_step 92750, g_loss 0.478448, d_loss 0.442338 accuracy 0.8125\n",
      "Training in progress @ global_step 92800, g_loss 0.484021, d_loss 0.438454 accuracy 0.890625\n",
      "Training in progress @ global_step 92850, g_loss 0.47775, d_loss 0.445512 accuracy 0.8125\n",
      "Training in progress @ global_step 92900, g_loss 0.479861, d_loss 0.446693 accuracy 0.765625\n",
      "Training in progress @ global_step 92950, g_loss 0.476453, d_loss 0.436487 accuracy 0.78125\n",
      "Training in progress @ global_step 93000, g_loss 0.477625, d_loss 0.441108 accuracy 0.796875\n",
      "Training in progress @ global_step 93050, g_loss 0.478108, d_loss 0.437252 accuracy 0.78125\n",
      "Training in progress @ global_step 93100, g_loss 0.477952, d_loss 0.442139 accuracy 0.796875\n",
      "Training in progress @ global_step 93150, g_loss 0.480674, d_loss 0.4387 accuracy 0.84375\n",
      "Training in progress @ global_step 93200, g_loss 0.472386, d_loss 0.441845 accuracy 0.640625\n",
      "Training in progress @ global_step 93250, g_loss 0.474942, d_loss 0.444515 accuracy 0.71875\n",
      "Training in progress @ global_step 93300, g_loss 0.481432, d_loss 0.445223 accuracy 0.84375\n",
      "Training in progress @ global_step 93350, g_loss 0.47809, d_loss 0.439135 accuracy 0.84375\n",
      "Training in progress @ global_step 93400, g_loss 0.476266, d_loss 0.436904 accuracy 0.765625\n",
      "Training in progress @ global_step 93450, g_loss 0.476499, d_loss 0.438609 accuracy 0.78125\n",
      "Training in progress @ global_step 93500, g_loss 0.4767, d_loss 0.432936 accuracy 0.8125\n",
      "Training in progress @ global_step 93550, g_loss 0.479516, d_loss 0.440037 accuracy 0.875\n",
      "Training in progress @ global_step 93600, g_loss 0.469452, d_loss 0.436071 accuracy 0.734375\n",
      "Training in progress @ global_step 93650, g_loss 0.479974, d_loss 0.440848 accuracy 0.875\n",
      "Training in progress @ global_step 93700, g_loss 0.477794, d_loss 0.438564 accuracy 0.828125\n",
      "Training in progress @ global_step 93750, g_loss 0.474274, d_loss 0.43521 accuracy 0.78125\n",
      "Training in progress @ global_step 93800, g_loss 0.480583, d_loss 0.442316 accuracy 0.890625\n",
      "Training in progress @ global_step 93850, g_loss 0.479067, d_loss 0.435611 accuracy 0.859375\n",
      "Training in progress @ global_step 93900, g_loss 0.481077, d_loss 0.439429 accuracy 0.859375\n",
      "Training in progress @ global_step 93950, g_loss 0.475145, d_loss 0.446621 accuracy 0.75\n",
      "Training in progress @ global_step 94000, g_loss 0.480836, d_loss 0.443235 accuracy 0.8125\n",
      "Training in progress @ global_step 94050, g_loss 0.482657, d_loss 0.438458 accuracy 0.84375\n",
      "Training in progress @ global_step 94100, g_loss 0.472607, d_loss 0.433592 accuracy 0.75\n",
      "Training in progress @ global_step 94150, g_loss 0.473288, d_loss 0.44079 accuracy 0.71875\n",
      "Training in progress @ global_step 94200, g_loss 0.47592, d_loss 0.441085 accuracy 0.796875\n",
      "Training in progress @ global_step 94250, g_loss 0.479865, d_loss 0.433354 accuracy 0.75\n",
      "Training in progress @ global_step 94300, g_loss 0.488216, d_loss 0.440996 accuracy 0.875\n",
      "Training in progress @ global_step 94350, g_loss 0.482299, d_loss 0.438326 accuracy 0.859375\n",
      "Training in progress @ global_step 94400, g_loss 0.479361, d_loss 0.438216 accuracy 0.828125\n",
      "Training in progress @ global_step 94450, g_loss 0.480756, d_loss 0.44057 accuracy 0.796875\n",
      "Training in progress @ global_step 94500, g_loss 0.476233, d_loss 0.443452 accuracy 0.71875\n",
      "Training in progress @ global_step 94550, g_loss 0.482418, d_loss 0.440484 accuracy 0.734375\n",
      "Training in progress @ global_step 94600, g_loss 0.483036, d_loss 0.441494 accuracy 0.796875\n",
      "Training in progress @ global_step 94650, g_loss 0.480565, d_loss 0.438948 accuracy 0.859375\n",
      "Training in progress @ global_step 94700, g_loss 0.477925, d_loss 0.433301 accuracy 0.75\n",
      "Training in progress @ global_step 94750, g_loss 0.478107, d_loss 0.438218 accuracy 0.765625\n",
      "Training in progress @ global_step 94800, g_loss 0.476261, d_loss 0.441359 accuracy 0.765625\n",
      "Training in progress @ global_step 94850, g_loss 0.477171, d_loss 0.438208 accuracy 0.8125\n",
      "Training in progress @ global_step 94900, g_loss 0.478275, d_loss 0.437731 accuracy 0.796875\n",
      "Training in progress @ global_step 94950, g_loss 0.472612, d_loss 0.441903 accuracy 0.671875\n",
      "Training in progress @ global_step 95000, g_loss 0.47864, d_loss 0.438117 accuracy 0.765625\n",
      "Training in progress @ global_step 95050, g_loss 0.476273, d_loss 0.444134 accuracy 0.78125\n",
      "Training in progress @ global_step 95100, g_loss 0.470023, d_loss 0.443691 accuracy 0.671875\n",
      "Training in progress @ global_step 95150, g_loss 0.47319, d_loss 0.440936 accuracy 0.671875\n",
      "Training in progress @ global_step 95200, g_loss 0.473781, d_loss 0.442956 accuracy 0.703125\n",
      "Training in progress @ global_step 95250, g_loss 0.473179, d_loss 0.441771 accuracy 0.65625\n",
      "Training in progress @ global_step 95300, g_loss 0.473576, d_loss 0.442171 accuracy 0.71875\n",
      "Training in progress @ global_step 95350, g_loss 0.479886, d_loss 0.444811 accuracy 0.78125\n",
      "Training in progress @ global_step 95400, g_loss 0.476491, d_loss 0.440839 accuracy 0.734375\n",
      "Training in progress @ global_step 95450, g_loss 0.476798, d_loss 0.440352 accuracy 0.734375\n",
      "Training in progress @ global_step 95500, g_loss 0.478845, d_loss 0.43518 accuracy 0.84375\n",
      "Training in progress @ global_step 95550, g_loss 0.476462, d_loss 0.431487 accuracy 0.765625\n",
      "Training in progress @ global_step 95600, g_loss 0.472635, d_loss 0.436621 accuracy 0.671875\n",
      "Training in progress @ global_step 95650, g_loss 0.477219, d_loss 0.44229 accuracy 0.78125\n",
      "Training in progress @ global_step 95700, g_loss 0.47324, d_loss 0.443535 accuracy 0.71875\n",
      "Training in progress @ global_step 95750, g_loss 0.474306, d_loss 0.443667 accuracy 0.71875\n",
      "Training in progress @ global_step 95800, g_loss 0.475679, d_loss 0.445791 accuracy 0.734375\n",
      "Training in progress @ global_step 95850, g_loss 0.47342, d_loss 0.434663 accuracy 0.6875\n",
      "Training in progress @ global_step 95900, g_loss 0.475557, d_loss 0.439751 accuracy 0.75\n",
      "Training in progress @ global_step 95950, g_loss 0.470878, d_loss 0.446662 accuracy 0.640625\n",
      "Training in progress @ global_step 96000, g_loss 0.470765, d_loss 0.439699 accuracy 0.640625\n",
      "Training in progress @ global_step 96050, g_loss 0.472781, d_loss 0.436544 accuracy 0.703125\n",
      "Training in progress @ global_step 96100, g_loss 0.471462, d_loss 0.437499 accuracy 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 96150, g_loss 0.469578, d_loss 0.442639 accuracy 0.65625\n",
      "Training in progress @ global_step 96200, g_loss 0.474585, d_loss 0.438718 accuracy 0.765625\n",
      "Training in progress @ global_step 96250, g_loss 0.47219, d_loss 0.433136 accuracy 0.6875\n",
      "Training in progress @ global_step 96300, g_loss 0.470183, d_loss 0.441396 accuracy 0.578125\n",
      "Training in progress @ global_step 96350, g_loss 0.473286, d_loss 0.437547 accuracy 0.703125\n",
      "Training in progress @ global_step 96400, g_loss 0.47539, d_loss 0.428761 accuracy 0.75\n",
      "Training in progress @ global_step 96450, g_loss 0.470857, d_loss 0.426368 accuracy 0.703125\n",
      "Training in progress @ global_step 96500, g_loss 0.473515, d_loss 0.44068 accuracy 0.734375\n",
      "Training in progress @ global_step 96550, g_loss 0.475359, d_loss 0.44095 accuracy 0.734375\n",
      "Training in progress @ global_step 96600, g_loss 0.472138, d_loss 0.437793 accuracy 0.765625\n",
      "Training in progress @ global_step 96650, g_loss 0.478895, d_loss 0.438449 accuracy 0.765625\n",
      "Training in progress @ global_step 96700, g_loss 0.471913, d_loss 0.428918 accuracy 0.765625\n",
      "Training in progress @ global_step 96750, g_loss 0.474476, d_loss 0.430657 accuracy 0.796875\n",
      "Training in progress @ global_step 96800, g_loss 0.470007, d_loss 0.429658 accuracy 0.734375\n",
      "Training in progress @ global_step 96850, g_loss 0.477373, d_loss 0.440372 accuracy 0.796875\n",
      "Training in progress @ global_step 96900, g_loss 0.472147, d_loss 0.432623 accuracy 0.71875\n",
      "Training in progress @ global_step 96950, g_loss 0.470031, d_loss 0.433377 accuracy 0.703125\n",
      "Training in progress @ global_step 97000, g_loss 0.475333, d_loss 0.43972 accuracy 0.84375\n",
      "Training in progress @ global_step 97050, g_loss 0.47329, d_loss 0.436716 accuracy 0.75\n",
      "Training in progress @ global_step 97100, g_loss 0.474869, d_loss 0.437727 accuracy 0.765625\n",
      "Training in progress @ global_step 97150, g_loss 0.475915, d_loss 0.427265 accuracy 0.8125\n",
      "Training in progress @ global_step 97200, g_loss 0.47114, d_loss 0.432278 accuracy 0.75\n",
      "Training in progress @ global_step 97250, g_loss 0.467815, d_loss 0.434156 accuracy 0.703125\n",
      "Training in progress @ global_step 97300, g_loss 0.472093, d_loss 0.430734 accuracy 0.71875\n",
      "Training in progress @ global_step 97350, g_loss 0.476504, d_loss 0.435024 accuracy 0.75\n",
      "Training in progress @ global_step 97400, g_loss 0.475845, d_loss 0.43819 accuracy 0.734375\n",
      "Training in progress @ global_step 97450, g_loss 0.474226, d_loss 0.428621 accuracy 0.71875\n",
      "Training in progress @ global_step 97500, g_loss 0.477189, d_loss 0.426326 accuracy 0.8125\n",
      "Training in progress @ global_step 97550, g_loss 0.471776, d_loss 0.431873 accuracy 0.71875\n",
      "Training in progress @ global_step 97600, g_loss 0.481874, d_loss 0.431873 accuracy 0.84375\n",
      "Training in progress @ global_step 97650, g_loss 0.476846, d_loss 0.435699 accuracy 0.75\n",
      "Training in progress @ global_step 97700, g_loss 0.4759, d_loss 0.426714 accuracy 0.765625\n",
      "Training in progress @ global_step 97750, g_loss 0.478938, d_loss 0.436016 accuracy 0.765625\n",
      "Training in progress @ global_step 97800, g_loss 0.468999, d_loss 0.434754 accuracy 0.65625\n",
      "Training in progress @ global_step 97850, g_loss 0.474143, d_loss 0.422435 accuracy 0.75\n",
      "Training in progress @ global_step 97900, g_loss 0.479135, d_loss 0.431139 accuracy 0.765625\n",
      "Training in progress @ global_step 97950, g_loss 0.475189, d_loss 0.439798 accuracy 0.75\n",
      "Training in progress @ global_step 98000, g_loss 0.479371, d_loss 0.430037 accuracy 0.75\n",
      "Training in progress @ global_step 98050, g_loss 0.473585, d_loss 0.425304 accuracy 0.6875\n",
      "Training in progress @ global_step 98100, g_loss 0.471354, d_loss 0.430055 accuracy 0.71875\n",
      "Training in progress @ global_step 98150, g_loss 0.48023, d_loss 0.434186 accuracy 0.8125\n",
      "Training in progress @ global_step 98200, g_loss 0.476723, d_loss 0.433804 accuracy 0.75\n",
      "Training in progress @ global_step 98250, g_loss 0.47347, d_loss 0.433515 accuracy 0.734375\n",
      "Training in progress @ global_step 98300, g_loss 0.477389, d_loss 0.419802 accuracy 0.796875\n",
      "Training in progress @ global_step 98350, g_loss 0.476446, d_loss 0.438338 accuracy 0.71875\n",
      "Training in progress @ global_step 98400, g_loss 0.479423, d_loss 0.436944 accuracy 0.765625\n",
      "Training in progress @ global_step 98450, g_loss 0.485214, d_loss 0.422629 accuracy 0.859375\n",
      "Training in progress @ global_step 98500, g_loss 0.477548, d_loss 0.435821 accuracy 0.71875\n",
      "Training in progress @ global_step 98550, g_loss 0.480711, d_loss 0.434022 accuracy 0.78125\n",
      "Training in progress @ global_step 98600, g_loss 0.474987, d_loss 0.430071 accuracy 0.6875\n",
      "Training in progress @ global_step 98650, g_loss 0.487257, d_loss 0.444621 accuracy 0.8125\n",
      "Training in progress @ global_step 98700, g_loss 0.474883, d_loss 0.438298 accuracy 0.6875\n",
      "Training in progress @ global_step 98750, g_loss 0.477121, d_loss 0.430723 accuracy 0.75\n",
      "Training in progress @ global_step 98800, g_loss 0.474814, d_loss 0.436448 accuracy 0.71875\n",
      "Training in progress @ global_step 98850, g_loss 0.477991, d_loss 0.433276 accuracy 0.765625\n",
      "Training in progress @ global_step 98900, g_loss 0.474484, d_loss 0.434563 accuracy 0.6875\n",
      "Training in progress @ global_step 98950, g_loss 0.485863, d_loss 0.439904 accuracy 0.8125\n",
      "Training in progress @ global_step 99000, g_loss 0.471805, d_loss 0.428851 accuracy 0.71875\n",
      "Training in progress @ global_step 99050, g_loss 0.479228, d_loss 0.431248 accuracy 0.75\n",
      "Training in progress @ global_step 99100, g_loss 0.47911, d_loss 0.435905 accuracy 0.734375\n",
      "Training in progress @ global_step 99150, g_loss 0.477452, d_loss 0.435818 accuracy 0.765625\n",
      "Training in progress @ global_step 99200, g_loss 0.476516, d_loss 0.444902 accuracy 0.703125\n",
      "Training in progress @ global_step 99250, g_loss 0.471664, d_loss 0.4457 accuracy 0.703125\n",
      "Training in progress @ global_step 99300, g_loss 0.47266, d_loss 0.435041 accuracy 0.671875\n",
      "Training in progress @ global_step 99350, g_loss 0.475104, d_loss 0.440609 accuracy 0.625\n",
      "Training in progress @ global_step 99400, g_loss 0.476801, d_loss 0.433176 accuracy 0.75\n",
      "Training in progress @ global_step 99450, g_loss 0.475243, d_loss 0.438813 accuracy 0.671875\n",
      "Training in progress @ global_step 99500, g_loss 0.477938, d_loss 0.442538 accuracy 0.703125\n",
      "Training in progress @ global_step 99550, g_loss 0.473769, d_loss 0.441335 accuracy 0.640625\n",
      "Training in progress @ global_step 99600, g_loss 0.474259, d_loss 0.448671 accuracy 0.65625\n",
      "Training in progress @ global_step 99650, g_loss 0.475006, d_loss 0.445554 accuracy 0.609375\n",
      "Training in progress @ global_step 99700, g_loss 0.470899, d_loss 0.437524 accuracy 0.65625\n",
      "Training in progress @ global_step 99750, g_loss 0.484409, d_loss 0.437935 accuracy 0.78125\n",
      "Training in progress @ global_step 99800, g_loss 0.478324, d_loss 0.440628 accuracy 0.75\n",
      "Training in progress @ global_step 99850, g_loss 0.469366, d_loss 0.43864 accuracy 0.609375\n",
      "Training in progress @ global_step 99900, g_loss 0.47023, d_loss 0.448106 accuracy 0.609375\n",
      "Training in progress @ global_step 99950, g_loss 0.470099, d_loss 0.442802 accuracy 0.6875\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        RESTORE=False\n",
    "        if not RESTORE:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer.add_graph(sess.graph)\n",
    "            saver = tf.train.Saver()\n",
    "        else: \n",
    "            latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "            print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        print \"Begin training ...\"\n",
    "        # Run training loop\n",
    "        for i in xrange(50000):\n",
    "            step = sess.run(global_step)\n",
    "\n",
    "            # Receive data (this will hang if IO thread is still running = this\n",
    "            # will wait for thread to finish & receive data)\n",
    "\n",
    "            sigma = max(1.0*(40000. - step) / (40000), 0.01)\n",
    "\n",
    "            # Update the generator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "\n",
    "            real_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "            fake_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "\n",
    "\n",
    "            [  acc_fake, _ ] = sess.run(\n",
    "                [accuracy_fake, \n",
    "                 generator_optimizer], \n",
    "                feed_dict = {noise_tensor: fake_input,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "            # Update the discriminator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "            [generated_mnist, _] = sess.run([fake_images, \n",
    "                                            discriminator_optimizer], \n",
    "                                            feed_dict = {noise_tensor : fake_input,\n",
    "                                                         real_flat : real_data,\n",
    "                                                         real_noise: real_noise_addition,\n",
    "                                                         fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            [summary, g_l, acc_fake, d_l_r, acc] = sess.run(\n",
    "                [merged_summary, g_loss, accuracy_fake,\n",
    "                 d_loss_real, total_accuracy],\n",
    "                feed_dict = {noise_tensor : fake,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "            if step != 0 and step % 500 == 0:\n",
    "                saver.save(\n",
    "                    sess,\n",
    "                    LOGDIR+\"/checkpoints/save\",\n",
    "                    global_step=step)\n",
    "\n",
    "\n",
    "            # train_writer.add_summary(summary, i)\n",
    "            # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "            if step % 50 == 0:\n",
    "                print 'Training in progress @ global_step %d, g_loss %g, d_loss %g accuracy %g' % (step, g_l, d_l_r, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, let's load this network back into memory and generate a few fake images for visualization.  As you'll see, this network does \"OK\" but not amazingly well.  In the next post, we'll see a deep convolutional network that does much better at generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model from ./mnist_gan_logs//checkpoints/save-99500\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_gan_logs//checkpoints/save-99500\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "        print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "        # We only need to make fake data and run it through the 'fake_images' tensor to see the output:\n",
    "        \n",
    "        fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "\n",
    "        [generated_images] = sess.run(\n",
    "                [fake_images], \n",
    "                feed_dict = {noise_tensor: fake_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to make it easier to draw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images = numpy.reshape(generated_images, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJKCAYAAAA84QGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f66355c4590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(generate_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well ... It's OK.  It's very obviously not a digit, but it looks a bit like it *could* be a digit.  If you train this network on specific digits (1, 2,etc) I'm sure you will get much better performance.  Why not try it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
