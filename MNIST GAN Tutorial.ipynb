{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particle physics, Generative Adversarial Networks hold a lot of promise for studying data/MC differences.  In principle, the physics tools used for simulation of particle interaction data are quite good, but they are never perfect.  Additionally, they are always imperfect in difficult to model ways.  Many experiments spend a lot of time studying the differences between the output of their simulation and their real detector data, and a deep network that can learn these differences is really useful for making progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll cover some basics of generative adversarial networks with very simple feed-forward neural networks (not even convolutional) as a demonstration of the basic techniques of GANs.  You can read the original paper on GANs here: https://arxiv.org/abs/1406.2661.  The basic idea is you train two networks to compete with each other.  The first (called the discriminator) makes a decision on whether or not the images it's looking at are real or fake.  The second (called the generator) tries to generate fake images from random noise to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the years since the original paper came out, GANs have grown increasinly more sophisticated and impressive, especially with the advent of the Deep Convolutional Generative Adversarial Network (DCGAN, original paper: https://arxiv.org/abs/1511.06434).  For this tutorial, we're going to eschew all of the recent advances to make a GAN that can generate artificial digits based on the mnist data set.  By stripping the networks down to their basics, it's easier to focus on the core aspects of the loss function and outputs, and to avoid the complications of training on GPUs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist data set is one of the most famous collections of labeled images that exists.  You can read all about it at the original website (http://yann.lecun.com/exdb/mnist/), but it has a few nice advantages that make it ideal for learning:\n",
    " * It's open, and has convient wrappers in many languages (we'll use tensorflows soon)\n",
    " * It's a large data set (60k training images) but each image is small (28x28)\n",
    " * The data has been preprocessed to center images of digits and make them easier to use\n",
    " \n",
    "Basically, you can focus on the fundamentals with this data set, which is why we'll use it here.  Some examples of loading it with tensorflow (more here: https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, you can specify which device to use.  The next cell will tell you what's available, and you can select from there.  By default, I select \"/gpu:0\" but you can change this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 562091587952366681\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11946606592\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 6214697808030998256\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()\n",
    "default_device = \"/gpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = mnist.train.next_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've loaded the mnist data from the tensorflow helper class, and asked for the next batch of images and labels.  Let's view those to see what the data looks like before delving into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 784)\n",
      "mnist_labels.shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n",
    "print \"mnist_labels.shape: {}\".format(mnist_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, variables are arrays with the outermost dimension equal to 5.  The images, though, comes unpacked as a 1D array per image instead of a 2D array.  We can reshape this to what we're more familar with, since we know mnist images are 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images = numpy.reshape(mnist_images, (-1, 28, 28)) # -1 can be used as a placeholder for batch size here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib gives a good interface for viewing these images in a notebook (or even in general, in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is labeled as 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJKCAYAAAA84QGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e912f3090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mnist_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "print \"This image is labeled as {}\".format(mnist_labels[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn like this, you can see each individual pixel clearly.  It's not a high resolution image, but you can clearly tell what the digit is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.000001\n",
    "BATCH_SIZE=64 # Keep this even\n",
    "N_HIDDEN_NEURONS=1024\n",
    "LOGDIR=\"./mnist_gan_logs/lr_{}_neurons_{}\".format(BASE_LEARNING_RATE, N_HIDDEN_NEURONS)\n",
    "RESTORE=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real images and random noise, so make a placeholder for both.  Additionally, based on this blog post (http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/) I add random gaussian noise to the real and fake images as they are fed to the discriminator to help stabalize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        # Input noise to the generator:\n",
    "        noise_tensor = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 10*10], name=\"noise\")\n",
    "        fake_input   = tf.reshape(noise_tensor, (tf.shape(noise_tensor)[0], 10,10, 1))\n",
    "\n",
    "        # Placeholder for the discriminator input:\n",
    "        real_flat  = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 784], name='x')\n",
    "        # label_tensor = tf.placeholder(tf.float32, [BATCH_SIZE, 1], name='labels')\n",
    "        real_images  = tf.reshape(real_flat, (tf.shape(real_flat)[0], 28, 28, 1))\n",
    "\n",
    "        # We augment the input to the discriminator with gaussian noise\n",
    "        # This makes it harder for the discriminator to do it's job, preventing\n",
    "        # it from always \"winning\" the GAN min/max contest\n",
    "        real_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"real_noise\")\n",
    "        fake_noise = tf.placeholder(tf.float32, [int(BATCH_SIZE*0.5), 28, 28, 1], name=\"fake_noise\")\n",
    "\n",
    "        real_images = real_noise + real_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input tensors (noise_tensor, real_images) are shaped in the 'flattened' way: (N/2, 100) for noise, (N/2, 784) for real images.  This lets me input the mnist images directly to tensorflow, as well as the noise.  They are then reshaped to be like tensorflow images (Batch, H, W, Filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to build the discriminator, using fully connected networks.  Note that a convolutional layer with the stride equal to the image size *is* a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor, reuse,reg=0.1):\n",
    "    # Use scoping to keep the variables nicely organized in the graph.\n",
    "    # Scoping is good practice always, but it's *essential* here as we'll see later on\n",
    "    with tf.variable_scope(\"mnist_discriminator\", reuse=reuse):\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=128, #Connecting to 128 output neurons, each it's own filter\n",
    "                             kernel_size=[28,28], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        # Apply a non linearity:\n",
    "        x = tf.nn.relu(x)\n",
    "        # That maps to a hidden layer, shape is (B, 1, 1, 128), let's now map to a single output\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=1,         #Connecting to 1 output neuron\n",
    "                             kernel_size=[1,1], \n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Since we want to predict \"real\" or \"fake\", an output of 0 or 1 is desired.  sigmoid is perfect for this:\n",
    "        x = tf.nn.sigmoid(x, name=\"discriminator_sigmoid\")\n",
    "        #Reshape this to bring it down to just one output per image:\n",
    "        x = tf.reshape(x, (x.get_shape().as_list()[0],))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        real_image_logits = build_discriminator(real_images, reuse=False,reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to generate random images from noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator(input_tensor, reg=0.2):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_generator\"):\n",
    "        # Input is Bx10x10x1, let's use two hidden layers this time to upsample to 28x28:\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             filters=N_HIDDEN_NEURONS,\n",
    "                             kernel_size=[10,10], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_1\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        # Another fully connected layer, no change to resolution yet:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=N_HIDDEN_NEURONS,\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_2\")\n",
    "        \n",
    "        # Apply nonlinearity, this time using a 'leaky relu':\n",
    "        x = tf.maximum(x, 0.1*x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Final layer, step up to \"full\" resolution and then reshape the tensor:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             filters=784, #784 is a 28x28 image\n",
    "                             kernel_size=[1,1], # Use the size of the mnist images to make this FC\n",
    "                             strides=(1, 1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             name=\"fully_connected_3\")\n",
    "        \n",
    "        # Reshape to match mnist images:\n",
    "        x = tf.reshape(x, (-1, 28, 28, 1))\n",
    "        \n",
    "        # The final non linearity applied here is to map the images onto the [-1,1] range.\n",
    "        x = tf.nn.tanh(x, name=\"generator_tanh\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_images = build_generator(fake_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to run the discriminator on the fake images, so set that up too.  Since it trains on both real and fake images, set reuse=True here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_image_logits = build_discriminator(fake_images, reuse=True, reg=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the generator and discriminator seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    # Build the loss functions:\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "            #Discriminator loss on real images (classify as 1):\n",
    "            d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_image_logits,\n",
    "                labels = tf.ones_like(real_image_logits)))\n",
    "            #Discriminator loss on fake images (classify as 0):\n",
    "            d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "                labels = tf.zeros_like(fake_image_logits)))\n",
    "\n",
    "            # Total discriminator loss is the sum:\n",
    "            d_loss_total = d_loss_real + d_loss_fake\n",
    "\n",
    "            # This is the adverserial step: g_loss tries to optimize fake_logits to one,\n",
    "            # While d_loss_fake tries to optimize fake_logits to zero.\n",
    "            g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "                labels = tf.ones_like(fake_image_logits)))\n",
    "\n",
    "            # This code is useful if you'll use tensorboard to monitor training:\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Fake_Loss\", d_loss_fake)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss_total)\n",
    "            d_loss_summary = tf.summary.scalar(\"Generator_Loss\", g_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"accuracy\") as scope:\n",
    "            # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "            accuracy_real  = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_logits), \n",
    "                                                             tf.ones_like(real_image_logits)), \n",
    "                                                    tf.float32))\n",
    "            accuracy_fake  = tf.reduce_mean(tf.cast(tf.equal(tf.round(fake_image_logits), \n",
    "                                                             tf.zeros_like(fake_image_logits)), \n",
    "                                                    tf.float32))\n",
    "\n",
    "            total_accuracy = 0.5*(accuracy_fake +  accuracy_real)\n",
    "\n",
    "            # Again, useful for tensorboard:\n",
    "            acc_real_summary = tf.summary.scalar(\"Real_Accuracy\", accuracy_real)\n",
    "            acc_real_summary = tf.summary.scalar(\"Fake_Accuracy\", accuracy_fake)\n",
    "            acc_real_summary = tf.summary.scalar(\"Total_Accuracy\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            # Global steps are useful for restoring training:\n",
    "            global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            # Make sure the optimizers are only operating on their own variables:\n",
    "\n",
    "            all_variables      = tf.trainable_variables()\n",
    "            discriminator_vars = [v for v in all_variables if v.name.startswith('mnist_discriminator/')]\n",
    "            generator_vars     = [v for v in all_variables if v.name.startswith('mnist_generator/')]\n",
    "\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                d_loss_total, global_step=global_step, var_list=discriminator_vars)\n",
    "            generator_optimizer     = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                g_loss, global_step=global_step, var_list=generator_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        tf.summary.image('fake_images', fake_images, max_outputs=4)\n",
    "        tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Set up a saver:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model from ./mnist_gan_logs/lr_1e-06_neurons_1024/checkpoints/save-99500\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_gan_logs/lr_1e-06_neurons_1024/checkpoints/save-99500\n",
      "Begin training ...\n",
      "Training in progress @ global_step 99550, g_loss 0.515897, d_loss 0.439522 accuracy 0.90625\n",
      "Training in progress @ global_step 99600, g_loss 0.514417, d_loss 0.440955 accuracy 0.921875\n",
      "Training in progress @ global_step 99650, g_loss 0.505968, d_loss 0.438132 accuracy 0.9375\n",
      "Training in progress @ global_step 99700, g_loss 0.512955, d_loss 0.445203 accuracy 0.921875\n",
      "Training in progress @ global_step 99750, g_loss 0.512432, d_loss 0.45219 accuracy 0.921875\n",
      "Training in progress @ global_step 99800, g_loss 0.511584, d_loss 0.447963 accuracy 0.890625\n",
      "Training in progress @ global_step 99850, g_loss 0.512335, d_loss 0.448797 accuracy 0.828125\n",
      "Training in progress @ global_step 99900, g_loss 0.516602, d_loss 0.448242 accuracy 0.921875\n",
      "Training in progress @ global_step 99950, g_loss 0.5136, d_loss 0.450659 accuracy 0.890625\n",
      "Training in progress @ global_step 100000, g_loss 0.516367, d_loss 0.44397 accuracy 0.875\n",
      "Training in progress @ global_step 100050, g_loss 0.519172, d_loss 0.440312 accuracy 0.875\n",
      "Training in progress @ global_step 100100, g_loss 0.523343, d_loss 0.450922 accuracy 0.921875\n",
      "Training in progress @ global_step 100150, g_loss 0.515055, d_loss 0.444743 accuracy 0.953125\n",
      "Training in progress @ global_step 100200, g_loss 0.517046, d_loss 0.452606 accuracy 0.921875\n",
      "Training in progress @ global_step 100250, g_loss 0.515542, d_loss 0.450284 accuracy 0.90625\n",
      "Training in progress @ global_step 100300, g_loss 0.508794, d_loss 0.447027 accuracy 0.890625\n",
      "Training in progress @ global_step 100350, g_loss 0.512564, d_loss 0.446518 accuracy 0.890625\n",
      "Training in progress @ global_step 100400, g_loss 0.512779, d_loss 0.442433 accuracy 0.921875\n",
      "Training in progress @ global_step 100450, g_loss 0.508855, d_loss 0.439473 accuracy 0.921875\n",
      "Training in progress @ global_step 100500, g_loss 0.519104, d_loss 0.449539 accuracy 0.84375\n",
      "Training in progress @ global_step 100550, g_loss 0.517556, d_loss 0.45864 accuracy 0.8125\n",
      "Training in progress @ global_step 100600, g_loss 0.517563, d_loss 0.449112 accuracy 0.90625\n",
      "Training in progress @ global_step 100650, g_loss 0.517133, d_loss 0.450501 accuracy 0.875\n",
      "Training in progress @ global_step 100700, g_loss 0.517917, d_loss 0.447321 accuracy 0.90625\n",
      "Training in progress @ global_step 100750, g_loss 0.517743, d_loss 0.446218 accuracy 0.859375\n",
      "Training in progress @ global_step 100800, g_loss 0.516651, d_loss 0.45546 accuracy 0.90625\n",
      "Training in progress @ global_step 100850, g_loss 0.515574, d_loss 0.446851 accuracy 0.90625\n",
      "Training in progress @ global_step 100900, g_loss 0.522678, d_loss 0.448775 accuracy 0.875\n",
      "Training in progress @ global_step 100950, g_loss 0.510801, d_loss 0.45377 accuracy 0.875\n",
      "Training in progress @ global_step 101000, g_loss 0.514858, d_loss 0.450976 accuracy 0.90625\n",
      "Training in progress @ global_step 101050, g_loss 0.522209, d_loss 0.448877 accuracy 0.859375\n",
      "Training in progress @ global_step 101100, g_loss 0.520905, d_loss 0.443219 accuracy 0.921875\n",
      "Training in progress @ global_step 101150, g_loss 0.520227, d_loss 0.457067 accuracy 0.859375\n",
      "Training in progress @ global_step 101200, g_loss 0.523191, d_loss 0.455379 accuracy 0.828125\n",
      "Training in progress @ global_step 101250, g_loss 0.520046, d_loss 0.448904 accuracy 0.90625\n",
      "Training in progress @ global_step 101300, g_loss 0.515668, d_loss 0.438895 accuracy 0.953125\n",
      "Training in progress @ global_step 101350, g_loss 0.515795, d_loss 0.456264 accuracy 0.796875\n",
      "Training in progress @ global_step 101400, g_loss 0.517511, d_loss 0.454215 accuracy 0.90625\n",
      "Training in progress @ global_step 101450, g_loss 0.520353, d_loss 0.458924 accuracy 0.84375\n",
      "Training in progress @ global_step 101500, g_loss 0.519194, d_loss 0.438948 accuracy 0.953125\n",
      "Training in progress @ global_step 101550, g_loss 0.519514, d_loss 0.447159 accuracy 0.921875\n",
      "Training in progress @ global_step 101600, g_loss 0.517565, d_loss 0.45915 accuracy 0.84375\n",
      "Training in progress @ global_step 101650, g_loss 0.51485, d_loss 0.455312 accuracy 0.859375\n",
      "Training in progress @ global_step 101700, g_loss 0.52154, d_loss 0.44864 accuracy 0.953125\n",
      "Training in progress @ global_step 101750, g_loss 0.51654, d_loss 0.455085 accuracy 0.875\n",
      "Training in progress @ global_step 101800, g_loss 0.515493, d_loss 0.454255 accuracy 0.875\n",
      "Training in progress @ global_step 101850, g_loss 0.520027, d_loss 0.454379 accuracy 0.921875\n",
      "Training in progress @ global_step 101900, g_loss 0.522233, d_loss 0.443465 accuracy 0.890625\n",
      "Training in progress @ global_step 101950, g_loss 0.514789, d_loss 0.434274 accuracy 0.875\n",
      "Training in progress @ global_step 102000, g_loss 0.52006, d_loss 0.443707 accuracy 0.859375\n",
      "Training in progress @ global_step 102050, g_loss 0.5202, d_loss 0.455534 accuracy 0.90625\n",
      "Training in progress @ global_step 102100, g_loss 0.517035, d_loss 0.44552 accuracy 0.921875\n",
      "Training in progress @ global_step 102150, g_loss 0.519403, d_loss 0.455772 accuracy 0.875\n",
      "Training in progress @ global_step 102200, g_loss 0.521194, d_loss 0.451116 accuracy 0.875\n",
      "Training in progress @ global_step 102250, g_loss 0.523107, d_loss 0.444555 accuracy 0.90625\n",
      "Training in progress @ global_step 102300, g_loss 0.516223, d_loss 0.457009 accuracy 0.828125\n",
      "Training in progress @ global_step 102350, g_loss 0.522541, d_loss 0.447622 accuracy 0.84375\n",
      "Training in progress @ global_step 102400, g_loss 0.522556, d_loss 0.453759 accuracy 0.875\n",
      "Training in progress @ global_step 102450, g_loss 0.522044, d_loss 0.457071 accuracy 0.84375\n",
      "Training in progress @ global_step 102500, g_loss 0.525142, d_loss 0.456771 accuracy 0.875\n",
      "Training in progress @ global_step 102550, g_loss 0.518699, d_loss 0.461581 accuracy 0.8125\n",
      "Training in progress @ global_step 102600, g_loss 0.52475, d_loss 0.456565 accuracy 0.828125\n",
      "Training in progress @ global_step 102650, g_loss 0.522665, d_loss 0.460137 accuracy 0.8125\n",
      "Training in progress @ global_step 102700, g_loss 0.515576, d_loss 0.457901 accuracy 0.828125\n",
      "Training in progress @ global_step 102750, g_loss 0.519989, d_loss 0.458999 accuracy 0.8125\n",
      "Training in progress @ global_step 102800, g_loss 0.521767, d_loss 0.455101 accuracy 0.8125\n",
      "Training in progress @ global_step 102850, g_loss 0.520002, d_loss 0.465627 accuracy 0.734375\n",
      "Training in progress @ global_step 102900, g_loss 0.523435, d_loss 0.452165 accuracy 0.859375\n",
      "Training in progress @ global_step 102950, g_loss 0.526569, d_loss 0.462153 accuracy 0.796875\n",
      "Training in progress @ global_step 103000, g_loss 0.519067, d_loss 0.455055 accuracy 0.875\n",
      "Training in progress @ global_step 103050, g_loss 0.517989, d_loss 0.463773 accuracy 0.75\n",
      "Training in progress @ global_step 103100, g_loss 0.522747, d_loss 0.46379 accuracy 0.796875\n",
      "Training in progress @ global_step 103150, g_loss 0.524699, d_loss 0.444836 accuracy 0.875\n",
      "Training in progress @ global_step 103200, g_loss 0.523324, d_loss 0.454751 accuracy 0.8125\n",
      "Training in progress @ global_step 103250, g_loss 0.523099, d_loss 0.454224 accuracy 0.828125\n",
      "Training in progress @ global_step 103300, g_loss 0.518237, d_loss 0.445146 accuracy 0.859375\n",
      "Training in progress @ global_step 103350, g_loss 0.520685, d_loss 0.456277 accuracy 0.890625\n",
      "Training in progress @ global_step 103400, g_loss 0.521451, d_loss 0.455384 accuracy 0.875\n",
      "Training in progress @ global_step 103450, g_loss 0.523201, d_loss 0.439348 accuracy 0.921875\n",
      "Training in progress @ global_step 103500, g_loss 0.519739, d_loss 0.467194 accuracy 0.796875\n",
      "Training in progress @ global_step 103550, g_loss 0.515374, d_loss 0.448388 accuracy 0.859375\n",
      "Training in progress @ global_step 103600, g_loss 0.516413, d_loss 0.450223 accuracy 0.90625\n",
      "Training in progress @ global_step 103650, g_loss 0.519274, d_loss 0.47059 accuracy 0.734375\n",
      "Training in progress @ global_step 103700, g_loss 0.520568, d_loss 0.458197 accuracy 0.859375\n",
      "Training in progress @ global_step 103750, g_loss 0.523103, d_loss 0.451409 accuracy 0.875\n",
      "Training in progress @ global_step 103800, g_loss 0.521829, d_loss 0.46549 accuracy 0.8125\n",
      "Training in progress @ global_step 103850, g_loss 0.521203, d_loss 0.464989 accuracy 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 103900, g_loss 0.522268, d_loss 0.456855 accuracy 0.890625\n",
      "Training in progress @ global_step 103950, g_loss 0.517163, d_loss 0.452988 accuracy 0.875\n",
      "Training in progress @ global_step 104000, g_loss 0.522549, d_loss 0.463116 accuracy 0.78125\n",
      "Training in progress @ global_step 104050, g_loss 0.52256, d_loss 0.461217 accuracy 0.796875\n",
      "Training in progress @ global_step 104100, g_loss 0.520439, d_loss 0.454556 accuracy 0.828125\n",
      "Training in progress @ global_step 104150, g_loss 0.518956, d_loss 0.454203 accuracy 0.890625\n",
      "Training in progress @ global_step 104200, g_loss 0.524235, d_loss 0.454659 accuracy 0.90625\n",
      "Training in progress @ global_step 104250, g_loss 0.516084, d_loss 0.460315 accuracy 0.8125\n",
      "Training in progress @ global_step 104300, g_loss 0.517899, d_loss 0.458605 accuracy 0.796875\n",
      "Training in progress @ global_step 104350, g_loss 0.523921, d_loss 0.465676 accuracy 0.78125\n",
      "Training in progress @ global_step 104400, g_loss 0.523182, d_loss 0.455051 accuracy 0.890625\n",
      "Training in progress @ global_step 104450, g_loss 0.520346, d_loss 0.462088 accuracy 0.796875\n",
      "Training in progress @ global_step 104500, g_loss 0.516504, d_loss 0.451269 accuracy 0.859375\n",
      "Training in progress @ global_step 104550, g_loss 0.520229, d_loss 0.454285 accuracy 0.859375\n",
      "Training in progress @ global_step 104600, g_loss 0.524342, d_loss 0.465397 accuracy 0.78125\n",
      "Training in progress @ global_step 104650, g_loss 0.525764, d_loss 0.460304 accuracy 0.796875\n",
      "Training in progress @ global_step 104700, g_loss 0.521807, d_loss 0.466331 accuracy 0.78125\n",
      "Training in progress @ global_step 104750, g_loss 0.518521, d_loss 0.453299 accuracy 0.84375\n",
      "Training in progress @ global_step 104800, g_loss 0.519533, d_loss 0.456856 accuracy 0.84375\n",
      "Training in progress @ global_step 104850, g_loss 0.518837, d_loss 0.458844 accuracy 0.828125\n",
      "Training in progress @ global_step 104900, g_loss 0.515966, d_loss 0.461057 accuracy 0.828125\n",
      "Training in progress @ global_step 104950, g_loss 0.524027, d_loss 0.459256 accuracy 0.890625\n",
      "Training in progress @ global_step 105000, g_loss 0.519175, d_loss 0.462368 accuracy 0.765625\n",
      "Training in progress @ global_step 105050, g_loss 0.518444, d_loss 0.467786 accuracy 0.796875\n",
      "Training in progress @ global_step 105100, g_loss 0.524685, d_loss 0.461143 accuracy 0.828125\n",
      "Training in progress @ global_step 105150, g_loss 0.526916, d_loss 0.458109 accuracy 0.8125\n",
      "Training in progress @ global_step 105200, g_loss 0.522074, d_loss 0.468076 accuracy 0.75\n",
      "Training in progress @ global_step 105250, g_loss 0.521166, d_loss 0.453355 accuracy 0.84375\n",
      "Training in progress @ global_step 105300, g_loss 0.520875, d_loss 0.456641 accuracy 0.828125\n",
      "Training in progress @ global_step 105350, g_loss 0.522084, d_loss 0.462218 accuracy 0.78125\n",
      "Training in progress @ global_step 105400, g_loss 0.521527, d_loss 0.461643 accuracy 0.8125\n",
      "Training in progress @ global_step 105450, g_loss 0.517269, d_loss 0.453213 accuracy 0.828125\n",
      "Training in progress @ global_step 105500, g_loss 0.520126, d_loss 0.466346 accuracy 0.84375\n",
      "Training in progress @ global_step 105550, g_loss 0.519907, d_loss 0.449881 accuracy 0.828125\n",
      "Training in progress @ global_step 105600, g_loss 0.524763, d_loss 0.4708 accuracy 0.796875\n",
      "Training in progress @ global_step 105650, g_loss 0.516696, d_loss 0.467947 accuracy 0.78125\n",
      "Training in progress @ global_step 105700, g_loss 0.520373, d_loss 0.458244 accuracy 0.8125\n",
      "Training in progress @ global_step 105750, g_loss 0.522615, d_loss 0.456671 accuracy 0.84375\n",
      "Training in progress @ global_step 105800, g_loss 0.517501, d_loss 0.466953 accuracy 0.765625\n",
      "Training in progress @ global_step 105850, g_loss 0.517837, d_loss 0.454174 accuracy 0.84375\n",
      "Training in progress @ global_step 105900, g_loss 0.516588, d_loss 0.454013 accuracy 0.84375\n",
      "Training in progress @ global_step 105950, g_loss 0.514678, d_loss 0.464935 accuracy 0.796875\n",
      "Training in progress @ global_step 106000, g_loss 0.5195, d_loss 0.46536 accuracy 0.75\n",
      "Training in progress @ global_step 106050, g_loss 0.522285, d_loss 0.470214 accuracy 0.75\n",
      "Training in progress @ global_step 106100, g_loss 0.520202, d_loss 0.456751 accuracy 0.828125\n",
      "Training in progress @ global_step 106150, g_loss 0.522958, d_loss 0.468331 accuracy 0.84375\n",
      "Training in progress @ global_step 106200, g_loss 0.522168, d_loss 0.470948 accuracy 0.734375\n",
      "Training in progress @ global_step 106250, g_loss 0.518882, d_loss 0.450135 accuracy 0.90625\n",
      "Training in progress @ global_step 106300, g_loss 0.521677, d_loss 0.467971 accuracy 0.796875\n",
      "Training in progress @ global_step 106350, g_loss 0.52028, d_loss 0.466198 accuracy 0.859375\n",
      "Training in progress @ global_step 106400, g_loss 0.52206, d_loss 0.453656 accuracy 0.875\n",
      "Training in progress @ global_step 106450, g_loss 0.522178, d_loss 0.458217 accuracy 0.796875\n",
      "Training in progress @ global_step 106500, g_loss 0.524212, d_loss 0.459046 accuracy 0.859375\n",
      "Training in progress @ global_step 106550, g_loss 0.523036, d_loss 0.459208 accuracy 0.78125\n",
      "Training in progress @ global_step 106600, g_loss 0.523453, d_loss 0.459561 accuracy 0.765625\n",
      "Training in progress @ global_step 106650, g_loss 0.522697, d_loss 0.465343 accuracy 0.71875\n",
      "Training in progress @ global_step 106700, g_loss 0.521469, d_loss 0.464005 accuracy 0.796875\n",
      "Training in progress @ global_step 106750, g_loss 0.521113, d_loss 0.471074 accuracy 0.71875\n",
      "Training in progress @ global_step 106800, g_loss 0.520743, d_loss 0.47351 accuracy 0.734375\n",
      "Training in progress @ global_step 106850, g_loss 0.520972, d_loss 0.481453 accuracy 0.671875\n",
      "Training in progress @ global_step 106900, g_loss 0.519605, d_loss 0.464988 accuracy 0.765625\n",
      "Training in progress @ global_step 106950, g_loss 0.522164, d_loss 0.471114 accuracy 0.75\n",
      "Training in progress @ global_step 107000, g_loss 0.522901, d_loss 0.480917 accuracy 0.671875\n",
      "Training in progress @ global_step 107050, g_loss 0.523685, d_loss 0.45928 accuracy 0.8125\n",
      "Training in progress @ global_step 107100, g_loss 0.522537, d_loss 0.464952 accuracy 0.703125\n",
      "Training in progress @ global_step 107150, g_loss 0.519137, d_loss 0.46622 accuracy 0.734375\n",
      "Training in progress @ global_step 107200, g_loss 0.521671, d_loss 0.469943 accuracy 0.765625\n",
      "Training in progress @ global_step 107250, g_loss 0.520057, d_loss 0.466435 accuracy 0.75\n",
      "Training in progress @ global_step 107300, g_loss 0.526827, d_loss 0.466811 accuracy 0.796875\n",
      "Training in progress @ global_step 107350, g_loss 0.520221, d_loss 0.477163 accuracy 0.71875\n",
      "Training in progress @ global_step 107400, g_loss 0.523118, d_loss 0.46644 accuracy 0.75\n",
      "Training in progress @ global_step 107450, g_loss 0.522556, d_loss 0.463779 accuracy 0.796875\n",
      "Training in progress @ global_step 107500, g_loss 0.522431, d_loss 0.465756 accuracy 0.796875\n",
      "Training in progress @ global_step 107550, g_loss 0.521128, d_loss 0.456502 accuracy 0.84375\n",
      "Training in progress @ global_step 107600, g_loss 0.518002, d_loss 0.468624 accuracy 0.75\n",
      "Training in progress @ global_step 107650, g_loss 0.519897, d_loss 0.478488 accuracy 0.71875\n",
      "Training in progress @ global_step 107700, g_loss 0.522271, d_loss 0.452858 accuracy 0.828125\n",
      "Training in progress @ global_step 107750, g_loss 0.521096, d_loss 0.47194 accuracy 0.71875\n",
      "Training in progress @ global_step 107800, g_loss 0.519109, d_loss 0.468469 accuracy 0.75\n",
      "Training in progress @ global_step 107850, g_loss 0.523443, d_loss 0.460589 accuracy 0.734375\n",
      "Training in progress @ global_step 107900, g_loss 0.521495, d_loss 0.456019 accuracy 0.828125\n",
      "Training in progress @ global_step 107950, g_loss 0.519871, d_loss 0.467219 accuracy 0.78125\n",
      "Training in progress @ global_step 108000, g_loss 0.521599, d_loss 0.47492 accuracy 0.734375\n",
      "Training in progress @ global_step 108050, g_loss 0.523018, d_loss 0.461658 accuracy 0.828125\n",
      "Training in progress @ global_step 108100, g_loss 0.517812, d_loss 0.46742 accuracy 0.765625\n",
      "Training in progress @ global_step 108150, g_loss 0.520047, d_loss 0.473062 accuracy 0.703125\n",
      "Training in progress @ global_step 108200, g_loss 0.520304, d_loss 0.468983 accuracy 0.75\n",
      "Training in progress @ global_step 108250, g_loss 0.522353, d_loss 0.465478 accuracy 0.828125\n",
      "Training in progress @ global_step 108300, g_loss 0.522588, d_loss 0.459856 accuracy 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 108350, g_loss 0.522351, d_loss 0.454131 accuracy 0.84375\n",
      "Training in progress @ global_step 108400, g_loss 0.522121, d_loss 0.466444 accuracy 0.8125\n",
      "Training in progress @ global_step 108450, g_loss 0.518233, d_loss 0.472175 accuracy 0.734375\n",
      "Training in progress @ global_step 108500, g_loss 0.522523, d_loss 0.458004 accuracy 0.8125\n",
      "Training in progress @ global_step 108550, g_loss 0.520879, d_loss 0.472909 accuracy 0.75\n",
      "Training in progress @ global_step 108600, g_loss 0.521493, d_loss 0.460195 accuracy 0.796875\n",
      "Training in progress @ global_step 108650, g_loss 0.521364, d_loss 0.453476 accuracy 0.828125\n",
      "Training in progress @ global_step 108700, g_loss 0.51995, d_loss 0.45934 accuracy 0.828125\n",
      "Training in progress @ global_step 108750, g_loss 0.521945, d_loss 0.470604 accuracy 0.734375\n",
      "Training in progress @ global_step 108800, g_loss 0.523455, d_loss 0.474288 accuracy 0.703125\n",
      "Training in progress @ global_step 108850, g_loss 0.516539, d_loss 0.462917 accuracy 0.8125\n",
      "Training in progress @ global_step 108900, g_loss 0.521178, d_loss 0.465659 accuracy 0.8125\n",
      "Training in progress @ global_step 108950, g_loss 0.522471, d_loss 0.467251 accuracy 0.734375\n",
      "Training in progress @ global_step 109000, g_loss 0.522516, d_loss 0.479122 accuracy 0.71875\n",
      "Training in progress @ global_step 109050, g_loss 0.52657, d_loss 0.462073 accuracy 0.8125\n",
      "Training in progress @ global_step 109100, g_loss 0.516546, d_loss 0.475003 accuracy 0.75\n",
      "Training in progress @ global_step 109150, g_loss 0.523677, d_loss 0.474554 accuracy 0.6875\n",
      "Training in progress @ global_step 109200, g_loss 0.519069, d_loss 0.460844 accuracy 0.859375\n",
      "Training in progress @ global_step 109250, g_loss 0.518673, d_loss 0.458598 accuracy 0.84375\n",
      "Training in progress @ global_step 109300, g_loss 0.520364, d_loss 0.465652 accuracy 0.796875\n",
      "Training in progress @ global_step 109350, g_loss 0.518524, d_loss 0.464729 accuracy 0.765625\n",
      "Training in progress @ global_step 109400, g_loss 0.519202, d_loss 0.460461 accuracy 0.84375\n",
      "Training in progress @ global_step 109450, g_loss 0.525178, d_loss 0.468269 accuracy 0.734375\n",
      "Training in progress @ global_step 109500, g_loss 0.518963, d_loss 0.456152 accuracy 0.84375\n",
      "Training in progress @ global_step 109550, g_loss 0.521888, d_loss 0.477973 accuracy 0.71875\n",
      "Training in progress @ global_step 109600, g_loss 0.523842, d_loss 0.462809 accuracy 0.734375\n",
      "Training in progress @ global_step 109650, g_loss 0.516382, d_loss 0.454915 accuracy 0.828125\n",
      "Training in progress @ global_step 109700, g_loss 0.520633, d_loss 0.475205 accuracy 0.734375\n",
      "Training in progress @ global_step 109750, g_loss 0.518104, d_loss 0.47933 accuracy 0.703125\n",
      "Training in progress @ global_step 109800, g_loss 0.51873, d_loss 0.464131 accuracy 0.8125\n",
      "Training in progress @ global_step 109850, g_loss 0.52092, d_loss 0.469071 accuracy 0.71875\n",
      "Training in progress @ global_step 109900, g_loss 0.524127, d_loss 0.466237 accuracy 0.78125\n",
      "Training in progress @ global_step 109950, g_loss 0.523792, d_loss 0.463947 accuracy 0.8125\n",
      "Training in progress @ global_step 110000, g_loss 0.517119, d_loss 0.466313 accuracy 0.8125\n",
      "Training in progress @ global_step 110050, g_loss 0.515555, d_loss 0.47189 accuracy 0.6875\n",
      "Training in progress @ global_step 110100, g_loss 0.519974, d_loss 0.464115 accuracy 0.78125\n",
      "Training in progress @ global_step 110150, g_loss 0.520759, d_loss 0.453655 accuracy 0.859375\n",
      "Training in progress @ global_step 110200, g_loss 0.51578, d_loss 0.468071 accuracy 0.71875\n",
      "Training in progress @ global_step 110250, g_loss 0.5226, d_loss 0.476904 accuracy 0.703125\n",
      "Training in progress @ global_step 110300, g_loss 0.518682, d_loss 0.469451 accuracy 0.8125\n",
      "Training in progress @ global_step 110350, g_loss 0.520603, d_loss 0.460078 accuracy 0.78125\n",
      "Training in progress @ global_step 110400, g_loss 0.524122, d_loss 0.466526 accuracy 0.78125\n",
      "Training in progress @ global_step 110450, g_loss 0.519958, d_loss 0.469256 accuracy 0.796875\n",
      "Training in progress @ global_step 110500, g_loss 0.520747, d_loss 0.455336 accuracy 0.796875\n",
      "Training in progress @ global_step 110550, g_loss 0.523264, d_loss 0.46558 accuracy 0.796875\n",
      "Training in progress @ global_step 110600, g_loss 0.517682, d_loss 0.469495 accuracy 0.796875\n",
      "Training in progress @ global_step 110650, g_loss 0.518314, d_loss 0.46369 accuracy 0.75\n",
      "Training in progress @ global_step 110700, g_loss 0.519592, d_loss 0.473585 accuracy 0.6875\n",
      "Training in progress @ global_step 110750, g_loss 0.51987, d_loss 0.457595 accuracy 0.828125\n",
      "Training in progress @ global_step 110800, g_loss 0.516186, d_loss 0.472377 accuracy 0.75\n",
      "Training in progress @ global_step 110850, g_loss 0.5172, d_loss 0.467112 accuracy 0.6875\n",
      "Training in progress @ global_step 110900, g_loss 0.514792, d_loss 0.468985 accuracy 0.734375\n",
      "Training in progress @ global_step 110950, g_loss 0.520079, d_loss 0.464331 accuracy 0.75\n",
      "Training in progress @ global_step 111000, g_loss 0.519287, d_loss 0.463173 accuracy 0.78125\n",
      "Training in progress @ global_step 111050, g_loss 0.515591, d_loss 0.460425 accuracy 0.765625\n",
      "Training in progress @ global_step 111100, g_loss 0.518437, d_loss 0.466382 accuracy 0.75\n",
      "Training in progress @ global_step 111150, g_loss 0.520765, d_loss 0.472083 accuracy 0.71875\n",
      "Training in progress @ global_step 111200, g_loss 0.520174, d_loss 0.461661 accuracy 0.828125\n",
      "Training in progress @ global_step 111250, g_loss 0.52246, d_loss 0.468369 accuracy 0.765625\n",
      "Training in progress @ global_step 111300, g_loss 0.517846, d_loss 0.465254 accuracy 0.828125\n",
      "Training in progress @ global_step 111350, g_loss 0.522685, d_loss 0.467171 accuracy 0.75\n",
      "Training in progress @ global_step 111400, g_loss 0.521411, d_loss 0.476131 accuracy 0.75\n",
      "Training in progress @ global_step 111450, g_loss 0.511961, d_loss 0.463209 accuracy 0.796875\n",
      "Training in progress @ global_step 111500, g_loss 0.515197, d_loss 0.469832 accuracy 0.75\n",
      "Training in progress @ global_step 111550, g_loss 0.517193, d_loss 0.472861 accuracy 0.703125\n",
      "Training in progress @ global_step 111600, g_loss 0.517451, d_loss 0.460973 accuracy 0.828125\n",
      "Training in progress @ global_step 111650, g_loss 0.508169, d_loss 0.462017 accuracy 0.828125\n",
      "Training in progress @ global_step 111700, g_loss 0.5166, d_loss 0.467149 accuracy 0.75\n",
      "Training in progress @ global_step 111750, g_loss 0.519478, d_loss 0.46588 accuracy 0.765625\n",
      "Training in progress @ global_step 111800, g_loss 0.513028, d_loss 0.471662 accuracy 0.734375\n",
      "Training in progress @ global_step 111850, g_loss 0.514178, d_loss 0.465338 accuracy 0.796875\n",
      "Training in progress @ global_step 111900, g_loss 0.517548, d_loss 0.475157 accuracy 0.734375\n",
      "Training in progress @ global_step 111950, g_loss 0.513481, d_loss 0.467371 accuracy 0.765625\n",
      "Training in progress @ global_step 112000, g_loss 0.513134, d_loss 0.459094 accuracy 0.796875\n",
      "Training in progress @ global_step 112050, g_loss 0.520087, d_loss 0.454747 accuracy 0.890625\n",
      "Training in progress @ global_step 112100, g_loss 0.509222, d_loss 0.459456 accuracy 0.8125\n",
      "Training in progress @ global_step 112150, g_loss 0.519489, d_loss 0.456487 accuracy 0.828125\n",
      "Training in progress @ global_step 112200, g_loss 0.516183, d_loss 0.470433 accuracy 0.734375\n",
      "Training in progress @ global_step 112250, g_loss 0.514011, d_loss 0.466107 accuracy 0.78125\n",
      "Training in progress @ global_step 112300, g_loss 0.513237, d_loss 0.459634 accuracy 0.8125\n",
      "Training in progress @ global_step 112350, g_loss 0.513979, d_loss 0.471363 accuracy 0.75\n",
      "Training in progress @ global_step 112400, g_loss 0.513862, d_loss 0.46311 accuracy 0.8125\n",
      "Training in progress @ global_step 112450, g_loss 0.515619, d_loss 0.460457 accuracy 0.765625\n",
      "Training in progress @ global_step 112500, g_loss 0.508277, d_loss 0.474432 accuracy 0.71875\n",
      "Training in progress @ global_step 112550, g_loss 0.517139, d_loss 0.468237 accuracy 0.765625\n",
      "Training in progress @ global_step 112600, g_loss 0.512198, d_loss 0.467839 accuracy 0.75\n",
      "Training in progress @ global_step 112650, g_loss 0.512348, d_loss 0.471944 accuracy 0.71875\n",
      "Training in progress @ global_step 112700, g_loss 0.515032, d_loss 0.473685 accuracy 0.734375\n",
      "Training in progress @ global_step 112750, g_loss 0.513031, d_loss 0.469337 accuracy 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 112800, g_loss 0.514921, d_loss 0.45678 accuracy 0.765625\n",
      "Training in progress @ global_step 112850, g_loss 0.514402, d_loss 0.469495 accuracy 0.71875\n",
      "Training in progress @ global_step 112900, g_loss 0.513415, d_loss 0.467311 accuracy 0.765625\n",
      "Training in progress @ global_step 112950, g_loss 0.512437, d_loss 0.465841 accuracy 0.78125\n",
      "Training in progress @ global_step 113000, g_loss 0.512096, d_loss 0.471102 accuracy 0.6875\n",
      "Training in progress @ global_step 113050, g_loss 0.515082, d_loss 0.469061 accuracy 0.765625\n",
      "Training in progress @ global_step 113100, g_loss 0.512084, d_loss 0.460739 accuracy 0.796875\n",
      "Training in progress @ global_step 113150, g_loss 0.512868, d_loss 0.469599 accuracy 0.765625\n",
      "Training in progress @ global_step 113200, g_loss 0.513269, d_loss 0.467535 accuracy 0.734375\n",
      "Training in progress @ global_step 113250, g_loss 0.519415, d_loss 0.46374 accuracy 0.84375\n",
      "Training in progress @ global_step 113300, g_loss 0.514497, d_loss 0.472005 accuracy 0.71875\n",
      "Training in progress @ global_step 113350, g_loss 0.518281, d_loss 0.471884 accuracy 0.703125\n",
      "Training in progress @ global_step 113400, g_loss 0.513227, d_loss 0.467039 accuracy 0.78125\n",
      "Training in progress @ global_step 113450, g_loss 0.515245, d_loss 0.472248 accuracy 0.71875\n",
      "Training in progress @ global_step 113500, g_loss 0.513492, d_loss 0.46714 accuracy 0.765625\n",
      "Training in progress @ global_step 113550, g_loss 0.515548, d_loss 0.468404 accuracy 0.734375\n",
      "Training in progress @ global_step 113600, g_loss 0.515449, d_loss 0.46858 accuracy 0.765625\n",
      "Training in progress @ global_step 113650, g_loss 0.510553, d_loss 0.473573 accuracy 0.65625\n",
      "Training in progress @ global_step 113700, g_loss 0.51975, d_loss 0.464893 accuracy 0.765625\n",
      "Training in progress @ global_step 113750, g_loss 0.512161, d_loss 0.468331 accuracy 0.765625\n",
      "Training in progress @ global_step 113800, g_loss 0.51673, d_loss 0.459694 accuracy 0.8125\n",
      "Training in progress @ global_step 113850, g_loss 0.514725, d_loss 0.475286 accuracy 0.6875\n",
      "Training in progress @ global_step 113900, g_loss 0.513508, d_loss 0.47493 accuracy 0.6875\n",
      "Training in progress @ global_step 113950, g_loss 0.508626, d_loss 0.47474 accuracy 0.75\n",
      "Training in progress @ global_step 114000, g_loss 0.50817, d_loss 0.468566 accuracy 0.703125\n",
      "Training in progress @ global_step 114050, g_loss 0.512443, d_loss 0.463967 accuracy 0.75\n",
      "Training in progress @ global_step 114100, g_loss 0.516551, d_loss 0.468758 accuracy 0.734375\n",
      "Training in progress @ global_step 114150, g_loss 0.513049, d_loss 0.466212 accuracy 0.75\n",
      "Training in progress @ global_step 114200, g_loss 0.511948, d_loss 0.462781 accuracy 0.75\n",
      "Training in progress @ global_step 114250, g_loss 0.512308, d_loss 0.471662 accuracy 0.71875\n",
      "Training in progress @ global_step 114300, g_loss 0.511677, d_loss 0.462196 accuracy 0.765625\n",
      "Training in progress @ global_step 114350, g_loss 0.509095, d_loss 0.472881 accuracy 0.734375\n",
      "Training in progress @ global_step 114400, g_loss 0.512341, d_loss 0.470334 accuracy 0.765625\n",
      "Training in progress @ global_step 114450, g_loss 0.511033, d_loss 0.474182 accuracy 0.71875\n",
      "Training in progress @ global_step 114500, g_loss 0.515958, d_loss 0.470155 accuracy 0.734375\n",
      "Training in progress @ global_step 114550, g_loss 0.518202, d_loss 0.46878 accuracy 0.828125\n",
      "Training in progress @ global_step 114600, g_loss 0.515881, d_loss 0.466567 accuracy 0.765625\n",
      "Training in progress @ global_step 114650, g_loss 0.511901, d_loss 0.473412 accuracy 0.75\n",
      "Training in progress @ global_step 114700, g_loss 0.517685, d_loss 0.462411 accuracy 0.765625\n",
      "Training in progress @ global_step 114750, g_loss 0.510436, d_loss 0.464654 accuracy 0.765625\n",
      "Training in progress @ global_step 114800, g_loss 0.516144, d_loss 0.464812 accuracy 0.796875\n",
      "Training in progress @ global_step 114850, g_loss 0.514462, d_loss 0.465659 accuracy 0.78125\n",
      "Training in progress @ global_step 114900, g_loss 0.514325, d_loss 0.468777 accuracy 0.734375\n",
      "Training in progress @ global_step 114950, g_loss 0.512503, d_loss 0.458866 accuracy 0.8125\n",
      "Training in progress @ global_step 115000, g_loss 0.512853, d_loss 0.469137 accuracy 0.75\n",
      "Training in progress @ global_step 115050, g_loss 0.510095, d_loss 0.461362 accuracy 0.828125\n",
      "Training in progress @ global_step 115100, g_loss 0.506573, d_loss 0.465811 accuracy 0.78125\n",
      "Training in progress @ global_step 115150, g_loss 0.509377, d_loss 0.473375 accuracy 0.75\n",
      "Training in progress @ global_step 115200, g_loss 0.50727, d_loss 0.465383 accuracy 0.78125\n",
      "Training in progress @ global_step 115250, g_loss 0.510735, d_loss 0.470112 accuracy 0.765625\n",
      "Training in progress @ global_step 115300, g_loss 0.510091, d_loss 0.458664 accuracy 0.78125\n",
      "Training in progress @ global_step 115350, g_loss 0.511919, d_loss 0.461602 accuracy 0.8125\n",
      "Training in progress @ global_step 115400, g_loss 0.51055, d_loss 0.469773 accuracy 0.6875\n",
      "Training in progress @ global_step 115450, g_loss 0.51123, d_loss 0.453641 accuracy 0.765625\n",
      "Training in progress @ global_step 115500, g_loss 0.506268, d_loss 0.458589 accuracy 0.796875\n",
      "Training in progress @ global_step 115550, g_loss 0.512621, d_loss 0.468565 accuracy 0.75\n",
      "Training in progress @ global_step 115600, g_loss 0.508854, d_loss 0.462914 accuracy 0.765625\n",
      "Training in progress @ global_step 115650, g_loss 0.5114, d_loss 0.469526 accuracy 0.765625\n",
      "Training in progress @ global_step 115700, g_loss 0.508554, d_loss 0.465528 accuracy 0.75\n",
      "Training in progress @ global_step 115750, g_loss 0.509049, d_loss 0.474246 accuracy 0.6875\n",
      "Training in progress @ global_step 115800, g_loss 0.505142, d_loss 0.463328 accuracy 0.703125\n",
      "Training in progress @ global_step 115850, g_loss 0.511343, d_loss 0.461383 accuracy 0.828125\n",
      "Training in progress @ global_step 115900, g_loss 0.514892, d_loss 0.474235 accuracy 0.71875\n",
      "Training in progress @ global_step 115950, g_loss 0.50971, d_loss 0.45864 accuracy 0.828125\n",
      "Training in progress @ global_step 116000, g_loss 0.506369, d_loss 0.46199 accuracy 0.765625\n",
      "Training in progress @ global_step 116050, g_loss 0.508493, d_loss 0.462993 accuracy 0.78125\n",
      "Training in progress @ global_step 116100, g_loss 0.513972, d_loss 0.467333 accuracy 0.765625\n",
      "Training in progress @ global_step 116150, g_loss 0.50549, d_loss 0.465268 accuracy 0.765625\n",
      "Training in progress @ global_step 116200, g_loss 0.508603, d_loss 0.465278 accuracy 0.765625\n",
      "Training in progress @ global_step 116250, g_loss 0.50961, d_loss 0.472112 accuracy 0.75\n",
      "Training in progress @ global_step 116300, g_loss 0.511897, d_loss 0.471784 accuracy 0.734375\n",
      "Training in progress @ global_step 116350, g_loss 0.5107, d_loss 0.463654 accuracy 0.78125\n",
      "Training in progress @ global_step 116400, g_loss 0.512397, d_loss 0.46433 accuracy 0.796875\n",
      "Training in progress @ global_step 116450, g_loss 0.512769, d_loss 0.463112 accuracy 0.765625\n",
      "Training in progress @ global_step 116500, g_loss 0.510084, d_loss 0.469524 accuracy 0.734375\n",
      "Training in progress @ global_step 116550, g_loss 0.514501, d_loss 0.464222 accuracy 0.765625\n",
      "Training in progress @ global_step 116600, g_loss 0.513657, d_loss 0.465964 accuracy 0.734375\n",
      "Training in progress @ global_step 116650, g_loss 0.504977, d_loss 0.470619 accuracy 0.71875\n",
      "Training in progress @ global_step 116700, g_loss 0.507302, d_loss 0.470032 accuracy 0.640625\n",
      "Training in progress @ global_step 116750, g_loss 0.510426, d_loss 0.467294 accuracy 0.78125\n",
      "Training in progress @ global_step 116800, g_loss 0.50885, d_loss 0.461639 accuracy 0.765625\n",
      "Training in progress @ global_step 116850, g_loss 0.513457, d_loss 0.460986 accuracy 0.8125\n",
      "Training in progress @ global_step 116900, g_loss 0.512711, d_loss 0.462177 accuracy 0.828125\n",
      "Training in progress @ global_step 116950, g_loss 0.516267, d_loss 0.467583 accuracy 0.75\n",
      "Training in progress @ global_step 117000, g_loss 0.507669, d_loss 0.467296 accuracy 0.75\n",
      "Training in progress @ global_step 117050, g_loss 0.509469, d_loss 0.458582 accuracy 0.8125\n",
      "Training in progress @ global_step 117100, g_loss 0.510575, d_loss 0.474479 accuracy 0.65625\n",
      "Training in progress @ global_step 117150, g_loss 0.507221, d_loss 0.464586 accuracy 0.71875\n",
      "Training in progress @ global_step 117200, g_loss 0.509162, d_loss 0.461207 accuracy 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 117250, g_loss 0.513764, d_loss 0.460283 accuracy 0.765625\n",
      "Training in progress @ global_step 117300, g_loss 0.509405, d_loss 0.46014 accuracy 0.765625\n",
      "Training in progress @ global_step 117350, g_loss 0.506008, d_loss 0.461304 accuracy 0.8125\n",
      "Training in progress @ global_step 117400, g_loss 0.512385, d_loss 0.462342 accuracy 0.734375\n",
      "Training in progress @ global_step 117450, g_loss 0.515466, d_loss 0.463512 accuracy 0.71875\n",
      "Training in progress @ global_step 117500, g_loss 0.510633, d_loss 0.457992 accuracy 0.765625\n",
      "Training in progress @ global_step 117550, g_loss 0.51236, d_loss 0.467476 accuracy 0.71875\n",
      "Training in progress @ global_step 117600, g_loss 0.509508, d_loss 0.468005 accuracy 0.6875\n",
      "Training in progress @ global_step 117650, g_loss 0.499952, d_loss 0.471005 accuracy 0.59375\n",
      "Training in progress @ global_step 117700, g_loss 0.511487, d_loss 0.478206 accuracy 0.65625\n",
      "Training in progress @ global_step 117750, g_loss 0.511455, d_loss 0.45961 accuracy 0.8125\n",
      "Training in progress @ global_step 117800, g_loss 0.506587, d_loss 0.465426 accuracy 0.703125\n",
      "Training in progress @ global_step 117850, g_loss 0.510925, d_loss 0.469149 accuracy 0.75\n",
      "Training in progress @ global_step 117900, g_loss 0.509553, d_loss 0.467048 accuracy 0.703125\n",
      "Training in progress @ global_step 117950, g_loss 0.5124, d_loss 0.473552 accuracy 0.703125\n",
      "Training in progress @ global_step 118000, g_loss 0.513458, d_loss 0.469149 accuracy 0.71875\n",
      "Training in progress @ global_step 118050, g_loss 0.509503, d_loss 0.464699 accuracy 0.703125\n",
      "Training in progress @ global_step 118100, g_loss 0.513663, d_loss 0.470193 accuracy 0.703125\n",
      "Training in progress @ global_step 118150, g_loss 0.509434, d_loss 0.478025 accuracy 0.640625\n",
      "Training in progress @ global_step 118200, g_loss 0.513723, d_loss 0.478013 accuracy 0.671875\n",
      "Training in progress @ global_step 118250, g_loss 0.505359, d_loss 0.468392 accuracy 0.71875\n",
      "Training in progress @ global_step 118300, g_loss 0.509217, d_loss 0.469429 accuracy 0.765625\n",
      "Training in progress @ global_step 118350, g_loss 0.513686, d_loss 0.467236 accuracy 0.734375\n",
      "Training in progress @ global_step 118400, g_loss 0.509038, d_loss 0.474603 accuracy 0.71875\n",
      "Training in progress @ global_step 118450, g_loss 0.511735, d_loss 0.466195 accuracy 0.828125\n",
      "Training in progress @ global_step 118500, g_loss 0.512862, d_loss 0.467749 accuracy 0.796875\n",
      "Training in progress @ global_step 118550, g_loss 0.508082, d_loss 0.465537 accuracy 0.765625\n",
      "Training in progress @ global_step 118600, g_loss 0.509483, d_loss 0.463683 accuracy 0.75\n",
      "Training in progress @ global_step 118650, g_loss 0.511004, d_loss 0.469699 accuracy 0.765625\n",
      "Training in progress @ global_step 118700, g_loss 0.507705, d_loss 0.47002 accuracy 0.765625\n",
      "Training in progress @ global_step 118750, g_loss 0.514011, d_loss 0.466529 accuracy 0.84375\n",
      "Training in progress @ global_step 118800, g_loss 0.510483, d_loss 0.4716 accuracy 0.75\n",
      "Training in progress @ global_step 118850, g_loss 0.511543, d_loss 0.464338 accuracy 0.8125\n",
      "Training in progress @ global_step 118900, g_loss 0.505993, d_loss 0.462181 accuracy 0.796875\n",
      "Training in progress @ global_step 118950, g_loss 0.508558, d_loss 0.472301 accuracy 0.71875\n",
      "Training in progress @ global_step 119000, g_loss 0.51056, d_loss 0.467838 accuracy 0.859375\n",
      "Training in progress @ global_step 119050, g_loss 0.50922, d_loss 0.478 accuracy 0.6875\n",
      "Training in progress @ global_step 119100, g_loss 0.509976, d_loss 0.466444 accuracy 0.796875\n",
      "Training in progress @ global_step 119150, g_loss 0.514713, d_loss 0.468267 accuracy 0.796875\n",
      "Training in progress @ global_step 119200, g_loss 0.508925, d_loss 0.469249 accuracy 0.765625\n",
      "Training in progress @ global_step 119250, g_loss 0.509987, d_loss 0.460743 accuracy 0.875\n",
      "Training in progress @ global_step 119300, g_loss 0.511142, d_loss 0.465434 accuracy 0.828125\n",
      "Training in progress @ global_step 119350, g_loss 0.515694, d_loss 0.469938 accuracy 0.75\n",
      "Training in progress @ global_step 119400, g_loss 0.51386, d_loss 0.455063 accuracy 0.828125\n",
      "Training in progress @ global_step 119450, g_loss 0.505845, d_loss 0.472359 accuracy 0.65625\n",
      "Training in progress @ global_step 119500, g_loss 0.512622, d_loss 0.460854 accuracy 0.8125\n",
      "Training in progress @ global_step 119550, g_loss 0.510255, d_loss 0.471394 accuracy 0.703125\n",
      "Training in progress @ global_step 119600, g_loss 0.515563, d_loss 0.464351 accuracy 0.8125\n",
      "Training in progress @ global_step 119650, g_loss 0.517887, d_loss 0.47113 accuracy 0.78125\n",
      "Training in progress @ global_step 119700, g_loss 0.514956, d_loss 0.478189 accuracy 0.578125\n",
      "Training in progress @ global_step 119750, g_loss 0.515305, d_loss 0.470364 accuracy 0.734375\n",
      "Training in progress @ global_step 119800, g_loss 0.510566, d_loss 0.464804 accuracy 0.796875\n",
      "Training in progress @ global_step 119850, g_loss 0.514635, d_loss 0.469285 accuracy 0.78125\n",
      "Training in progress @ global_step 119900, g_loss 0.510562, d_loss 0.471305 accuracy 0.765625\n",
      "Training in progress @ global_step 119950, g_loss 0.519158, d_loss 0.462734 accuracy 0.8125\n",
      "Training in progress @ global_step 120000, g_loss 0.514747, d_loss 0.465379 accuracy 0.8125\n",
      "Training in progress @ global_step 120050, g_loss 0.511726, d_loss 0.470238 accuracy 0.71875\n",
      "Training in progress @ global_step 120100, g_loss 0.515347, d_loss 0.470076 accuracy 0.71875\n",
      "Training in progress @ global_step 120150, g_loss 0.512586, d_loss 0.475972 accuracy 0.6875\n",
      "Training in progress @ global_step 120200, g_loss 0.519744, d_loss 0.467037 accuracy 0.765625\n",
      "Training in progress @ global_step 120250, g_loss 0.512271, d_loss 0.459268 accuracy 0.828125\n",
      "Training in progress @ global_step 120300, g_loss 0.512657, d_loss 0.466884 accuracy 0.828125\n",
      "Training in progress @ global_step 120350, g_loss 0.51296, d_loss 0.474206 accuracy 0.703125\n",
      "Training in progress @ global_step 120400, g_loss 0.510764, d_loss 0.464836 accuracy 0.765625\n",
      "Training in progress @ global_step 120450, g_loss 0.505087, d_loss 0.474895 accuracy 0.625\n",
      "Training in progress @ global_step 120500, g_loss 0.510526, d_loss 0.473851 accuracy 0.71875\n",
      "Training in progress @ global_step 120550, g_loss 0.5084, d_loss 0.47684 accuracy 0.78125\n",
      "Training in progress @ global_step 120600, g_loss 0.506773, d_loss 0.466382 accuracy 0.734375\n",
      "Training in progress @ global_step 120650, g_loss 0.509272, d_loss 0.476609 accuracy 0.640625\n",
      "Training in progress @ global_step 120700, g_loss 0.512235, d_loss 0.471487 accuracy 0.703125\n",
      "Training in progress @ global_step 120750, g_loss 0.519558, d_loss 0.475792 accuracy 0.65625\n",
      "Training in progress @ global_step 120800, g_loss 0.511545, d_loss 0.475303 accuracy 0.71875\n",
      "Training in progress @ global_step 120850, g_loss 0.511667, d_loss 0.4763 accuracy 0.703125\n",
      "Training in progress @ global_step 120900, g_loss 0.509546, d_loss 0.469392 accuracy 0.71875\n",
      "Training in progress @ global_step 120950, g_loss 0.508333, d_loss 0.468566 accuracy 0.765625\n",
      "Training in progress @ global_step 121000, g_loss 0.515234, d_loss 0.469975 accuracy 0.765625\n",
      "Training in progress @ global_step 121050, g_loss 0.514101, d_loss 0.475798 accuracy 0.75\n",
      "Training in progress @ global_step 121100, g_loss 0.517829, d_loss 0.465172 accuracy 0.8125\n",
      "Training in progress @ global_step 121150, g_loss 0.511575, d_loss 0.47408 accuracy 0.75\n",
      "Training in progress @ global_step 121200, g_loss 0.512306, d_loss 0.469867 accuracy 0.765625\n",
      "Training in progress @ global_step 121250, g_loss 0.510187, d_loss 0.46657 accuracy 0.828125\n",
      "Training in progress @ global_step 121300, g_loss 0.518111, d_loss 0.473567 accuracy 0.734375\n",
      "Training in progress @ global_step 121350, g_loss 0.520732, d_loss 0.466859 accuracy 0.796875\n",
      "Training in progress @ global_step 121400, g_loss 0.514049, d_loss 0.475417 accuracy 0.671875\n",
      "Training in progress @ global_step 121450, g_loss 0.514168, d_loss 0.47596 accuracy 0.765625\n",
      "Training in progress @ global_step 121500, g_loss 0.514655, d_loss 0.466136 accuracy 0.75\n",
      "Training in progress @ global_step 121550, g_loss 0.517538, d_loss 0.472681 accuracy 0.796875\n",
      "Training in progress @ global_step 121600, g_loss 0.518146, d_loss 0.474685 accuracy 0.71875\n",
      "Training in progress @ global_step 121650, g_loss 0.518799, d_loss 0.469853 accuracy 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 121700, g_loss 0.515729, d_loss 0.472063 accuracy 0.796875\n",
      "Training in progress @ global_step 121750, g_loss 0.513097, d_loss 0.469436 accuracy 0.765625\n",
      "Training in progress @ global_step 121800, g_loss 0.515552, d_loss 0.472388 accuracy 0.71875\n",
      "Training in progress @ global_step 121850, g_loss 0.518465, d_loss 0.469728 accuracy 0.75\n",
      "Training in progress @ global_step 121900, g_loss 0.517624, d_loss 0.469544 accuracy 0.75\n",
      "Training in progress @ global_step 121950, g_loss 0.517951, d_loss 0.475701 accuracy 0.71875\n",
      "Training in progress @ global_step 122000, g_loss 0.520539, d_loss 0.472656 accuracy 0.703125\n",
      "Training in progress @ global_step 122050, g_loss 0.520768, d_loss 0.482119 accuracy 0.640625\n",
      "Training in progress @ global_step 122100, g_loss 0.52016, d_loss 0.47592 accuracy 0.6875\n",
      "Training in progress @ global_step 122150, g_loss 0.523064, d_loss 0.475905 accuracy 0.71875\n",
      "Training in progress @ global_step 122200, g_loss 0.519174, d_loss 0.484162 accuracy 0.625\n",
      "Training in progress @ global_step 122250, g_loss 0.521891, d_loss 0.482419 accuracy 0.640625\n",
      "Training in progress @ global_step 122300, g_loss 0.519779, d_loss 0.475549 accuracy 0.765625\n",
      "Training in progress @ global_step 122350, g_loss 0.515773, d_loss 0.481853 accuracy 0.640625\n",
      "Training in progress @ global_step 122400, g_loss 0.515281, d_loss 0.471303 accuracy 0.734375\n",
      "Training in progress @ global_step 122450, g_loss 0.516212, d_loss 0.470073 accuracy 0.75\n",
      "Training in progress @ global_step 122500, g_loss 0.514271, d_loss 0.47483 accuracy 0.703125\n",
      "Training in progress @ global_step 122550, g_loss 0.522318, d_loss 0.476438 accuracy 0.71875\n",
      "Training in progress @ global_step 122600, g_loss 0.522561, d_loss 0.478068 accuracy 0.703125\n",
      "Training in progress @ global_step 122650, g_loss 0.51939, d_loss 0.472135 accuracy 0.75\n",
      "Training in progress @ global_step 122700, g_loss 0.51999, d_loss 0.474477 accuracy 0.71875\n",
      "Training in progress @ global_step 122750, g_loss 0.514189, d_loss 0.483689 accuracy 0.65625\n",
      "Training in progress @ global_step 122800, g_loss 0.515101, d_loss 0.481617 accuracy 0.65625\n",
      "Training in progress @ global_step 122850, g_loss 0.519615, d_loss 0.48926 accuracy 0.625\n",
      "Training in progress @ global_step 122900, g_loss 0.522014, d_loss 0.477 accuracy 0.75\n",
      "Training in progress @ global_step 122950, g_loss 0.523745, d_loss 0.479622 accuracy 0.703125\n",
      "Training in progress @ global_step 123000, g_loss 0.522169, d_loss 0.473466 accuracy 0.703125\n",
      "Training in progress @ global_step 123050, g_loss 0.519274, d_loss 0.47888 accuracy 0.6875\n",
      "Training in progress @ global_step 123100, g_loss 0.518508, d_loss 0.47892 accuracy 0.703125\n",
      "Training in progress @ global_step 123150, g_loss 0.519776, d_loss 0.476335 accuracy 0.703125\n",
      "Training in progress @ global_step 123200, g_loss 0.522637, d_loss 0.479085 accuracy 0.703125\n",
      "Training in progress @ global_step 123250, g_loss 0.522065, d_loss 0.482978 accuracy 0.65625\n",
      "Training in progress @ global_step 123300, g_loss 0.520548, d_loss 0.474967 accuracy 0.71875\n",
      "Training in progress @ global_step 123350, g_loss 0.51696, d_loss 0.471802 accuracy 0.765625\n",
      "Training in progress @ global_step 123400, g_loss 0.519085, d_loss 0.47905 accuracy 0.6875\n",
      "Training in progress @ global_step 123450, g_loss 0.5181, d_loss 0.4735 accuracy 0.765625\n",
      "Training in progress @ global_step 123500, g_loss 0.518887, d_loss 0.481139 accuracy 0.703125\n",
      "Training in progress @ global_step 123550, g_loss 0.521854, d_loss 0.482679 accuracy 0.609375\n",
      "Training in progress @ global_step 123600, g_loss 0.51987, d_loss 0.479555 accuracy 0.703125\n",
      "Training in progress @ global_step 123650, g_loss 0.522828, d_loss 0.481207 accuracy 0.71875\n",
      "Training in progress @ global_step 123700, g_loss 0.522341, d_loss 0.479594 accuracy 0.71875\n",
      "Training in progress @ global_step 123750, g_loss 0.521211, d_loss 0.478512 accuracy 0.6875\n",
      "Training in progress @ global_step 123800, g_loss 0.520705, d_loss 0.476372 accuracy 0.6875\n",
      "Training in progress @ global_step 123850, g_loss 0.520929, d_loss 0.482841 accuracy 0.71875\n",
      "Training in progress @ global_step 123900, g_loss 0.521283, d_loss 0.481429 accuracy 0.671875\n",
      "Training in progress @ global_step 123950, g_loss 0.524613, d_loss 0.477906 accuracy 0.671875\n",
      "Training in progress @ global_step 124000, g_loss 0.521853, d_loss 0.469538 accuracy 0.75\n",
      "Training in progress @ global_step 124050, g_loss 0.52299, d_loss 0.479137 accuracy 0.6875\n",
      "Training in progress @ global_step 124100, g_loss 0.524415, d_loss 0.476919 accuracy 0.71875\n",
      "Training in progress @ global_step 124150, g_loss 0.518658, d_loss 0.472087 accuracy 0.796875\n",
      "Training in progress @ global_step 124200, g_loss 0.519348, d_loss 0.475363 accuracy 0.6875\n",
      "Training in progress @ global_step 124250, g_loss 0.522029, d_loss 0.478555 accuracy 0.703125\n",
      "Training in progress @ global_step 124300, g_loss 0.523742, d_loss 0.478218 accuracy 0.75\n",
      "Training in progress @ global_step 124350, g_loss 0.521447, d_loss 0.482315 accuracy 0.703125\n",
      "Training in progress @ global_step 124400, g_loss 0.518361, d_loss 0.470365 accuracy 0.734375\n",
      "Training in progress @ global_step 124450, g_loss 0.52533, d_loss 0.484311 accuracy 0.6875\n",
      "Training in progress @ global_step 124500, g_loss 0.525759, d_loss 0.488734 accuracy 0.609375\n",
      "Training in progress @ global_step 124550, g_loss 0.524759, d_loss 0.481465 accuracy 0.65625\n",
      "Training in progress @ global_step 124600, g_loss 0.526127, d_loss 0.482185 accuracy 0.625\n",
      "Training in progress @ global_step 124650, g_loss 0.527328, d_loss 0.483437 accuracy 0.6875\n",
      "Training in progress @ global_step 124700, g_loss 0.526249, d_loss 0.479835 accuracy 0.703125\n",
      "Training in progress @ global_step 124750, g_loss 0.520706, d_loss 0.486851 accuracy 0.640625\n",
      "Training in progress @ global_step 124800, g_loss 0.524584, d_loss 0.483323 accuracy 0.71875\n",
      "Training in progress @ global_step 124850, g_loss 0.521979, d_loss 0.481202 accuracy 0.6875\n",
      "Training in progress @ global_step 124900, g_loss 0.521239, d_loss 0.479261 accuracy 0.671875\n",
      "Training in progress @ global_step 124950, g_loss 0.52373, d_loss 0.476826 accuracy 0.796875\n",
      "Training in progress @ global_step 125000, g_loss 0.524064, d_loss 0.48942 accuracy 0.625\n",
      "Training in progress @ global_step 125050, g_loss 0.521732, d_loss 0.473615 accuracy 0.75\n",
      "Training in progress @ global_step 125100, g_loss 0.520878, d_loss 0.485233 accuracy 0.640625\n",
      "Training in progress @ global_step 125150, g_loss 0.521734, d_loss 0.478668 accuracy 0.671875\n",
      "Training in progress @ global_step 125200, g_loss 0.527233, d_loss 0.485493 accuracy 0.640625\n",
      "Training in progress @ global_step 125250, g_loss 0.522039, d_loss 0.484643 accuracy 0.609375\n",
      "Training in progress @ global_step 125300, g_loss 0.528089, d_loss 0.489488 accuracy 0.609375\n",
      "Training in progress @ global_step 125350, g_loss 0.524555, d_loss 0.483555 accuracy 0.703125\n",
      "Training in progress @ global_step 125400, g_loss 0.528925, d_loss 0.482799 accuracy 0.625\n",
      "Training in progress @ global_step 125450, g_loss 0.528423, d_loss 0.483451 accuracy 0.6875\n",
      "Training in progress @ global_step 125500, g_loss 0.522779, d_loss 0.492358 accuracy 0.578125\n",
      "Training in progress @ global_step 125550, g_loss 0.526779, d_loss 0.49229 accuracy 0.5625\n",
      "Training in progress @ global_step 125600, g_loss 0.528462, d_loss 0.487213 accuracy 0.640625\n",
      "Training in progress @ global_step 125650, g_loss 0.527781, d_loss 0.48044 accuracy 0.6875\n",
      "Training in progress @ global_step 125700, g_loss 0.524088, d_loss 0.475015 accuracy 0.71875\n",
      "Training in progress @ global_step 125750, g_loss 0.524173, d_loss 0.483761 accuracy 0.65625\n",
      "Training in progress @ global_step 125800, g_loss 0.523887, d_loss 0.482763 accuracy 0.671875\n",
      "Training in progress @ global_step 125850, g_loss 0.525651, d_loss 0.48974 accuracy 0.59375\n",
      "Training in progress @ global_step 125900, g_loss 0.52718, d_loss 0.484056 accuracy 0.640625\n",
      "Training in progress @ global_step 125950, g_loss 0.526576, d_loss 0.486032 accuracy 0.609375\n",
      "Training in progress @ global_step 126000, g_loss 0.525731, d_loss 0.484587 accuracy 0.65625\n",
      "Training in progress @ global_step 126050, g_loss 0.526817, d_loss 0.48193 accuracy 0.65625\n",
      "Training in progress @ global_step 126100, g_loss 0.528712, d_loss 0.486524 accuracy 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 126150, g_loss 0.527874, d_loss 0.488417 accuracy 0.65625\n",
      "Training in progress @ global_step 126200, g_loss 0.529414, d_loss 0.486151 accuracy 0.640625\n",
      "Training in progress @ global_step 126250, g_loss 0.526755, d_loss 0.479586 accuracy 0.6875\n",
      "Training in progress @ global_step 126300, g_loss 0.527759, d_loss 0.491512 accuracy 0.609375\n",
      "Training in progress @ global_step 126350, g_loss 0.531702, d_loss 0.478678 accuracy 0.6875\n",
      "Training in progress @ global_step 126400, g_loss 0.529822, d_loss 0.488462 accuracy 0.640625\n",
      "Training in progress @ global_step 126450, g_loss 0.529807, d_loss 0.486728 accuracy 0.609375\n",
      "Training in progress @ global_step 126500, g_loss 0.529037, d_loss 0.490519 accuracy 0.609375\n",
      "Training in progress @ global_step 126550, g_loss 0.530108, d_loss 0.489955 accuracy 0.578125\n",
      "Training in progress @ global_step 126600, g_loss 0.526802, d_loss 0.487818 accuracy 0.59375\n",
      "Training in progress @ global_step 126650, g_loss 0.52929, d_loss 0.48668 accuracy 0.625\n",
      "Training in progress @ global_step 126700, g_loss 0.528069, d_loss 0.491589 accuracy 0.578125\n",
      "Training in progress @ global_step 126750, g_loss 0.528184, d_loss 0.490436 accuracy 0.59375\n",
      "Training in progress @ global_step 126800, g_loss 0.528926, d_loss 0.487937 accuracy 0.625\n",
      "Training in progress @ global_step 126850, g_loss 0.530354, d_loss 0.488553 accuracy 0.625\n",
      "Training in progress @ global_step 126900, g_loss 0.531545, d_loss 0.488927 accuracy 0.65625\n",
      "Training in progress @ global_step 126950, g_loss 0.530772, d_loss 0.489656 accuracy 0.59375\n",
      "Training in progress @ global_step 127000, g_loss 0.530908, d_loss 0.490072 accuracy 0.578125\n",
      "Training in progress @ global_step 127050, g_loss 0.531391, d_loss 0.48876 accuracy 0.59375\n",
      "Training in progress @ global_step 127100, g_loss 0.529399, d_loss 0.49544 accuracy 0.5625\n",
      "Training in progress @ global_step 127150, g_loss 0.529686, d_loss 0.488846 accuracy 0.625\n",
      "Training in progress @ global_step 127200, g_loss 0.531941, d_loss 0.49546 accuracy 0.5625\n",
      "Training in progress @ global_step 127250, g_loss 0.530637, d_loss 0.489868 accuracy 0.625\n",
      "Training in progress @ global_step 127300, g_loss 0.529669, d_loss 0.489774 accuracy 0.578125\n",
      "Training in progress @ global_step 127350, g_loss 0.530351, d_loss 0.484703 accuracy 0.640625\n",
      "Training in progress @ global_step 127400, g_loss 0.532035, d_loss 0.488338 accuracy 0.625\n",
      "Training in progress @ global_step 127450, g_loss 0.535064, d_loss 0.489862 accuracy 0.59375\n",
      "Training in progress @ global_step 127500, g_loss 0.533385, d_loss 0.477028 accuracy 0.71875\n",
      "Training in progress @ global_step 127550, g_loss 0.525696, d_loss 0.488203 accuracy 0.625\n",
      "Training in progress @ global_step 127600, g_loss 0.531317, d_loss 0.486947 accuracy 0.625\n",
      "Training in progress @ global_step 127650, g_loss 0.528678, d_loss 0.48686 accuracy 0.625\n",
      "Training in progress @ global_step 127700, g_loss 0.529422, d_loss 0.491361 accuracy 0.5625\n",
      "Training in progress @ global_step 127750, g_loss 0.528264, d_loss 0.495467 accuracy 0.515625\n",
      "Training in progress @ global_step 127800, g_loss 0.533477, d_loss 0.489372 accuracy 0.609375\n",
      "Training in progress @ global_step 127850, g_loss 0.530757, d_loss 0.483426 accuracy 0.640625\n",
      "Training in progress @ global_step 127900, g_loss 0.530519, d_loss 0.48062 accuracy 0.65625\n",
      "Training in progress @ global_step 127950, g_loss 0.529348, d_loss 0.485681 accuracy 0.65625\n",
      "Training in progress @ global_step 128000, g_loss 0.528103, d_loss 0.490413 accuracy 0.59375\n",
      "Training in progress @ global_step 128050, g_loss 0.530765, d_loss 0.488058 accuracy 0.625\n",
      "Training in progress @ global_step 128100, g_loss 0.528755, d_loss 0.486825 accuracy 0.640625\n",
      "Training in progress @ global_step 128150, g_loss 0.528538, d_loss 0.485891 accuracy 0.65625\n",
      "Training in progress @ global_step 128200, g_loss 0.529627, d_loss 0.488401 accuracy 0.625\n",
      "Training in progress @ global_step 128250, g_loss 0.529744, d_loss 0.484257 accuracy 0.625\n",
      "Training in progress @ global_step 128300, g_loss 0.527202, d_loss 0.483924 accuracy 0.625\n",
      "Training in progress @ global_step 128350, g_loss 0.530728, d_loss 0.486029 accuracy 0.59375\n",
      "Training in progress @ global_step 128400, g_loss 0.530294, d_loss 0.489592 accuracy 0.59375\n",
      "Training in progress @ global_step 128450, g_loss 0.530623, d_loss 0.486355 accuracy 0.59375\n",
      "Training in progress @ global_step 128500, g_loss 0.527932, d_loss 0.483545 accuracy 0.640625\n",
      "Training in progress @ global_step 128550, g_loss 0.531876, d_loss 0.47981 accuracy 0.671875\n",
      "Training in progress @ global_step 128600, g_loss 0.52872, d_loss 0.481002 accuracy 0.640625\n",
      "Training in progress @ global_step 128650, g_loss 0.527933, d_loss 0.490098 accuracy 0.625\n",
      "Training in progress @ global_step 128700, g_loss 0.525301, d_loss 0.48429 accuracy 0.625\n",
      "Training in progress @ global_step 128750, g_loss 0.527577, d_loss 0.483392 accuracy 0.703125\n",
      "Training in progress @ global_step 128800, g_loss 0.529012, d_loss 0.48734 accuracy 0.625\n",
      "Training in progress @ global_step 128850, g_loss 0.529825, d_loss 0.485159 accuracy 0.640625\n",
      "Training in progress @ global_step 128900, g_loss 0.524655, d_loss 0.483186 accuracy 0.625\n",
      "Training in progress @ global_step 128950, g_loss 0.528022, d_loss 0.486384 accuracy 0.640625\n",
      "Training in progress @ global_step 129000, g_loss 0.528782, d_loss 0.480081 accuracy 0.671875\n",
      "Training in progress @ global_step 129050, g_loss 0.529086, d_loss 0.481151 accuracy 0.65625\n",
      "Training in progress @ global_step 129100, g_loss 0.529891, d_loss 0.48185 accuracy 0.625\n",
      "Training in progress @ global_step 129150, g_loss 0.526686, d_loss 0.475338 accuracy 0.703125\n",
      "Training in progress @ global_step 129200, g_loss 0.527384, d_loss 0.481512 accuracy 0.671875\n",
      "Training in progress @ global_step 129250, g_loss 0.528497, d_loss 0.480459 accuracy 0.640625\n",
      "Training in progress @ global_step 129300, g_loss 0.52623, d_loss 0.483451 accuracy 0.671875\n",
      "Training in progress @ global_step 129350, g_loss 0.527086, d_loss 0.487607 accuracy 0.625\n",
      "Training in progress @ global_step 129400, g_loss 0.525223, d_loss 0.481182 accuracy 0.671875\n",
      "Training in progress @ global_step 129450, g_loss 0.526231, d_loss 0.484104 accuracy 0.625\n",
      "Training in progress @ global_step 129500, g_loss 0.523905, d_loss 0.481063 accuracy 0.640625\n",
      "Training in progress @ global_step 129550, g_loss 0.523808, d_loss 0.478044 accuracy 0.671875\n",
      "Training in progress @ global_step 129600, g_loss 0.526214, d_loss 0.480729 accuracy 0.671875\n",
      "Training in progress @ global_step 129650, g_loss 0.525146, d_loss 0.482661 accuracy 0.640625\n",
      "Training in progress @ global_step 129700, g_loss 0.527234, d_loss 0.488188 accuracy 0.609375\n",
      "Training in progress @ global_step 129750, g_loss 0.527357, d_loss 0.485954 accuracy 0.578125\n",
      "Training in progress @ global_step 129800, g_loss 0.525043, d_loss 0.483724 accuracy 0.625\n",
      "Training in progress @ global_step 129850, g_loss 0.525712, d_loss 0.476497 accuracy 0.71875\n",
      "Training in progress @ global_step 129900, g_loss 0.529116, d_loss 0.479173 accuracy 0.6875\n",
      "Training in progress @ global_step 129950, g_loss 0.525563, d_loss 0.482625 accuracy 0.6875\n",
      "Training in progress @ global_step 130000, g_loss 0.524959, d_loss 0.480526 accuracy 0.671875\n",
      "Training in progress @ global_step 130050, g_loss 0.528106, d_loss 0.477598 accuracy 0.6875\n",
      "Training in progress @ global_step 130100, g_loss 0.528258, d_loss 0.476067 accuracy 0.703125\n",
      "Training in progress @ global_step 130150, g_loss 0.529094, d_loss 0.47706 accuracy 0.671875\n",
      "Training in progress @ global_step 130200, g_loss 0.522638, d_loss 0.480621 accuracy 0.703125\n",
      "Training in progress @ global_step 130250, g_loss 0.526647, d_loss 0.478142 accuracy 0.6875\n",
      "Training in progress @ global_step 130300, g_loss 0.525959, d_loss 0.477672 accuracy 0.671875\n",
      "Training in progress @ global_step 130350, g_loss 0.525691, d_loss 0.475611 accuracy 0.6875\n",
      "Training in progress @ global_step 130400, g_loss 0.525783, d_loss 0.486084 accuracy 0.65625\n",
      "Training in progress @ global_step 130450, g_loss 0.524756, d_loss 0.480427 accuracy 0.65625\n",
      "Training in progress @ global_step 130500, g_loss 0.527679, d_loss 0.485232 accuracy 0.640625\n",
      "Training in progress @ global_step 130550, g_loss 0.52651, d_loss 0.480164 accuracy 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 130600, g_loss 0.525073, d_loss 0.481471 accuracy 0.671875\n",
      "Training in progress @ global_step 130650, g_loss 0.524978, d_loss 0.48606 accuracy 0.671875\n",
      "Training in progress @ global_step 130700, g_loss 0.525715, d_loss 0.481084 accuracy 0.65625\n",
      "Training in progress @ global_step 130750, g_loss 0.527309, d_loss 0.482065 accuracy 0.65625\n",
      "Training in progress @ global_step 130800, g_loss 0.527348, d_loss 0.478231 accuracy 0.65625\n",
      "Training in progress @ global_step 130850, g_loss 0.525942, d_loss 0.484242 accuracy 0.609375\n",
      "Training in progress @ global_step 130900, g_loss 0.525963, d_loss 0.486262 accuracy 0.625\n",
      "Training in progress @ global_step 130950, g_loss 0.52909, d_loss 0.471177 accuracy 0.703125\n",
      "Training in progress @ global_step 131000, g_loss 0.527974, d_loss 0.479404 accuracy 0.703125\n",
      "Training in progress @ global_step 131050, g_loss 0.526983, d_loss 0.477742 accuracy 0.65625\n",
      "Training in progress @ global_step 131100, g_loss 0.526658, d_loss 0.482385 accuracy 0.6875\n",
      "Training in progress @ global_step 131150, g_loss 0.525653, d_loss 0.482394 accuracy 0.671875\n",
      "Training in progress @ global_step 131200, g_loss 0.527296, d_loss 0.479126 accuracy 0.671875\n",
      "Training in progress @ global_step 131250, g_loss 0.524171, d_loss 0.484509 accuracy 0.640625\n",
      "Training in progress @ global_step 131300, g_loss 0.524959, d_loss 0.481749 accuracy 0.65625\n",
      "Training in progress @ global_step 131350, g_loss 0.524253, d_loss 0.476881 accuracy 0.6875\n",
      "Training in progress @ global_step 131400, g_loss 0.52561, d_loss 0.477061 accuracy 0.703125\n",
      "Training in progress @ global_step 131450, g_loss 0.528046, d_loss 0.477092 accuracy 0.703125\n",
      "Training in progress @ global_step 131500, g_loss 0.525489, d_loss 0.474408 accuracy 0.71875\n",
      "Training in progress @ global_step 131550, g_loss 0.525708, d_loss 0.474156 accuracy 0.703125\n",
      "Training in progress @ global_step 131600, g_loss 0.526208, d_loss 0.479463 accuracy 0.671875\n",
      "Training in progress @ global_step 131650, g_loss 0.52511, d_loss 0.473371 accuracy 0.734375\n",
      "Training in progress @ global_step 131700, g_loss 0.523119, d_loss 0.479724 accuracy 0.703125\n",
      "Training in progress @ global_step 131750, g_loss 0.524946, d_loss 0.483132 accuracy 0.640625\n",
      "Training in progress @ global_step 131800, g_loss 0.526178, d_loss 0.475144 accuracy 0.640625\n",
      "Training in progress @ global_step 131850, g_loss 0.526929, d_loss 0.479635 accuracy 0.625\n",
      "Training in progress @ global_step 131900, g_loss 0.523624, d_loss 0.47546 accuracy 0.734375\n",
      "Training in progress @ global_step 131950, g_loss 0.525785, d_loss 0.473725 accuracy 0.703125\n",
      "Training in progress @ global_step 132000, g_loss 0.524307, d_loss 0.472014 accuracy 0.734375\n",
      "Training in progress @ global_step 132050, g_loss 0.525412, d_loss 0.481953 accuracy 0.625\n",
      "Training in progress @ global_step 132100, g_loss 0.524171, d_loss 0.473484 accuracy 0.71875\n",
      "Training in progress @ global_step 132150, g_loss 0.523589, d_loss 0.473941 accuracy 0.6875\n",
      "Training in progress @ global_step 132200, g_loss 0.524498, d_loss 0.477334 accuracy 0.703125\n",
      "Training in progress @ global_step 132250, g_loss 0.522156, d_loss 0.467419 accuracy 0.71875\n",
      "Training in progress @ global_step 132300, g_loss 0.522331, d_loss 0.473553 accuracy 0.6875\n",
      "Training in progress @ global_step 132350, g_loss 0.523347, d_loss 0.475063 accuracy 0.671875\n",
      "Training in progress @ global_step 132400, g_loss 0.521542, d_loss 0.478943 accuracy 0.625\n",
      "Training in progress @ global_step 132450, g_loss 0.524628, d_loss 0.467689 accuracy 0.8125\n",
      "Training in progress @ global_step 132500, g_loss 0.52244, d_loss 0.467939 accuracy 0.78125\n",
      "Training in progress @ global_step 132550, g_loss 0.525544, d_loss 0.476172 accuracy 0.671875\n",
      "Training in progress @ global_step 132600, g_loss 0.521962, d_loss 0.47455 accuracy 0.703125\n",
      "Training in progress @ global_step 132650, g_loss 0.525909, d_loss 0.478624 accuracy 0.65625\n",
      "Training in progress @ global_step 132700, g_loss 0.523767, d_loss 0.471301 accuracy 0.734375\n",
      "Training in progress @ global_step 132750, g_loss 0.521678, d_loss 0.471746 accuracy 0.71875\n",
      "Training in progress @ global_step 132800, g_loss 0.520918, d_loss 0.481511 accuracy 0.6875\n",
      "Training in progress @ global_step 132850, g_loss 0.520713, d_loss 0.471764 accuracy 0.78125\n",
      "Training in progress @ global_step 132900, g_loss 0.518932, d_loss 0.476725 accuracy 0.609375\n",
      "Training in progress @ global_step 132950, g_loss 0.523131, d_loss 0.47095 accuracy 0.71875\n",
      "Training in progress @ global_step 133000, g_loss 0.519317, d_loss 0.46888 accuracy 0.765625\n",
      "Training in progress @ global_step 133050, g_loss 0.521812, d_loss 0.470027 accuracy 0.71875\n",
      "Training in progress @ global_step 133100, g_loss 0.524435, d_loss 0.474111 accuracy 0.75\n",
      "Training in progress @ global_step 133150, g_loss 0.523931, d_loss 0.473661 accuracy 0.703125\n",
      "Training in progress @ global_step 133200, g_loss 0.524165, d_loss 0.476393 accuracy 0.734375\n",
      "Training in progress @ global_step 133250, g_loss 0.522836, d_loss 0.475868 accuracy 0.71875\n",
      "Training in progress @ global_step 133300, g_loss 0.521261, d_loss 0.476603 accuracy 0.71875\n",
      "Training in progress @ global_step 133350, g_loss 0.520722, d_loss 0.471864 accuracy 0.734375\n",
      "Training in progress @ global_step 133400, g_loss 0.523893, d_loss 0.471398 accuracy 0.765625\n",
      "Training in progress @ global_step 133450, g_loss 0.520082, d_loss 0.482745 accuracy 0.640625\n",
      "Training in progress @ global_step 133500, g_loss 0.522372, d_loss 0.474871 accuracy 0.671875\n",
      "Training in progress @ global_step 133550, g_loss 0.519475, d_loss 0.467857 accuracy 0.78125\n",
      "Training in progress @ global_step 133600, g_loss 0.518524, d_loss 0.465349 accuracy 0.796875\n",
      "Training in progress @ global_step 133650, g_loss 0.51971, d_loss 0.469813 accuracy 0.734375\n",
      "Training in progress @ global_step 133700, g_loss 0.519389, d_loss 0.473831 accuracy 0.71875\n",
      "Training in progress @ global_step 133750, g_loss 0.520718, d_loss 0.462371 accuracy 0.78125\n",
      "Training in progress @ global_step 133800, g_loss 0.522312, d_loss 0.468515 accuracy 0.703125\n",
      "Training in progress @ global_step 133850, g_loss 0.523837, d_loss 0.470659 accuracy 0.71875\n",
      "Training in progress @ global_step 133900, g_loss 0.524455, d_loss 0.475426 accuracy 0.71875\n",
      "Training in progress @ global_step 133950, g_loss 0.523025, d_loss 0.475334 accuracy 0.734375\n",
      "Training in progress @ global_step 134000, g_loss 0.519639, d_loss 0.464791 accuracy 0.8125\n",
      "Training in progress @ global_step 134050, g_loss 0.519429, d_loss 0.474403 accuracy 0.765625\n",
      "Training in progress @ global_step 134100, g_loss 0.522135, d_loss 0.474976 accuracy 0.71875\n",
      "Training in progress @ global_step 134150, g_loss 0.521453, d_loss 0.471207 accuracy 0.6875\n",
      "Training in progress @ global_step 134200, g_loss 0.520197, d_loss 0.473174 accuracy 0.703125\n",
      "Training in progress @ global_step 134250, g_loss 0.521527, d_loss 0.469204 accuracy 0.78125\n",
      "Training in progress @ global_step 134300, g_loss 0.516816, d_loss 0.462413 accuracy 0.8125\n",
      "Training in progress @ global_step 134350, g_loss 0.52186, d_loss 0.475254 accuracy 0.734375\n",
      "Training in progress @ global_step 134400, g_loss 0.522094, d_loss 0.475805 accuracy 0.71875\n",
      "Training in progress @ global_step 134450, g_loss 0.521245, d_loss 0.466789 accuracy 0.828125\n",
      "Training in progress @ global_step 134500, g_loss 0.523352, d_loss 0.471164 accuracy 0.75\n",
      "Training in progress @ global_step 134550, g_loss 0.518893, d_loss 0.469668 accuracy 0.765625\n",
      "Training in progress @ global_step 134600, g_loss 0.520957, d_loss 0.473222 accuracy 0.71875\n",
      "Training in progress @ global_step 134650, g_loss 0.519095, d_loss 0.470498 accuracy 0.796875\n",
      "Training in progress @ global_step 134700, g_loss 0.517827, d_loss 0.46305 accuracy 0.796875\n",
      "Training in progress @ global_step 134750, g_loss 0.519631, d_loss 0.469727 accuracy 0.78125\n",
      "Training in progress @ global_step 134800, g_loss 0.518795, d_loss 0.46102 accuracy 0.84375\n",
      "Training in progress @ global_step 134850, g_loss 0.522533, d_loss 0.476916 accuracy 0.71875\n",
      "Training in progress @ global_step 134900, g_loss 0.519627, d_loss 0.463768 accuracy 0.75\n",
      "Training in progress @ global_step 134950, g_loss 0.520444, d_loss 0.464507 accuracy 0.828125\n",
      "Training in progress @ global_step 135000, g_loss 0.518304, d_loss 0.466468 accuracy 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 135050, g_loss 0.517753, d_loss 0.463834 accuracy 0.8125\n",
      "Training in progress @ global_step 135100, g_loss 0.521081, d_loss 0.473429 accuracy 0.703125\n",
      "Training in progress @ global_step 135150, g_loss 0.522817, d_loss 0.475245 accuracy 0.671875\n",
      "Training in progress @ global_step 135200, g_loss 0.521394, d_loss 0.467138 accuracy 0.78125\n",
      "Training in progress @ global_step 135250, g_loss 0.519764, d_loss 0.469175 accuracy 0.765625\n",
      "Training in progress @ global_step 135300, g_loss 0.51957, d_loss 0.46599 accuracy 0.828125\n",
      "Training in progress @ global_step 135350, g_loss 0.517043, d_loss 0.468114 accuracy 0.8125\n",
      "Training in progress @ global_step 135400, g_loss 0.521464, d_loss 0.471414 accuracy 0.796875\n",
      "Training in progress @ global_step 135450, g_loss 0.518702, d_loss 0.468831 accuracy 0.75\n",
      "Training in progress @ global_step 135500, g_loss 0.519794, d_loss 0.465839 accuracy 0.828125\n",
      "Training in progress @ global_step 135550, g_loss 0.516832, d_loss 0.466303 accuracy 0.796875\n",
      "Training in progress @ global_step 135600, g_loss 0.516192, d_loss 0.459565 accuracy 0.8125\n",
      "Training in progress @ global_step 135650, g_loss 0.520099, d_loss 0.460144 accuracy 0.8125\n",
      "Training in progress @ global_step 135700, g_loss 0.518198, d_loss 0.474064 accuracy 0.734375\n",
      "Training in progress @ global_step 135750, g_loss 0.517189, d_loss 0.467594 accuracy 0.8125\n",
      "Training in progress @ global_step 135800, g_loss 0.516971, d_loss 0.471134 accuracy 0.765625\n",
      "Training in progress @ global_step 135850, g_loss 0.519731, d_loss 0.465289 accuracy 0.828125\n",
      "Training in progress @ global_step 135900, g_loss 0.519273, d_loss 0.46651 accuracy 0.796875\n",
      "Training in progress @ global_step 135950, g_loss 0.518187, d_loss 0.468658 accuracy 0.78125\n",
      "Training in progress @ global_step 136000, g_loss 0.517996, d_loss 0.46498 accuracy 0.78125\n",
      "Training in progress @ global_step 136050, g_loss 0.515546, d_loss 0.462819 accuracy 0.796875\n",
      "Training in progress @ global_step 136100, g_loss 0.520646, d_loss 0.464781 accuracy 0.796875\n",
      "Training in progress @ global_step 136150, g_loss 0.519894, d_loss 0.471772 accuracy 0.71875\n",
      "Training in progress @ global_step 136200, g_loss 0.521382, d_loss 0.466702 accuracy 0.796875\n",
      "Training in progress @ global_step 136250, g_loss 0.519411, d_loss 0.464331 accuracy 0.828125\n",
      "Training in progress @ global_step 136300, g_loss 0.516543, d_loss 0.467785 accuracy 0.765625\n",
      "Training in progress @ global_step 136350, g_loss 0.519637, d_loss 0.46844 accuracy 0.796875\n",
      "Training in progress @ global_step 136400, g_loss 0.51837, d_loss 0.466186 accuracy 0.78125\n",
      "Training in progress @ global_step 136450, g_loss 0.519923, d_loss 0.464821 accuracy 0.8125\n",
      "Training in progress @ global_step 136500, g_loss 0.518055, d_loss 0.467445 accuracy 0.828125\n",
      "Training in progress @ global_step 136550, g_loss 0.518392, d_loss 0.460708 accuracy 0.875\n",
      "Training in progress @ global_step 136600, g_loss 0.517606, d_loss 0.463228 accuracy 0.890625\n",
      "Training in progress @ global_step 136650, g_loss 0.517986, d_loss 0.464123 accuracy 0.8125\n",
      "Training in progress @ global_step 136700, g_loss 0.515505, d_loss 0.468674 accuracy 0.703125\n",
      "Training in progress @ global_step 136750, g_loss 0.518098, d_loss 0.465434 accuracy 0.8125\n",
      "Training in progress @ global_step 136800, g_loss 0.519816, d_loss 0.465447 accuracy 0.78125\n",
      "Training in progress @ global_step 136850, g_loss 0.52008, d_loss 0.464541 accuracy 0.8125\n",
      "Training in progress @ global_step 136900, g_loss 0.519791, d_loss 0.466551 accuracy 0.765625\n",
      "Training in progress @ global_step 136950, g_loss 0.520489, d_loss 0.478046 accuracy 0.71875\n",
      "Training in progress @ global_step 137000, g_loss 0.519131, d_loss 0.469031 accuracy 0.78125\n",
      "Training in progress @ global_step 137050, g_loss 0.515158, d_loss 0.469173 accuracy 0.765625\n",
      "Training in progress @ global_step 137100, g_loss 0.516376, d_loss 0.463555 accuracy 0.8125\n",
      "Training in progress @ global_step 137150, g_loss 0.517081, d_loss 0.472056 accuracy 0.78125\n",
      "Training in progress @ global_step 137200, g_loss 0.516469, d_loss 0.467458 accuracy 0.78125\n",
      "Training in progress @ global_step 137250, g_loss 0.515883, d_loss 0.462935 accuracy 0.796875\n",
      "Training in progress @ global_step 137300, g_loss 0.516582, d_loss 0.468713 accuracy 0.796875\n",
      "Training in progress @ global_step 137350, g_loss 0.518216, d_loss 0.461655 accuracy 0.8125\n",
      "Training in progress @ global_step 137400, g_loss 0.517579, d_loss 0.470907 accuracy 0.765625\n",
      "Training in progress @ global_step 137450, g_loss 0.515505, d_loss 0.470184 accuracy 0.75\n",
      "Training in progress @ global_step 137500, g_loss 0.517863, d_loss 0.466494 accuracy 0.8125\n",
      "Training in progress @ global_step 137550, g_loss 0.519863, d_loss 0.463537 accuracy 0.828125\n",
      "Training in progress @ global_step 137600, g_loss 0.514882, d_loss 0.468855 accuracy 0.78125\n",
      "Training in progress @ global_step 137650, g_loss 0.516591, d_loss 0.465104 accuracy 0.78125\n",
      "Training in progress @ global_step 137700, g_loss 0.517826, d_loss 0.468921 accuracy 0.75\n",
      "Training in progress @ global_step 137750, g_loss 0.517948, d_loss 0.469783 accuracy 0.78125\n",
      "Training in progress @ global_step 137800, g_loss 0.517311, d_loss 0.470443 accuracy 0.75\n",
      "Training in progress @ global_step 137850, g_loss 0.521524, d_loss 0.471798 accuracy 0.75\n",
      "Training in progress @ global_step 137900, g_loss 0.517586, d_loss 0.466979 accuracy 0.796875\n",
      "Training in progress @ global_step 137950, g_loss 0.519354, d_loss 0.471316 accuracy 0.765625\n",
      "Training in progress @ global_step 138000, g_loss 0.515736, d_loss 0.467278 accuracy 0.71875\n",
      "Training in progress @ global_step 138050, g_loss 0.51514, d_loss 0.468791 accuracy 0.765625\n",
      "Training in progress @ global_step 138100, g_loss 0.521449, d_loss 0.463248 accuracy 0.8125\n",
      "Training in progress @ global_step 138150, g_loss 0.520116, d_loss 0.471129 accuracy 0.78125\n",
      "Training in progress @ global_step 138200, g_loss 0.514874, d_loss 0.471374 accuracy 0.765625\n",
      "Training in progress @ global_step 138250, g_loss 0.518201, d_loss 0.47079 accuracy 0.796875\n",
      "Training in progress @ global_step 138300, g_loss 0.519099, d_loss 0.471835 accuracy 0.703125\n",
      "Training in progress @ global_step 138350, g_loss 0.519604, d_loss 0.47033 accuracy 0.78125\n",
      "Training in progress @ global_step 138400, g_loss 0.518716, d_loss 0.466871 accuracy 0.765625\n",
      "Training in progress @ global_step 138450, g_loss 0.519583, d_loss 0.468051 accuracy 0.8125\n",
      "Training in progress @ global_step 138500, g_loss 0.517401, d_loss 0.468458 accuracy 0.765625\n",
      "Training in progress @ global_step 138550, g_loss 0.513342, d_loss 0.459297 accuracy 0.84375\n",
      "Training in progress @ global_step 138600, g_loss 0.520799, d_loss 0.464922 accuracy 0.796875\n",
      "Training in progress @ global_step 138650, g_loss 0.516959, d_loss 0.461787 accuracy 0.8125\n",
      "Training in progress @ global_step 138700, g_loss 0.515397, d_loss 0.467277 accuracy 0.765625\n",
      "Training in progress @ global_step 138750, g_loss 0.515969, d_loss 0.46518 accuracy 0.828125\n",
      "Training in progress @ global_step 138800, g_loss 0.516735, d_loss 0.460373 accuracy 0.921875\n",
      "Training in progress @ global_step 138850, g_loss 0.515293, d_loss 0.465419 accuracy 0.8125\n",
      "Training in progress @ global_step 138900, g_loss 0.517696, d_loss 0.469067 accuracy 0.75\n",
      "Training in progress @ global_step 138950, g_loss 0.518044, d_loss 0.469801 accuracy 0.78125\n",
      "Training in progress @ global_step 139000, g_loss 0.511596, d_loss 0.46767 accuracy 0.765625\n",
      "Training in progress @ global_step 139050, g_loss 0.517998, d_loss 0.468103 accuracy 0.765625\n",
      "Training in progress @ global_step 139100, g_loss 0.513, d_loss 0.467051 accuracy 0.84375\n",
      "Training in progress @ global_step 139150, g_loss 0.516688, d_loss 0.464938 accuracy 0.796875\n",
      "Training in progress @ global_step 139200, g_loss 0.516108, d_loss 0.46448 accuracy 0.8125\n",
      "Training in progress @ global_step 139250, g_loss 0.513739, d_loss 0.462808 accuracy 0.828125\n",
      "Training in progress @ global_step 139300, g_loss 0.514131, d_loss 0.459108 accuracy 0.828125\n",
      "Training in progress @ global_step 139350, g_loss 0.519661, d_loss 0.462096 accuracy 0.828125\n",
      "Training in progress @ global_step 139400, g_loss 0.520064, d_loss 0.467778 accuracy 0.796875\n",
      "Training in progress @ global_step 139450, g_loss 0.521206, d_loss 0.466347 accuracy 0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 139500, g_loss 0.519717, d_loss 0.45927 accuracy 0.828125\n",
      "Training in progress @ global_step 139550, g_loss 0.51714, d_loss 0.465938 accuracy 0.78125\n",
      "Training in progress @ global_step 139600, g_loss 0.515168, d_loss 0.462827 accuracy 0.8125\n",
      "Training in progress @ global_step 139650, g_loss 0.515418, d_loss 0.466709 accuracy 0.765625\n",
      "Training in progress @ global_step 139700, g_loss 0.516435, d_loss 0.460313 accuracy 0.8125\n",
      "Training in progress @ global_step 139750, g_loss 0.516568, d_loss 0.472064 accuracy 0.765625\n",
      "Training in progress @ global_step 139800, g_loss 0.517492, d_loss 0.459184 accuracy 0.84375\n",
      "Training in progress @ global_step 139850, g_loss 0.518264, d_loss 0.461224 accuracy 0.796875\n",
      "Training in progress @ global_step 139900, g_loss 0.516123, d_loss 0.465329 accuracy 0.75\n",
      "Training in progress @ global_step 139950, g_loss 0.513444, d_loss 0.464551 accuracy 0.78125\n",
      "Training in progress @ global_step 140000, g_loss 0.51921, d_loss 0.467553 accuracy 0.796875\n",
      "Training in progress @ global_step 140050, g_loss 0.518361, d_loss 0.460759 accuracy 0.828125\n",
      "Training in progress @ global_step 140100, g_loss 0.517695, d_loss 0.463464 accuracy 0.765625\n",
      "Training in progress @ global_step 140150, g_loss 0.516356, d_loss 0.46387 accuracy 0.796875\n",
      "Training in progress @ global_step 140200, g_loss 0.519899, d_loss 0.471879 accuracy 0.71875\n",
      "Training in progress @ global_step 140250, g_loss 0.520843, d_loss 0.472811 accuracy 0.703125\n",
      "Training in progress @ global_step 140300, g_loss 0.520447, d_loss 0.468024 accuracy 0.84375\n",
      "Training in progress @ global_step 140350, g_loss 0.519213, d_loss 0.465457 accuracy 0.828125\n",
      "Training in progress @ global_step 140400, g_loss 0.518868, d_loss 0.457996 accuracy 0.796875\n",
      "Training in progress @ global_step 140450, g_loss 0.516389, d_loss 0.468396 accuracy 0.8125\n",
      "Training in progress @ global_step 140500, g_loss 0.517321, d_loss 0.459065 accuracy 0.8125\n",
      "Training in progress @ global_step 140550, g_loss 0.517926, d_loss 0.464639 accuracy 0.84375\n",
      "Training in progress @ global_step 140600, g_loss 0.516747, d_loss 0.466018 accuracy 0.8125\n",
      "Training in progress @ global_step 140650, g_loss 0.514758, d_loss 0.457445 accuracy 0.921875\n",
      "Training in progress @ global_step 140700, g_loss 0.513684, d_loss 0.469641 accuracy 0.75\n",
      "Training in progress @ global_step 140750, g_loss 0.512928, d_loss 0.467313 accuracy 0.765625\n",
      "Training in progress @ global_step 140800, g_loss 0.517445, d_loss 0.452785 accuracy 0.90625\n",
      "Training in progress @ global_step 140850, g_loss 0.514352, d_loss 0.461463 accuracy 0.859375\n",
      "Training in progress @ global_step 140900, g_loss 0.515473, d_loss 0.463192 accuracy 0.859375\n",
      "Training in progress @ global_step 140950, g_loss 0.517734, d_loss 0.466942 accuracy 0.828125\n",
      "Training in progress @ global_step 141000, g_loss 0.516821, d_loss 0.462075 accuracy 0.84375\n",
      "Training in progress @ global_step 141050, g_loss 0.51501, d_loss 0.459172 accuracy 0.875\n",
      "Training in progress @ global_step 141100, g_loss 0.517768, d_loss 0.460491 accuracy 0.875\n",
      "Training in progress @ global_step 141150, g_loss 0.515557, d_loss 0.459771 accuracy 0.859375\n",
      "Training in progress @ global_step 141200, g_loss 0.511251, d_loss 0.458521 accuracy 0.828125\n",
      "Training in progress @ global_step 141250, g_loss 0.509034, d_loss 0.459775 accuracy 0.828125\n",
      "Training in progress @ global_step 141300, g_loss 0.515106, d_loss 0.462861 accuracy 0.859375\n",
      "Training in progress @ global_step 141350, g_loss 0.511365, d_loss 0.458086 accuracy 0.890625\n",
      "Training in progress @ global_step 141400, g_loss 0.509069, d_loss 0.459298 accuracy 0.859375\n",
      "Training in progress @ global_step 141450, g_loss 0.51026, d_loss 0.455132 accuracy 0.90625\n",
      "Training in progress @ global_step 141500, g_loss 0.514843, d_loss 0.465321 accuracy 0.828125\n",
      "Training in progress @ global_step 141550, g_loss 0.511264, d_loss 0.461995 accuracy 0.828125\n",
      "Training in progress @ global_step 141600, g_loss 0.515396, d_loss 0.459505 accuracy 0.8125\n",
      "Training in progress @ global_step 141650, g_loss 0.511629, d_loss 0.456206 accuracy 0.890625\n",
      "Training in progress @ global_step 141700, g_loss 0.51129, d_loss 0.457625 accuracy 0.90625\n",
      "Training in progress @ global_step 141750, g_loss 0.511177, d_loss 0.45925 accuracy 0.859375\n",
      "Training in progress @ global_step 141800, g_loss 0.513819, d_loss 0.457302 accuracy 0.859375\n",
      "Training in progress @ global_step 141850, g_loss 0.51596, d_loss 0.456595 accuracy 0.875\n",
      "Training in progress @ global_step 141900, g_loss 0.509303, d_loss 0.459201 accuracy 0.859375\n",
      "Training in progress @ global_step 141950, g_loss 0.517353, d_loss 0.463637 accuracy 0.796875\n",
      "Training in progress @ global_step 142000, g_loss 0.511998, d_loss 0.455368 accuracy 0.859375\n",
      "Training in progress @ global_step 142050, g_loss 0.506381, d_loss 0.458403 accuracy 0.859375\n",
      "Training in progress @ global_step 142100, g_loss 0.507642, d_loss 0.457924 accuracy 0.8125\n",
      "Training in progress @ global_step 142150, g_loss 0.512037, d_loss 0.452147 accuracy 0.875\n",
      "Training in progress @ global_step 142200, g_loss 0.51479, d_loss 0.456642 accuracy 0.859375\n",
      "Training in progress @ global_step 142250, g_loss 0.511656, d_loss 0.453612 accuracy 0.921875\n",
      "Training in progress @ global_step 142300, g_loss 0.511921, d_loss 0.461176 accuracy 0.890625\n",
      "Training in progress @ global_step 142350, g_loss 0.512626, d_loss 0.449911 accuracy 0.921875\n",
      "Training in progress @ global_step 142400, g_loss 0.516631, d_loss 0.460674 accuracy 0.875\n",
      "Training in progress @ global_step 142450, g_loss 0.508804, d_loss 0.456787 accuracy 0.875\n",
      "Training in progress @ global_step 142500, g_loss 0.5067, d_loss 0.451406 accuracy 0.921875\n",
      "Training in progress @ global_step 142550, g_loss 0.513083, d_loss 0.460811 accuracy 0.875\n",
      "Training in progress @ global_step 142600, g_loss 0.516432, d_loss 0.455026 accuracy 0.90625\n",
      "Training in progress @ global_step 142650, g_loss 0.513098, d_loss 0.45477 accuracy 0.921875\n",
      "Training in progress @ global_step 142700, g_loss 0.506577, d_loss 0.459188 accuracy 0.890625\n",
      "Training in progress @ global_step 142750, g_loss 0.511034, d_loss 0.454199 accuracy 0.859375\n",
      "Training in progress @ global_step 142800, g_loss 0.506982, d_loss 0.449333 accuracy 0.953125\n",
      "Training in progress @ global_step 142850, g_loss 0.512336, d_loss 0.461632 accuracy 0.84375\n",
      "Training in progress @ global_step 142900, g_loss 0.51263, d_loss 0.456359 accuracy 0.921875\n",
      "Training in progress @ global_step 142950, g_loss 0.509587, d_loss 0.453593 accuracy 0.9375\n",
      "Training in progress @ global_step 143000, g_loss 0.507592, d_loss 0.452408 accuracy 0.96875\n",
      "Training in progress @ global_step 143050, g_loss 0.508129, d_loss 0.45315 accuracy 0.90625\n",
      "Training in progress @ global_step 143100, g_loss 0.505067, d_loss 0.456202 accuracy 0.875\n",
      "Training in progress @ global_step 143150, g_loss 0.514888, d_loss 0.456636 accuracy 0.875\n",
      "Training in progress @ global_step 143200, g_loss 0.508805, d_loss 0.458161 accuracy 0.875\n",
      "Training in progress @ global_step 143250, g_loss 0.508082, d_loss 0.460004 accuracy 0.828125\n",
      "Training in progress @ global_step 143300, g_loss 0.508591, d_loss 0.459338 accuracy 0.890625\n",
      "Training in progress @ global_step 143350, g_loss 0.511728, d_loss 0.459474 accuracy 0.875\n",
      "Training in progress @ global_step 143400, g_loss 0.518589, d_loss 0.45867 accuracy 0.90625\n",
      "Training in progress @ global_step 143450, g_loss 0.510809, d_loss 0.457795 accuracy 0.875\n",
      "Training in progress @ global_step 143500, g_loss 0.508014, d_loss 0.449324 accuracy 0.890625\n",
      "Training in progress @ global_step 143550, g_loss 0.514728, d_loss 0.455123 accuracy 0.921875\n",
      "Training in progress @ global_step 143600, g_loss 0.511987, d_loss 0.457011 accuracy 0.859375\n",
      "Training in progress @ global_step 143650, g_loss 0.509125, d_loss 0.454627 accuracy 0.875\n",
      "Training in progress @ global_step 143700, g_loss 0.512314, d_loss 0.453868 accuracy 0.921875\n",
      "Training in progress @ global_step 143750, g_loss 0.509246, d_loss 0.452644 accuracy 0.859375\n",
      "Training in progress @ global_step 143800, g_loss 0.508358, d_loss 0.452537 accuracy 0.90625\n",
      "Training in progress @ global_step 143850, g_loss 0.510072, d_loss 0.458765 accuracy 0.859375\n",
      "Training in progress @ global_step 143900, g_loss 0.51231, d_loss 0.450209 accuracy 0.90625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 143950, g_loss 0.511067, d_loss 0.457595 accuracy 0.90625\n",
      "Training in progress @ global_step 144000, g_loss 0.512273, d_loss 0.455788 accuracy 0.890625\n",
      "Training in progress @ global_step 144050, g_loss 0.509852, d_loss 0.455736 accuracy 0.953125\n",
      "Training in progress @ global_step 144100, g_loss 0.510741, d_loss 0.458333 accuracy 0.9375\n",
      "Training in progress @ global_step 144150, g_loss 0.511673, d_loss 0.452488 accuracy 0.9375\n",
      "Training in progress @ global_step 144200, g_loss 0.516797, d_loss 0.448355 accuracy 0.96875\n",
      "Training in progress @ global_step 144250, g_loss 0.510761, d_loss 0.451663 accuracy 0.9375\n",
      "Training in progress @ global_step 144300, g_loss 0.514883, d_loss 0.454868 accuracy 0.890625\n",
      "Training in progress @ global_step 144350, g_loss 0.509662, d_loss 0.453382 accuracy 0.890625\n",
      "Training in progress @ global_step 144400, g_loss 0.513954, d_loss 0.458809 accuracy 0.921875\n",
      "Training in progress @ global_step 144450, g_loss 0.509425, d_loss 0.457 accuracy 0.875\n",
      "Training in progress @ global_step 144500, g_loss 0.515092, d_loss 0.46103 accuracy 0.859375\n",
      "Training in progress @ global_step 144550, g_loss 0.511821, d_loss 0.461311 accuracy 0.84375\n",
      "Training in progress @ global_step 144600, g_loss 0.516812, d_loss 0.458239 accuracy 0.890625\n",
      "Training in progress @ global_step 144650, g_loss 0.515548, d_loss 0.455953 accuracy 0.90625\n",
      "Training in progress @ global_step 144700, g_loss 0.512147, d_loss 0.455562 accuracy 0.875\n",
      "Training in progress @ global_step 144750, g_loss 0.510815, d_loss 0.453569 accuracy 0.953125\n",
      "Training in progress @ global_step 144800, g_loss 0.517129, d_loss 0.453398 accuracy 0.953125\n",
      "Training in progress @ global_step 144850, g_loss 0.512347, d_loss 0.456965 accuracy 0.875\n",
      "Training in progress @ global_step 144900, g_loss 0.515658, d_loss 0.457657 accuracy 0.90625\n",
      "Training in progress @ global_step 144950, g_loss 0.518543, d_loss 0.457032 accuracy 0.90625\n",
      "Training in progress @ global_step 145000, g_loss 0.511704, d_loss 0.451902 accuracy 0.96875\n",
      "Training in progress @ global_step 145050, g_loss 0.512697, d_loss 0.457176 accuracy 0.90625\n",
      "Training in progress @ global_step 145100, g_loss 0.511774, d_loss 0.454381 accuracy 0.890625\n",
      "Training in progress @ global_step 145150, g_loss 0.515073, d_loss 0.457649 accuracy 0.890625\n",
      "Training in progress @ global_step 145200, g_loss 0.516137, d_loss 0.456347 accuracy 0.921875\n",
      "Training in progress @ global_step 145250, g_loss 0.515037, d_loss 0.454042 accuracy 0.875\n",
      "Training in progress @ global_step 145300, g_loss 0.516118, d_loss 0.455495 accuracy 0.921875\n",
      "Training in progress @ global_step 145350, g_loss 0.514525, d_loss 0.451989 accuracy 0.90625\n",
      "Training in progress @ global_step 145400, g_loss 0.519688, d_loss 0.462178 accuracy 0.828125\n",
      "Training in progress @ global_step 145450, g_loss 0.510834, d_loss 0.458723 accuracy 0.84375\n",
      "Training in progress @ global_step 145500, g_loss 0.513702, d_loss 0.465786 accuracy 0.890625\n",
      "Training in progress @ global_step 145550, g_loss 0.517773, d_loss 0.456857 accuracy 0.96875\n",
      "Training in progress @ global_step 145600, g_loss 0.518186, d_loss 0.455938 accuracy 0.953125\n",
      "Training in progress @ global_step 145650, g_loss 0.516559, d_loss 0.458872 accuracy 0.875\n",
      "Training in progress @ global_step 145700, g_loss 0.523355, d_loss 0.453162 accuracy 0.890625\n",
      "Training in progress @ global_step 145750, g_loss 0.519144, d_loss 0.452798 accuracy 0.953125\n",
      "Training in progress @ global_step 145800, g_loss 0.518098, d_loss 0.451594 accuracy 0.875\n",
      "Training in progress @ global_step 145850, g_loss 0.514146, d_loss 0.461978 accuracy 0.828125\n",
      "Training in progress @ global_step 145900, g_loss 0.517456, d_loss 0.461608 accuracy 0.90625\n",
      "Training in progress @ global_step 145950, g_loss 0.516603, d_loss 0.46142 accuracy 0.859375\n",
      "Training in progress @ global_step 146000, g_loss 0.517061, d_loss 0.462912 accuracy 0.890625\n",
      "Training in progress @ global_step 146050, g_loss 0.513481, d_loss 0.462672 accuracy 0.828125\n",
      "Training in progress @ global_step 146100, g_loss 0.516939, d_loss 0.459607 accuracy 0.890625\n",
      "Training in progress @ global_step 146150, g_loss 0.512334, d_loss 0.454457 accuracy 0.921875\n",
      "Training in progress @ global_step 146200, g_loss 0.516239, d_loss 0.455837 accuracy 0.890625\n",
      "Training in progress @ global_step 146250, g_loss 0.515287, d_loss 0.462647 accuracy 0.859375\n",
      "Training in progress @ global_step 146300, g_loss 0.516967, d_loss 0.459687 accuracy 0.90625\n",
      "Training in progress @ global_step 146350, g_loss 0.520191, d_loss 0.465076 accuracy 0.828125\n",
      "Training in progress @ global_step 146400, g_loss 0.515355, d_loss 0.45757 accuracy 0.90625\n",
      "Training in progress @ global_step 146450, g_loss 0.513727, d_loss 0.461946 accuracy 0.890625\n",
      "Training in progress @ global_step 146500, g_loss 0.516338, d_loss 0.460526 accuracy 0.84375\n",
      "Training in progress @ global_step 146550, g_loss 0.517568, d_loss 0.45691 accuracy 0.9375\n",
      "Training in progress @ global_step 146600, g_loss 0.515308, d_loss 0.467281 accuracy 0.8125\n",
      "Training in progress @ global_step 146650, g_loss 0.518979, d_loss 0.468027 accuracy 0.78125\n",
      "Training in progress @ global_step 146700, g_loss 0.522376, d_loss 0.4612 accuracy 0.84375\n",
      "Training in progress @ global_step 146750, g_loss 0.519846, d_loss 0.461664 accuracy 0.8125\n",
      "Training in progress @ global_step 146800, g_loss 0.520559, d_loss 0.465187 accuracy 0.84375\n",
      "Training in progress @ global_step 146850, g_loss 0.523947, d_loss 0.462915 accuracy 0.84375\n",
      "Training in progress @ global_step 146900, g_loss 0.52498, d_loss 0.466303 accuracy 0.8125\n",
      "Training in progress @ global_step 146950, g_loss 0.520491, d_loss 0.463443 accuracy 0.828125\n",
      "Training in progress @ global_step 147000, g_loss 0.519421, d_loss 0.466779 accuracy 0.8125\n",
      "Training in progress @ global_step 147050, g_loss 0.522015, d_loss 0.461726 accuracy 0.859375\n",
      "Training in progress @ global_step 147100, g_loss 0.519771, d_loss 0.468463 accuracy 0.8125\n",
      "Training in progress @ global_step 147150, g_loss 0.524485, d_loss 0.469959 accuracy 0.796875\n",
      "Training in progress @ global_step 147200, g_loss 0.531633, d_loss 0.475342 accuracy 0.703125\n",
      "Training in progress @ global_step 147250, g_loss 0.524684, d_loss 0.474726 accuracy 0.6875\n",
      "Training in progress @ global_step 147300, g_loss 0.526531, d_loss 0.470635 accuracy 0.765625\n",
      "Training in progress @ global_step 147350, g_loss 0.520783, d_loss 0.466193 accuracy 0.828125\n",
      "Training in progress @ global_step 147400, g_loss 0.519625, d_loss 0.468089 accuracy 0.8125\n",
      "Training in progress @ global_step 147450, g_loss 0.521838, d_loss 0.468735 accuracy 0.765625\n",
      "Training in progress @ global_step 147500, g_loss 0.520208, d_loss 0.467192 accuracy 0.765625\n",
      "Training in progress @ global_step 147550, g_loss 0.522718, d_loss 0.471089 accuracy 0.75\n",
      "Training in progress @ global_step 147600, g_loss 0.520072, d_loss 0.473317 accuracy 0.75\n",
      "Training in progress @ global_step 147650, g_loss 0.52691, d_loss 0.472479 accuracy 0.765625\n",
      "Training in progress @ global_step 147700, g_loss 0.524671, d_loss 0.472733 accuracy 0.734375\n",
      "Training in progress @ global_step 147750, g_loss 0.527293, d_loss 0.476641 accuracy 0.71875\n",
      "Training in progress @ global_step 147800, g_loss 0.528946, d_loss 0.479138 accuracy 0.703125\n",
      "Training in progress @ global_step 147850, g_loss 0.528498, d_loss 0.473658 accuracy 0.6875\n",
      "Training in progress @ global_step 147900, g_loss 0.52631, d_loss 0.473324 accuracy 0.75\n",
      "Training in progress @ global_step 147950, g_loss 0.530929, d_loss 0.475263 accuracy 0.703125\n",
      "Training in progress @ global_step 148000, g_loss 0.526915, d_loss 0.473774 accuracy 0.71875\n",
      "Training in progress @ global_step 148050, g_loss 0.521375, d_loss 0.471897 accuracy 0.78125\n",
      "Training in progress @ global_step 148100, g_loss 0.529892, d_loss 0.463766 accuracy 0.8125\n",
      "Training in progress @ global_step 148150, g_loss 0.532137, d_loss 0.472329 accuracy 0.734375\n",
      "Training in progress @ global_step 148200, g_loss 0.528328, d_loss 0.474464 accuracy 0.703125\n",
      "Training in progress @ global_step 148250, g_loss 0.530505, d_loss 0.470627 accuracy 0.75\n",
      "Training in progress @ global_step 148300, g_loss 0.528381, d_loss 0.472436 accuracy 0.78125\n",
      "Training in progress @ global_step 148350, g_loss 0.527387, d_loss 0.480262 accuracy 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 148400, g_loss 0.53022, d_loss 0.476851 accuracy 0.703125\n",
      "Training in progress @ global_step 148450, g_loss 0.52976, d_loss 0.479583 accuracy 0.6875\n",
      "Training in progress @ global_step 148500, g_loss 0.534028, d_loss 0.464875 accuracy 0.796875\n",
      "Training in progress @ global_step 148550, g_loss 0.529691, d_loss 0.473054 accuracy 0.75\n",
      "Training in progress @ global_step 148600, g_loss 0.526728, d_loss 0.466835 accuracy 0.78125\n",
      "Training in progress @ global_step 148650, g_loss 0.529853, d_loss 0.472628 accuracy 0.796875\n",
      "Training in progress @ global_step 148700, g_loss 0.530504, d_loss 0.477015 accuracy 0.703125\n",
      "Training in progress @ global_step 148750, g_loss 0.530895, d_loss 0.481364 accuracy 0.671875\n",
      "Training in progress @ global_step 148800, g_loss 0.531089, d_loss 0.477703 accuracy 0.65625\n",
      "Training in progress @ global_step 148850, g_loss 0.53104, d_loss 0.481454 accuracy 0.65625\n",
      "Training in progress @ global_step 148900, g_loss 0.525213, d_loss 0.475634 accuracy 0.734375\n",
      "Training in progress @ global_step 148950, g_loss 0.526945, d_loss 0.478765 accuracy 0.71875\n",
      "Training in progress @ global_step 149000, g_loss 0.530213, d_loss 0.480639 accuracy 0.671875\n",
      "Training in progress @ global_step 149050, g_loss 0.527577, d_loss 0.473329 accuracy 0.765625\n",
      "Training in progress @ global_step 149100, g_loss 0.532354, d_loss 0.472158 accuracy 0.75\n",
      "Training in progress @ global_step 149150, g_loss 0.53202, d_loss 0.478422 accuracy 0.671875\n",
      "Training in progress @ global_step 149200, g_loss 0.528134, d_loss 0.474024 accuracy 0.8125\n",
      "Training in progress @ global_step 149250, g_loss 0.527445, d_loss 0.483237 accuracy 0.65625\n",
      "Training in progress @ global_step 149300, g_loss 0.531666, d_loss 0.477543 accuracy 0.703125\n",
      "Training in progress @ global_step 149350, g_loss 0.531426, d_loss 0.484409 accuracy 0.671875\n",
      "Training in progress @ global_step 149400, g_loss 0.528023, d_loss 0.475592 accuracy 0.71875\n",
      "Training in progress @ global_step 149450, g_loss 0.532907, d_loss 0.479333 accuracy 0.71875\n",
      "Training in progress @ global_step 149500, g_loss 0.530371, d_loss 0.47808 accuracy 0.6875\n",
      "Training in progress @ global_step 149550, g_loss 0.528425, d_loss 0.479528 accuracy 0.71875\n",
      "Training in progress @ global_step 149600, g_loss 0.532544, d_loss 0.483071 accuracy 0.640625\n",
      "Training in progress @ global_step 149650, g_loss 0.531219, d_loss 0.479821 accuracy 0.703125\n",
      "Training in progress @ global_step 149700, g_loss 0.533259, d_loss 0.477527 accuracy 0.6875\n",
      "Training in progress @ global_step 149750, g_loss 0.531893, d_loss 0.487246 accuracy 0.625\n",
      "Training in progress @ global_step 149800, g_loss 0.535742, d_loss 0.480269 accuracy 0.71875\n",
      "Training in progress @ global_step 149850, g_loss 0.532719, d_loss 0.487251 accuracy 0.640625\n",
      "Training in progress @ global_step 149900, g_loss 0.53691, d_loss 0.474309 accuracy 0.734375\n",
      "Training in progress @ global_step 149950, g_loss 0.53047, d_loss 0.486072 accuracy 0.640625\n",
      "Training in progress @ global_step 150000, g_loss 0.537472, d_loss 0.474642 accuracy 0.734375\n",
      "Training in progress @ global_step 150050, g_loss 0.534288, d_loss 0.481362 accuracy 0.671875\n",
      "Training in progress @ global_step 150100, g_loss 0.539095, d_loss 0.480525 accuracy 0.671875\n",
      "Training in progress @ global_step 150150, g_loss 0.537044, d_loss 0.47189 accuracy 0.75\n",
      "Training in progress @ global_step 150200, g_loss 0.544228, d_loss 0.475332 accuracy 0.765625\n",
      "Training in progress @ global_step 150250, g_loss 0.535938, d_loss 0.475407 accuracy 0.734375\n",
      "Training in progress @ global_step 150300, g_loss 0.533918, d_loss 0.477194 accuracy 0.75\n",
      "Training in progress @ global_step 150350, g_loss 0.534857, d_loss 0.474966 accuracy 0.71875\n",
      "Training in progress @ global_step 150400, g_loss 0.535171, d_loss 0.48431 accuracy 0.671875\n",
      "Training in progress @ global_step 150450, g_loss 0.53281, d_loss 0.480761 accuracy 0.671875\n",
      "Training in progress @ global_step 150500, g_loss 0.536966, d_loss 0.47716 accuracy 0.703125\n",
      "Training in progress @ global_step 150550, g_loss 0.533189, d_loss 0.47592 accuracy 0.671875\n",
      "Training in progress @ global_step 150600, g_loss 0.536564, d_loss 0.474535 accuracy 0.78125\n",
      "Training in progress @ global_step 150650, g_loss 0.537753, d_loss 0.485904 accuracy 0.640625\n",
      "Training in progress @ global_step 150700, g_loss 0.539634, d_loss 0.482992 accuracy 0.671875\n",
      "Training in progress @ global_step 150750, g_loss 0.538058, d_loss 0.484728 accuracy 0.75\n",
      "Training in progress @ global_step 150800, g_loss 0.541198, d_loss 0.481107 accuracy 0.6875\n",
      "Training in progress @ global_step 150850, g_loss 0.532612, d_loss 0.485532 accuracy 0.671875\n",
      "Training in progress @ global_step 150900, g_loss 0.535867, d_loss 0.493152 accuracy 0.625\n",
      "Training in progress @ global_step 150950, g_loss 0.537279, d_loss 0.484884 accuracy 0.703125\n",
      "Training in progress @ global_step 151000, g_loss 0.53917, d_loss 0.494273 accuracy 0.640625\n",
      "Training in progress @ global_step 151050, g_loss 0.538137, d_loss 0.484897 accuracy 0.671875\n",
      "Training in progress @ global_step 151100, g_loss 0.538152, d_loss 0.490495 accuracy 0.640625\n",
      "Training in progress @ global_step 151150, g_loss 0.535554, d_loss 0.494238 accuracy 0.640625\n",
      "Training in progress @ global_step 151200, g_loss 0.538121, d_loss 0.494004 accuracy 0.640625\n",
      "Training in progress @ global_step 151250, g_loss 0.536309, d_loss 0.496421 accuracy 0.625\n",
      "Training in progress @ global_step 151300, g_loss 0.535092, d_loss 0.496598 accuracy 0.609375\n",
      "Training in progress @ global_step 151350, g_loss 0.536137, d_loss 0.495436 accuracy 0.59375\n",
      "Training in progress @ global_step 151400, g_loss 0.538561, d_loss 0.504095 accuracy 0.546875\n",
      "Training in progress @ global_step 151450, g_loss 0.540501, d_loss 0.493061 accuracy 0.625\n",
      "Training in progress @ global_step 151500, g_loss 0.541276, d_loss 0.493743 accuracy 0.578125\n",
      "Training in progress @ global_step 151550, g_loss 0.542106, d_loss 0.50223 accuracy 0.59375\n",
      "Training in progress @ global_step 151600, g_loss 0.541689, d_loss 0.503851 accuracy 0.578125\n",
      "Training in progress @ global_step 151650, g_loss 0.54058, d_loss 0.500786 accuracy 0.5625\n",
      "Training in progress @ global_step 151700, g_loss 0.535472, d_loss 0.500204 accuracy 0.578125\n",
      "Training in progress @ global_step 151750, g_loss 0.536026, d_loss 0.501849 accuracy 0.609375\n",
      "Training in progress @ global_step 151800, g_loss 0.537808, d_loss 0.493926 accuracy 0.59375\n",
      "Training in progress @ global_step 151850, g_loss 0.53937, d_loss 0.50116 accuracy 0.546875\n",
      "Training in progress @ global_step 151900, g_loss 0.538836, d_loss 0.492503 accuracy 0.59375\n",
      "Training in progress @ global_step 151950, g_loss 0.544346, d_loss 0.494563 accuracy 0.609375\n",
      "Training in progress @ global_step 152000, g_loss 0.538205, d_loss 0.493558 accuracy 0.59375\n",
      "Training in progress @ global_step 152050, g_loss 0.53805, d_loss 0.49238 accuracy 0.59375\n",
      "Training in progress @ global_step 152100, g_loss 0.541018, d_loss 0.498577 accuracy 0.5625\n",
      "Training in progress @ global_step 152150, g_loss 0.541433, d_loss 0.494309 accuracy 0.59375\n",
      "Training in progress @ global_step 152200, g_loss 0.540175, d_loss 0.491568 accuracy 0.5625\n",
      "Training in progress @ global_step 152250, g_loss 0.539044, d_loss 0.491429 accuracy 0.625\n",
      "Training in progress @ global_step 152300, g_loss 0.544419, d_loss 0.491336 accuracy 0.5625\n",
      "Training in progress @ global_step 152350, g_loss 0.540955, d_loss 0.500359 accuracy 0.5625\n",
      "Training in progress @ global_step 152400, g_loss 0.543403, d_loss 0.491629 accuracy 0.625\n",
      "Training in progress @ global_step 152450, g_loss 0.541252, d_loss 0.482877 accuracy 0.609375\n",
      "Training in progress @ global_step 152500, g_loss 0.542664, d_loss 0.484251 accuracy 0.640625\n",
      "Training in progress @ global_step 152550, g_loss 0.541875, d_loss 0.481512 accuracy 0.703125\n",
      "Training in progress @ global_step 152600, g_loss 0.542295, d_loss 0.484655 accuracy 0.6875\n",
      "Training in progress @ global_step 152650, g_loss 0.542743, d_loss 0.488605 accuracy 0.609375\n",
      "Training in progress @ global_step 152700, g_loss 0.544211, d_loss 0.486162 accuracy 0.65625\n",
      "Training in progress @ global_step 152750, g_loss 0.541416, d_loss 0.481544 accuracy 0.671875\n",
      "Training in progress @ global_step 152800, g_loss 0.544973, d_loss 0.47148 accuracy 0.734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 152850, g_loss 0.542212, d_loss 0.48385 accuracy 0.65625\n",
      "Training in progress @ global_step 152900, g_loss 0.543135, d_loss 0.487365 accuracy 0.65625\n",
      "Training in progress @ global_step 152950, g_loss 0.545145, d_loss 0.486842 accuracy 0.65625\n",
      "Training in progress @ global_step 153000, g_loss 0.546526, d_loss 0.489681 accuracy 0.609375\n",
      "Training in progress @ global_step 153050, g_loss 0.54633, d_loss 0.479287 accuracy 0.75\n",
      "Training in progress @ global_step 153100, g_loss 0.542066, d_loss 0.479455 accuracy 0.671875\n",
      "Training in progress @ global_step 153150, g_loss 0.543559, d_loss 0.481147 accuracy 0.703125\n",
      "Training in progress @ global_step 153200, g_loss 0.546563, d_loss 0.491881 accuracy 0.640625\n",
      "Training in progress @ global_step 153250, g_loss 0.545631, d_loss 0.475397 accuracy 0.734375\n",
      "Training in progress @ global_step 153300, g_loss 0.539969, d_loss 0.482293 accuracy 0.703125\n",
      "Training in progress @ global_step 153350, g_loss 0.536826, d_loss 0.477542 accuracy 0.75\n",
      "Training in progress @ global_step 153400, g_loss 0.539804, d_loss 0.489427 accuracy 0.65625\n",
      "Training in progress @ global_step 153450, g_loss 0.5439, d_loss 0.498344 accuracy 0.609375\n",
      "Training in progress @ global_step 153500, g_loss 0.54101, d_loss 0.499589 accuracy 0.625\n",
      "Training in progress @ global_step 153550, g_loss 0.540661, d_loss 0.492299 accuracy 0.625\n",
      "Training in progress @ global_step 153600, g_loss 0.535077, d_loss 0.481455 accuracy 0.734375\n",
      "Training in progress @ global_step 153650, g_loss 0.540498, d_loss 0.51032 accuracy 0.5625\n",
      "Training in progress @ global_step 153700, g_loss 0.540971, d_loss 0.504216 accuracy 0.59375\n",
      "Training in progress @ global_step 153750, g_loss 0.53486, d_loss 0.497905 accuracy 0.59375\n",
      "Training in progress @ global_step 153800, g_loss 0.539796, d_loss 0.488423 accuracy 0.6875\n",
      "Training in progress @ global_step 153850, g_loss 0.541619, d_loss 0.497815 accuracy 0.59375\n",
      "Training in progress @ global_step 153900, g_loss 0.541102, d_loss 0.504051 accuracy 0.625\n",
      "Training in progress @ global_step 153950, g_loss 0.539066, d_loss 0.491834 accuracy 0.65625\n",
      "Training in progress @ global_step 154000, g_loss 0.540186, d_loss 0.498901 accuracy 0.625\n",
      "Training in progress @ global_step 154050, g_loss 0.54121, d_loss 0.501828 accuracy 0.5625\n",
      "Training in progress @ global_step 154100, g_loss 0.536283, d_loss 0.506545 accuracy 0.59375\n",
      "Training in progress @ global_step 154150, g_loss 0.535447, d_loss 0.509631 accuracy 0.5625\n",
      "Training in progress @ global_step 154200, g_loss 0.536406, d_loss 0.510556 accuracy 0.53125\n",
      "Training in progress @ global_step 154250, g_loss 0.534265, d_loss 0.506169 accuracy 0.546875\n",
      "Training in progress @ global_step 154300, g_loss 0.533053, d_loss 0.504973 accuracy 0.5625\n",
      "Training in progress @ global_step 154350, g_loss 0.539356, d_loss 0.500798 accuracy 0.5625\n",
      "Training in progress @ global_step 154400, g_loss 0.538428, d_loss 0.493683 accuracy 0.59375\n",
      "Training in progress @ global_step 154450, g_loss 0.538103, d_loss 0.49528 accuracy 0.609375\n",
      "Training in progress @ global_step 154500, g_loss 0.534757, d_loss 0.504896 accuracy 0.53125\n",
      "Training in progress @ global_step 154550, g_loss 0.533788, d_loss 0.501991 accuracy 0.5625\n",
      "Training in progress @ global_step 154600, g_loss 0.539081, d_loss 0.504405 accuracy 0.546875\n",
      "Training in progress @ global_step 154650, g_loss 0.535602, d_loss 0.500108 accuracy 0.578125\n",
      "Training in progress @ global_step 154700, g_loss 0.537014, d_loss 0.510786 accuracy 0.5625\n",
      "Training in progress @ global_step 154750, g_loss 0.536412, d_loss 0.50005 accuracy 0.546875\n",
      "Training in progress @ global_step 154800, g_loss 0.536894, d_loss 0.503014 accuracy 0.578125\n",
      "Training in progress @ global_step 154850, g_loss 0.535046, d_loss 0.493461 accuracy 0.640625\n",
      "Training in progress @ global_step 154900, g_loss 0.535542, d_loss 0.490828 accuracy 0.59375\n",
      "Training in progress @ global_step 154950, g_loss 0.535918, d_loss 0.490087 accuracy 0.59375\n",
      "Training in progress @ global_step 155000, g_loss 0.537384, d_loss 0.491894 accuracy 0.625\n",
      "Training in progress @ global_step 155050, g_loss 0.536952, d_loss 0.48816 accuracy 0.65625\n",
      "Training in progress @ global_step 155100, g_loss 0.535889, d_loss 0.482842 accuracy 0.671875\n",
      "Training in progress @ global_step 155150, g_loss 0.538944, d_loss 0.489874 accuracy 0.625\n",
      "Training in progress @ global_step 155200, g_loss 0.535558, d_loss 0.483803 accuracy 0.640625\n",
      "Training in progress @ global_step 155250, g_loss 0.538111, d_loss 0.483775 accuracy 0.625\n",
      "Training in progress @ global_step 155300, g_loss 0.541297, d_loss 0.48802 accuracy 0.625\n",
      "Training in progress @ global_step 155350, g_loss 0.542758, d_loss 0.483077 accuracy 0.6875\n",
      "Training in progress @ global_step 155400, g_loss 0.544927, d_loss 0.489158 accuracy 0.65625\n",
      "Training in progress @ global_step 155450, g_loss 0.544469, d_loss 0.485367 accuracy 0.671875\n",
      "Training in progress @ global_step 155500, g_loss 0.543815, d_loss 0.483157 accuracy 0.640625\n",
      "Training in progress @ global_step 155550, g_loss 0.543963, d_loss 0.47908 accuracy 0.71875\n",
      "Training in progress @ global_step 155600, g_loss 0.543612, d_loss 0.485449 accuracy 0.6875\n",
      "Training in progress @ global_step 155650, g_loss 0.544417, d_loss 0.485871 accuracy 0.671875\n",
      "Training in progress @ global_step 155700, g_loss 0.545594, d_loss 0.480778 accuracy 0.71875\n",
      "Training in progress @ global_step 155750, g_loss 0.549121, d_loss 0.476273 accuracy 0.703125\n",
      "Training in progress @ global_step 155800, g_loss 0.547621, d_loss 0.487546 accuracy 0.671875\n",
      "Training in progress @ global_step 155850, g_loss 0.54567, d_loss 0.490086 accuracy 0.640625\n",
      "Training in progress @ global_step 155900, g_loss 0.552492, d_loss 0.478095 accuracy 0.703125\n",
      "Training in progress @ global_step 155950, g_loss 0.551439, d_loss 0.488023 accuracy 0.71875\n",
      "Training in progress @ global_step 156000, g_loss 0.549101, d_loss 0.498763 accuracy 0.609375\n",
      "Training in progress @ global_step 156050, g_loss 0.551401, d_loss 0.493576 accuracy 0.609375\n",
      "Training in progress @ global_step 156100, g_loss 0.550688, d_loss 0.494739 accuracy 0.640625\n",
      "Training in progress @ global_step 156150, g_loss 0.551536, d_loss 0.482695 accuracy 0.71875\n",
      "Training in progress @ global_step 156200, g_loss 0.554174, d_loss 0.492074 accuracy 0.640625\n",
      "Training in progress @ global_step 156250, g_loss 0.547564, d_loss 0.482972 accuracy 0.671875\n",
      "Training in progress @ global_step 156300, g_loss 0.549702, d_loss 0.481685 accuracy 0.671875\n",
      "Training in progress @ global_step 156350, g_loss 0.553902, d_loss 0.481935 accuracy 0.734375\n",
      "Training in progress @ global_step 156400, g_loss 0.550592, d_loss 0.492338 accuracy 0.65625\n",
      "Training in progress @ global_step 156450, g_loss 0.545605, d_loss 0.509123 accuracy 0.5625\n",
      "Training in progress @ global_step 156500, g_loss 0.543471, d_loss 0.494903 accuracy 0.640625\n",
      "Training in progress @ global_step 156550, g_loss 0.548109, d_loss 0.499073 accuracy 0.640625\n",
      "Training in progress @ global_step 156600, g_loss 0.547665, d_loss 0.485271 accuracy 0.671875\n",
      "Training in progress @ global_step 156650, g_loss 0.547827, d_loss 0.491487 accuracy 0.6875\n",
      "Training in progress @ global_step 156700, g_loss 0.554946, d_loss 0.500242 accuracy 0.640625\n",
      "Training in progress @ global_step 156750, g_loss 0.552596, d_loss 0.522259 accuracy 0.5625\n",
      "Training in progress @ global_step 156800, g_loss 0.550112, d_loss 0.496748 accuracy 0.625\n",
      "Training in progress @ global_step 156850, g_loss 0.553556, d_loss 0.514683 accuracy 0.5625\n",
      "Training in progress @ global_step 156900, g_loss 0.55548, d_loss 0.510022 accuracy 0.59375\n",
      "Training in progress @ global_step 156950, g_loss 0.546369, d_loss 0.509603 accuracy 0.578125\n",
      "Training in progress @ global_step 157000, g_loss 0.549318, d_loss 0.508633 accuracy 0.609375\n",
      "Training in progress @ global_step 157050, g_loss 0.550994, d_loss 0.517253 accuracy 0.5625\n",
      "Training in progress @ global_step 157100, g_loss 0.552302, d_loss 0.513525 accuracy 0.625\n",
      "Training in progress @ global_step 157150, g_loss 0.550847, d_loss 0.525387 accuracy 0.53125\n",
      "Training in progress @ global_step 157200, g_loss 0.55377, d_loss 0.521827 accuracy 0.59375\n",
      "Training in progress @ global_step 157250, g_loss 0.55021, d_loss 0.528815 accuracy 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 157300, g_loss 0.551885, d_loss 0.537085 accuracy 0.53125\n",
      "Training in progress @ global_step 157350, g_loss 0.551687, d_loss 0.523285 accuracy 0.5625\n",
      "Training in progress @ global_step 157400, g_loss 0.551076, d_loss 0.527802 accuracy 0.5625\n",
      "Training in progress @ global_step 157450, g_loss 0.554747, d_loss 0.531748 accuracy 0.53125\n",
      "Training in progress @ global_step 157500, g_loss 0.555415, d_loss 0.523207 accuracy 0.546875\n",
      "Training in progress @ global_step 157550, g_loss 0.554785, d_loss 0.528602 accuracy 0.515625\n",
      "Training in progress @ global_step 157600, g_loss 0.551816, d_loss 0.534529 accuracy 0.53125\n",
      "Training in progress @ global_step 157650, g_loss 0.550648, d_loss 0.527296 accuracy 0.5625\n",
      "Training in progress @ global_step 157700, g_loss 0.554249, d_loss 0.530537 accuracy 0.53125\n",
      "Training in progress @ global_step 157750, g_loss 0.554597, d_loss 0.524133 accuracy 0.546875\n",
      "Training in progress @ global_step 157800, g_loss 0.549415, d_loss 0.52945 accuracy 0.53125\n",
      "Training in progress @ global_step 157850, g_loss 0.550295, d_loss 0.525019 accuracy 0.53125\n",
      "Training in progress @ global_step 157900, g_loss 0.548258, d_loss 0.519152 accuracy 0.53125\n",
      "Training in progress @ global_step 157950, g_loss 0.550294, d_loss 0.522693 accuracy 0.5625\n",
      "Training in progress @ global_step 158000, g_loss 0.547385, d_loss 0.52035 accuracy 0.5625\n",
      "Training in progress @ global_step 158050, g_loss 0.552778, d_loss 0.516242 accuracy 0.546875\n",
      "Training in progress @ global_step 158100, g_loss 0.553178, d_loss 0.51576 accuracy 0.53125\n",
      "Training in progress @ global_step 158150, g_loss 0.549665, d_loss 0.519003 accuracy 0.5\n",
      "Training in progress @ global_step 158200, g_loss 0.549467, d_loss 0.505852 accuracy 0.546875\n",
      "Training in progress @ global_step 158250, g_loss 0.550124, d_loss 0.505551 accuracy 0.59375\n",
      "Training in progress @ global_step 158300, g_loss 0.549016, d_loss 0.509841 accuracy 0.5625\n",
      "Training in progress @ global_step 158350, g_loss 0.55099, d_loss 0.505736 accuracy 0.578125\n",
      "Training in progress @ global_step 158400, g_loss 0.550169, d_loss 0.511412 accuracy 0.5625\n",
      "Training in progress @ global_step 158450, g_loss 0.548455, d_loss 0.494173 accuracy 0.625\n",
      "Training in progress @ global_step 158500, g_loss 0.550793, d_loss 0.506352 accuracy 0.5625\n",
      "Training in progress @ global_step 158550, g_loss 0.547053, d_loss 0.492724 accuracy 0.578125\n",
      "Training in progress @ global_step 158600, g_loss 0.55179, d_loss 0.518025 accuracy 0.5\n",
      "Training in progress @ global_step 158650, g_loss 0.550056, d_loss 0.494052 accuracy 0.625\n",
      "Training in progress @ global_step 158700, g_loss 0.546242, d_loss 0.495449 accuracy 0.578125\n",
      "Training in progress @ global_step 158750, g_loss 0.55132, d_loss 0.490367 accuracy 0.640625\n",
      "Training in progress @ global_step 158800, g_loss 0.547581, d_loss 0.494436 accuracy 0.59375\n",
      "Training in progress @ global_step 158850, g_loss 0.547599, d_loss 0.490498 accuracy 0.609375\n",
      "Training in progress @ global_step 158900, g_loss 0.549904, d_loss 0.483315 accuracy 0.671875\n",
      "Training in progress @ global_step 158950, g_loss 0.553056, d_loss 0.489393 accuracy 0.625\n",
      "Training in progress @ global_step 159000, g_loss 0.54978, d_loss 0.482275 accuracy 0.671875\n",
      "Training in progress @ global_step 159050, g_loss 0.552111, d_loss 0.488905 accuracy 0.640625\n",
      "Training in progress @ global_step 159100, g_loss 0.552754, d_loss 0.486248 accuracy 0.671875\n",
      "Training in progress @ global_step 159150, g_loss 0.551177, d_loss 0.490708 accuracy 0.671875\n",
      "Training in progress @ global_step 159200, g_loss 0.554738, d_loss 0.482009 accuracy 0.6875\n",
      "Training in progress @ global_step 159250, g_loss 0.553046, d_loss 0.487095 accuracy 0.671875\n",
      "Training in progress @ global_step 159300, g_loss 0.554048, d_loss 0.49352 accuracy 0.609375\n",
      "Training in progress @ global_step 159350, g_loss 0.553483, d_loss 0.478797 accuracy 0.71875\n",
      "Training in progress @ global_step 159400, g_loss 0.5521, d_loss 0.494804 accuracy 0.640625\n",
      "Training in progress @ global_step 159450, g_loss 0.550137, d_loss 0.500191 accuracy 0.609375\n",
      "Training in progress @ global_step 159500, g_loss 0.552863, d_loss 0.478087 accuracy 0.71875\n",
      "Training in progress @ global_step 159550, g_loss 0.548969, d_loss 0.476477 accuracy 0.6875\n",
      "Training in progress @ global_step 159600, g_loss 0.549624, d_loss 0.477333 accuracy 0.734375\n",
      "Training in progress @ global_step 159650, g_loss 0.554803, d_loss 0.488417 accuracy 0.6875\n",
      "Training in progress @ global_step 159700, g_loss 0.554817, d_loss 0.485399 accuracy 0.6875\n",
      "Training in progress @ global_step 159750, g_loss 0.556044, d_loss 0.479878 accuracy 0.6875\n",
      "Training in progress @ global_step 159800, g_loss 0.549223, d_loss 0.501553 accuracy 0.640625\n",
      "Training in progress @ global_step 159850, g_loss 0.555998, d_loss 0.477419 accuracy 0.703125\n",
      "Training in progress @ global_step 159900, g_loss 0.554781, d_loss 0.49216 accuracy 0.65625\n",
      "Training in progress @ global_step 159950, g_loss 0.554294, d_loss 0.494903 accuracy 0.59375\n",
      "Training in progress @ global_step 160000, g_loss 0.557749, d_loss 0.470735 accuracy 0.78125\n",
      "Training in progress @ global_step 160050, g_loss 0.552176, d_loss 0.48534 accuracy 0.703125\n",
      "Training in progress @ global_step 160100, g_loss 0.554527, d_loss 0.497934 accuracy 0.65625\n",
      "Training in progress @ global_step 160150, g_loss 0.551738, d_loss 0.498693 accuracy 0.625\n",
      "Training in progress @ global_step 160200, g_loss 0.548533, d_loss 0.490824 accuracy 0.6875\n",
      "Training in progress @ global_step 160250, g_loss 0.547388, d_loss 0.487629 accuracy 0.671875\n",
      "Training in progress @ global_step 160300, g_loss 0.548665, d_loss 0.493943 accuracy 0.65625\n",
      "Training in progress @ global_step 160350, g_loss 0.552635, d_loss 0.494014 accuracy 0.703125\n",
      "Training in progress @ global_step 160400, g_loss 0.543832, d_loss 0.502106 accuracy 0.640625\n",
      "Training in progress @ global_step 160450, g_loss 0.54704, d_loss 0.494205 accuracy 0.671875\n",
      "Training in progress @ global_step 160500, g_loss 0.55066, d_loss 0.498253 accuracy 0.640625\n",
      "Training in progress @ global_step 160550, g_loss 0.548491, d_loss 0.51636 accuracy 0.609375\n",
      "Training in progress @ global_step 160600, g_loss 0.549403, d_loss 0.490404 accuracy 0.703125\n",
      "Training in progress @ global_step 160650, g_loss 0.549018, d_loss 0.497269 accuracy 0.671875\n",
      "Training in progress @ global_step 160700, g_loss 0.546371, d_loss 0.517134 accuracy 0.609375\n",
      "Training in progress @ global_step 160750, g_loss 0.544904, d_loss 0.499366 accuracy 0.640625\n",
      "Training in progress @ global_step 160800, g_loss 0.546654, d_loss 0.511502 accuracy 0.578125\n",
      "Training in progress @ global_step 160850, g_loss 0.542183, d_loss 0.505637 accuracy 0.59375\n",
      "Training in progress @ global_step 160900, g_loss 0.544533, d_loss 0.506323 accuracy 0.578125\n",
      "Training in progress @ global_step 160950, g_loss 0.545506, d_loss 0.515346 accuracy 0.578125\n",
      "Training in progress @ global_step 161000, g_loss 0.546626, d_loss 0.518923 accuracy 0.546875\n",
      "Training in progress @ global_step 161050, g_loss 0.544132, d_loss 0.509226 accuracy 0.59375\n",
      "Training in progress @ global_step 161100, g_loss 0.541356, d_loss 0.506877 accuracy 0.5625\n",
      "Training in progress @ global_step 161150, g_loss 0.544423, d_loss 0.486111 accuracy 0.734375\n",
      "Training in progress @ global_step 161200, g_loss 0.547001, d_loss 0.506132 accuracy 0.59375\n",
      "Training in progress @ global_step 161250, g_loss 0.542076, d_loss 0.512482 accuracy 0.59375\n",
      "Training in progress @ global_step 161300, g_loss 0.539456, d_loss 0.506893 accuracy 0.59375\n",
      "Training in progress @ global_step 161350, g_loss 0.544305, d_loss 0.50335 accuracy 0.65625\n",
      "Training in progress @ global_step 161400, g_loss 0.544706, d_loss 0.50745 accuracy 0.5625\n",
      "Training in progress @ global_step 161450, g_loss 0.539443, d_loss 0.510593 accuracy 0.546875\n",
      "Training in progress @ global_step 161500, g_loss 0.546949, d_loss 0.515536 accuracy 0.578125\n",
      "Training in progress @ global_step 161550, g_loss 0.538773, d_loss 0.505022 accuracy 0.546875\n",
      "Training in progress @ global_step 161600, g_loss 0.546197, d_loss 0.504929 accuracy 0.59375\n",
      "Training in progress @ global_step 161650, g_loss 0.543505, d_loss 0.503477 accuracy 0.578125\n",
      "Training in progress @ global_step 161700, g_loss 0.538556, d_loss 0.503955 accuracy 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 161750, g_loss 0.542482, d_loss 0.509437 accuracy 0.5625\n",
      "Training in progress @ global_step 161800, g_loss 0.543047, d_loss 0.507761 accuracy 0.53125\n",
      "Training in progress @ global_step 161850, g_loss 0.540521, d_loss 0.518308 accuracy 0.546875\n",
      "Training in progress @ global_step 161900, g_loss 0.540028, d_loss 0.500861 accuracy 0.578125\n",
      "Training in progress @ global_step 161950, g_loss 0.542458, d_loss 0.504358 accuracy 0.625\n",
      "Training in progress @ global_step 162000, g_loss 0.537731, d_loss 0.497716 accuracy 0.609375\n",
      "Training in progress @ global_step 162050, g_loss 0.541205, d_loss 0.495268 accuracy 0.65625\n",
      "Training in progress @ global_step 162100, g_loss 0.540747, d_loss 0.4975 accuracy 0.609375\n",
      "Training in progress @ global_step 162150, g_loss 0.540914, d_loss 0.499726 accuracy 0.609375\n",
      "Training in progress @ global_step 162200, g_loss 0.540396, d_loss 0.497351 accuracy 0.640625\n",
      "Training in progress @ global_step 162250, g_loss 0.539199, d_loss 0.495523 accuracy 0.609375\n",
      "Training in progress @ global_step 162300, g_loss 0.54415, d_loss 0.493815 accuracy 0.609375\n",
      "Training in progress @ global_step 162350, g_loss 0.543011, d_loss 0.491408 accuracy 0.609375\n",
      "Training in progress @ global_step 162400, g_loss 0.543552, d_loss 0.501281 accuracy 0.609375\n",
      "Training in progress @ global_step 162450, g_loss 0.544103, d_loss 0.487174 accuracy 0.65625\n",
      "Training in progress @ global_step 162500, g_loss 0.541802, d_loss 0.490349 accuracy 0.625\n",
      "Training in progress @ global_step 162550, g_loss 0.546028, d_loss 0.495571 accuracy 0.609375\n",
      "Training in progress @ global_step 162600, g_loss 0.54692, d_loss 0.489759 accuracy 0.625\n",
      "Training in progress @ global_step 162650, g_loss 0.549631, d_loss 0.492935 accuracy 0.65625\n",
      "Training in progress @ global_step 162700, g_loss 0.548034, d_loss 0.490879 accuracy 0.640625\n",
      "Training in progress @ global_step 162750, g_loss 0.550092, d_loss 0.482244 accuracy 0.671875\n",
      "Training in progress @ global_step 162800, g_loss 0.549164, d_loss 0.492006 accuracy 0.640625\n",
      "Training in progress @ global_step 162850, g_loss 0.55133, d_loss 0.481093 accuracy 0.671875\n",
      "Training in progress @ global_step 162900, g_loss 0.549955, d_loss 0.485779 accuracy 0.65625\n",
      "Training in progress @ global_step 162950, g_loss 0.554669, d_loss 0.484258 accuracy 0.703125\n",
      "Training in progress @ global_step 163000, g_loss 0.550287, d_loss 0.494666 accuracy 0.65625\n",
      "Training in progress @ global_step 163050, g_loss 0.555014, d_loss 0.478238 accuracy 0.71875\n",
      "Training in progress @ global_step 163100, g_loss 0.554112, d_loss 0.490109 accuracy 0.640625\n",
      "Training in progress @ global_step 163150, g_loss 0.556786, d_loss 0.481275 accuracy 0.734375\n",
      "Training in progress @ global_step 163200, g_loss 0.560486, d_loss 0.480803 accuracy 0.71875\n",
      "Training in progress @ global_step 163250, g_loss 0.558562, d_loss 0.485874 accuracy 0.640625\n",
      "Training in progress @ global_step 163300, g_loss 0.556922, d_loss 0.479141 accuracy 0.71875\n",
      "Training in progress @ global_step 163350, g_loss 0.558217, d_loss 0.486092 accuracy 0.6875\n",
      "Training in progress @ global_step 163400, g_loss 0.555648, d_loss 0.477382 accuracy 0.703125\n",
      "Training in progress @ global_step 163450, g_loss 0.558877, d_loss 0.478392 accuracy 0.71875\n",
      "Training in progress @ global_step 163500, g_loss 0.55749, d_loss 0.497814 accuracy 0.640625\n",
      "Training in progress @ global_step 163550, g_loss 0.56088, d_loss 0.48396 accuracy 0.71875\n",
      "Training in progress @ global_step 163600, g_loss 0.557495, d_loss 0.481281 accuracy 0.71875\n",
      "Training in progress @ global_step 163650, g_loss 0.560897, d_loss 0.484184 accuracy 0.65625\n",
      "Training in progress @ global_step 163700, g_loss 0.559158, d_loss 0.490172 accuracy 0.671875\n",
      "Training in progress @ global_step 163750, g_loss 0.556887, d_loss 0.475484 accuracy 0.734375\n",
      "Training in progress @ global_step 163800, g_loss 0.559298, d_loss 0.507191 accuracy 0.609375\n",
      "Training in progress @ global_step 163850, g_loss 0.562614, d_loss 0.491891 accuracy 0.640625\n",
      "Training in progress @ global_step 163900, g_loss 0.557472, d_loss 0.496472 accuracy 0.640625\n",
      "Training in progress @ global_step 163950, g_loss 0.556689, d_loss 0.500781 accuracy 0.6875\n",
      "Training in progress @ global_step 164000, g_loss 0.561766, d_loss 0.49669 accuracy 0.640625\n",
      "Training in progress @ global_step 164050, g_loss 0.566266, d_loss 0.500149 accuracy 0.65625\n",
      "Training in progress @ global_step 164100, g_loss 0.565353, d_loss 0.504444 accuracy 0.640625\n",
      "Training in progress @ global_step 164150, g_loss 0.558782, d_loss 0.495344 accuracy 0.6875\n",
      "Training in progress @ global_step 164200, g_loss 0.560255, d_loss 0.509219 accuracy 0.609375\n",
      "Training in progress @ global_step 164250, g_loss 0.55798, d_loss 0.487153 accuracy 0.671875\n",
      "Training in progress @ global_step 164300, g_loss 0.561193, d_loss 0.53237 accuracy 0.546875\n",
      "Training in progress @ global_step 164350, g_loss 0.564614, d_loss 0.527876 accuracy 0.546875\n",
      "Training in progress @ global_step 164400, g_loss 0.55935, d_loss 0.515623 accuracy 0.59375\n",
      "Training in progress @ global_step 164450, g_loss 0.561019, d_loss 0.518139 accuracy 0.59375\n",
      "Training in progress @ global_step 164500, g_loss 0.564414, d_loss 0.527549 accuracy 0.578125\n",
      "Training in progress @ global_step 164550, g_loss 0.569333, d_loss 0.516937 accuracy 0.578125\n",
      "Training in progress @ global_step 164600, g_loss 0.563628, d_loss 0.5183 accuracy 0.5625\n",
      "Training in progress @ global_step 164650, g_loss 0.565932, d_loss 0.523503 accuracy 0.578125\n",
      "Training in progress @ global_step 164700, g_loss 0.562354, d_loss 0.528139 accuracy 0.546875\n",
      "Training in progress @ global_step 164750, g_loss 0.56308, d_loss 0.525746 accuracy 0.59375\n",
      "Training in progress @ global_step 164800, g_loss 0.565606, d_loss 0.522376 accuracy 0.59375\n",
      "Training in progress @ global_step 164850, g_loss 0.565844, d_loss 0.544674 accuracy 0.515625\n",
      "Training in progress @ global_step 164900, g_loss 0.560879, d_loss 0.532163 accuracy 0.5625\n",
      "Training in progress @ global_step 164950, g_loss 0.567721, d_loss 0.507282 accuracy 0.640625\n",
      "Training in progress @ global_step 165000, g_loss 0.565432, d_loss 0.53186 accuracy 0.546875\n",
      "Training in progress @ global_step 165050, g_loss 0.565942, d_loss 0.531235 accuracy 0.5625\n",
      "Training in progress @ global_step 165100, g_loss 0.561073, d_loss 0.522622 accuracy 0.609375\n",
      "Training in progress @ global_step 165150, g_loss 0.564375, d_loss 0.535135 accuracy 0.5625\n",
      "Training in progress @ global_step 165200, g_loss 0.563914, d_loss 0.523352 accuracy 0.578125\n",
      "Training in progress @ global_step 165250, g_loss 0.5646, d_loss 0.530764 accuracy 0.609375\n",
      "Training in progress @ global_step 165300, g_loss 0.565996, d_loss 0.536787 accuracy 0.515625\n",
      "Training in progress @ global_step 165350, g_loss 0.563469, d_loss 0.532 accuracy 0.53125\n",
      "Training in progress @ global_step 165400, g_loss 0.566717, d_loss 0.5287 accuracy 0.578125\n",
      "Training in progress @ global_step 165450, g_loss 0.565942, d_loss 0.530533 accuracy 0.53125\n",
      "Training in progress @ global_step 165500, g_loss 0.565534, d_loss 0.528227 accuracy 0.515625\n",
      "Training in progress @ global_step 165550, g_loss 0.562983, d_loss 0.53397 accuracy 0.578125\n",
      "Training in progress @ global_step 165600, g_loss 0.563091, d_loss 0.535022 accuracy 0.546875\n",
      "Training in progress @ global_step 165650, g_loss 0.565494, d_loss 0.511914 accuracy 0.609375\n",
      "Training in progress @ global_step 165700, g_loss 0.564819, d_loss 0.532722 accuracy 0.578125\n",
      "Training in progress @ global_step 165750, g_loss 0.566532, d_loss 0.526059 accuracy 0.578125\n",
      "Training in progress @ global_step 165800, g_loss 0.563713, d_loss 0.530476 accuracy 0.546875\n",
      "Training in progress @ global_step 165850, g_loss 0.561751, d_loss 0.524201 accuracy 0.59375\n",
      "Training in progress @ global_step 165900, g_loss 0.564573, d_loss 0.526654 accuracy 0.546875\n",
      "Training in progress @ global_step 165950, g_loss 0.564148, d_loss 0.528326 accuracy 0.53125\n",
      "Training in progress @ global_step 166000, g_loss 0.564816, d_loss 0.512912 accuracy 0.578125\n",
      "Training in progress @ global_step 166050, g_loss 0.565535, d_loss 0.529085 accuracy 0.546875\n",
      "Training in progress @ global_step 166100, g_loss 0.561771, d_loss 0.519345 accuracy 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 166150, g_loss 0.565111, d_loss 0.503008 accuracy 0.625\n",
      "Training in progress @ global_step 166200, g_loss 0.562904, d_loss 0.517108 accuracy 0.5625\n",
      "Training in progress @ global_step 166250, g_loss 0.562988, d_loss 0.501638 accuracy 0.578125\n",
      "Training in progress @ global_step 166300, g_loss 0.562432, d_loss 0.513113 accuracy 0.59375\n",
      "Training in progress @ global_step 166350, g_loss 0.558237, d_loss 0.512394 accuracy 0.609375\n",
      "Training in progress @ global_step 166400, g_loss 0.558457, d_loss 0.500518 accuracy 0.640625\n",
      "Training in progress @ global_step 166450, g_loss 0.555423, d_loss 0.507916 accuracy 0.578125\n",
      "Training in progress @ global_step 166500, g_loss 0.557575, d_loss 0.496703 accuracy 0.640625\n",
      "Training in progress @ global_step 166550, g_loss 0.55857, d_loss 0.50469 accuracy 0.625\n",
      "Training in progress @ global_step 166600, g_loss 0.564603, d_loss 0.498876 accuracy 0.625\n",
      "Training in progress @ global_step 166650, g_loss 0.567237, d_loss 0.501481 accuracy 0.59375\n",
      "Training in progress @ global_step 166700, g_loss 0.56203, d_loss 0.502117 accuracy 0.640625\n",
      "Training in progress @ global_step 166750, g_loss 0.558357, d_loss 0.495097 accuracy 0.65625\n",
      "Training in progress @ global_step 166800, g_loss 0.559408, d_loss 0.498313 accuracy 0.640625\n",
      "Training in progress @ global_step 166850, g_loss 0.559851, d_loss 0.499009 accuracy 0.609375\n",
      "Training in progress @ global_step 166900, g_loss 0.55957, d_loss 0.488383 accuracy 0.640625\n",
      "Training in progress @ global_step 166950, g_loss 0.562509, d_loss 0.490516 accuracy 0.671875\n",
      "Training in progress @ global_step 167000, g_loss 0.561136, d_loss 0.48772 accuracy 0.703125\n",
      "Training in progress @ global_step 167050, g_loss 0.564544, d_loss 0.499526 accuracy 0.640625\n",
      "Training in progress @ global_step 167100, g_loss 0.558795, d_loss 0.500473 accuracy 0.625\n",
      "Training in progress @ global_step 167150, g_loss 0.562736, d_loss 0.4904 accuracy 0.625\n",
      "Training in progress @ global_step 167200, g_loss 0.561445, d_loss 0.499515 accuracy 0.609375\n",
      "Training in progress @ global_step 167250, g_loss 0.557502, d_loss 0.493711 accuracy 0.65625\n",
      "Training in progress @ global_step 167300, g_loss 0.564383, d_loss 0.486465 accuracy 0.6875\n",
      "Training in progress @ global_step 167350, g_loss 0.558279, d_loss 0.485186 accuracy 0.703125\n",
      "Training in progress @ global_step 167400, g_loss 0.564969, d_loss 0.477728 accuracy 0.71875\n",
      "Training in progress @ global_step 167450, g_loss 0.563022, d_loss 0.492222 accuracy 0.65625\n",
      "Training in progress @ global_step 167500, g_loss 0.561284, d_loss 0.496616 accuracy 0.625\n",
      "Training in progress @ global_step 167550, g_loss 0.560766, d_loss 0.481957 accuracy 0.765625\n",
      "Training in progress @ global_step 167600, g_loss 0.566269, d_loss 0.495547 accuracy 0.65625\n",
      "Training in progress @ global_step 167650, g_loss 0.567373, d_loss 0.485998 accuracy 0.703125\n",
      "Training in progress @ global_step 167700, g_loss 0.5634, d_loss 0.484963 accuracy 0.71875\n",
      "Training in progress @ global_step 167750, g_loss 0.561533, d_loss 0.483384 accuracy 0.703125\n",
      "Training in progress @ global_step 167800, g_loss 0.561143, d_loss 0.493662 accuracy 0.65625\n",
      "Training in progress @ global_step 167850, g_loss 0.562549, d_loss 0.483837 accuracy 0.734375\n",
      "Training in progress @ global_step 167900, g_loss 0.56465, d_loss 0.498981 accuracy 0.640625\n",
      "Training in progress @ global_step 167950, g_loss 0.562864, d_loss 0.48659 accuracy 0.6875\n",
      "Training in progress @ global_step 168000, g_loss 0.563056, d_loss 0.477166 accuracy 0.71875\n",
      "Training in progress @ global_step 168050, g_loss 0.561954, d_loss 0.489425 accuracy 0.671875\n",
      "Training in progress @ global_step 168100, g_loss 0.559789, d_loss 0.494795 accuracy 0.6875\n",
      "Training in progress @ global_step 168150, g_loss 0.558253, d_loss 0.500818 accuracy 0.65625\n",
      "Training in progress @ global_step 168200, g_loss 0.556922, d_loss 0.478228 accuracy 0.71875\n",
      "Training in progress @ global_step 168250, g_loss 0.565558, d_loss 0.504597 accuracy 0.59375\n",
      "Training in progress @ global_step 168300, g_loss 0.561043, d_loss 0.472386 accuracy 0.734375\n",
      "Training in progress @ global_step 168350, g_loss 0.561467, d_loss 0.501466 accuracy 0.65625\n",
      "Training in progress @ global_step 168400, g_loss 0.562074, d_loss 0.508063 accuracy 0.640625\n",
      "Training in progress @ global_step 168450, g_loss 0.557884, d_loss 0.485079 accuracy 0.6875\n",
      "Training in progress @ global_step 168500, g_loss 0.561204, d_loss 0.489568 accuracy 0.71875\n",
      "Training in progress @ global_step 168550, g_loss 0.556497, d_loss 0.495275 accuracy 0.6875\n",
      "Training in progress @ global_step 168600, g_loss 0.55896, d_loss 0.500711 accuracy 0.640625\n",
      "Training in progress @ global_step 168650, g_loss 0.554904, d_loss 0.503972 accuracy 0.640625\n",
      "Training in progress @ global_step 168700, g_loss 0.558474, d_loss 0.505729 accuracy 0.609375\n",
      "Training in progress @ global_step 168750, g_loss 0.555787, d_loss 0.50748 accuracy 0.578125\n",
      "Training in progress @ global_step 168800, g_loss 0.553867, d_loss 0.504649 accuracy 0.625\n",
      "Training in progress @ global_step 168850, g_loss 0.560133, d_loss 0.498436 accuracy 0.671875\n",
      "Training in progress @ global_step 168900, g_loss 0.556325, d_loss 0.503407 accuracy 0.609375\n",
      "Training in progress @ global_step 168950, g_loss 0.556863, d_loss 0.515642 accuracy 0.59375\n",
      "Training in progress @ global_step 169000, g_loss 0.559896, d_loss 0.514653 accuracy 0.609375\n",
      "Training in progress @ global_step 169050, g_loss 0.558285, d_loss 0.518728 accuracy 0.546875\n",
      "Training in progress @ global_step 169100, g_loss 0.55885, d_loss 0.500758 accuracy 0.625\n",
      "Training in progress @ global_step 169150, g_loss 0.55615, d_loss 0.511362 accuracy 0.640625\n",
      "Training in progress @ global_step 169200, g_loss 0.553584, d_loss 0.524318 accuracy 0.5625\n",
      "Training in progress @ global_step 169250, g_loss 0.558669, d_loss 0.51638 accuracy 0.5625\n",
      "Training in progress @ global_step 169300, g_loss 0.557646, d_loss 0.515231 accuracy 0.609375\n",
      "Training in progress @ global_step 169350, g_loss 0.555839, d_loss 0.508671 accuracy 0.609375\n",
      "Training in progress @ global_step 169400, g_loss 0.554945, d_loss 0.523273 accuracy 0.5625\n",
      "Training in progress @ global_step 169450, g_loss 0.554113, d_loss 0.517036 accuracy 0.546875\n",
      "Training in progress @ global_step 169500, g_loss 0.558743, d_loss 0.504793 accuracy 0.640625\n",
      "Training in progress @ global_step 169550, g_loss 0.556631, d_loss 0.499803 accuracy 0.671875\n",
      "Training in progress @ global_step 169600, g_loss 0.553481, d_loss 0.506 accuracy 0.65625\n",
      "Training in progress @ global_step 169650, g_loss 0.556244, d_loss 0.518038 accuracy 0.5625\n",
      "Training in progress @ global_step 169700, g_loss 0.558187, d_loss 0.516884 accuracy 0.625\n",
      "Training in progress @ global_step 169750, g_loss 0.555175, d_loss 0.527249 accuracy 0.53125\n",
      "Training in progress @ global_step 169800, g_loss 0.554665, d_loss 0.5129 accuracy 0.578125\n",
      "Training in progress @ global_step 169850, g_loss 0.560984, d_loss 0.510616 accuracy 0.59375\n",
      "Training in progress @ global_step 169900, g_loss 0.5574, d_loss 0.519401 accuracy 0.546875\n",
      "Training in progress @ global_step 169950, g_loss 0.558957, d_loss 0.501689 accuracy 0.625\n",
      "Training in progress @ global_step 170000, g_loss 0.556537, d_loss 0.514539 accuracy 0.640625\n",
      "Training in progress @ global_step 170050, g_loss 0.564875, d_loss 0.499264 accuracy 0.671875\n",
      "Training in progress @ global_step 170100, g_loss 0.556043, d_loss 0.502908 accuracy 0.65625\n",
      "Training in progress @ global_step 170150, g_loss 0.557925, d_loss 0.50615 accuracy 0.640625\n",
      "Training in progress @ global_step 170200, g_loss 0.55826, d_loss 0.495758 accuracy 0.625\n",
      "Training in progress @ global_step 170250, g_loss 0.559974, d_loss 0.492325 accuracy 0.703125\n",
      "Training in progress @ global_step 170300, g_loss 0.563312, d_loss 0.506168 accuracy 0.640625\n",
      "Training in progress @ global_step 170350, g_loss 0.557032, d_loss 0.498162 accuracy 0.671875\n",
      "Training in progress @ global_step 170400, g_loss 0.565178, d_loss 0.4878 accuracy 0.6875\n",
      "Training in progress @ global_step 170450, g_loss 0.561317, d_loss 0.496226 accuracy 0.671875\n",
      "Training in progress @ global_step 170500, g_loss 0.565437, d_loss 0.489683 accuracy 0.6875\n",
      "Training in progress @ global_step 170550, g_loss 0.563843, d_loss 0.487903 accuracy 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 170600, g_loss 0.566517, d_loss 0.489837 accuracy 0.671875\n",
      "Training in progress @ global_step 170650, g_loss 0.563173, d_loss 0.487099 accuracy 0.71875\n",
      "Training in progress @ global_step 170700, g_loss 0.564566, d_loss 0.478681 accuracy 0.71875\n",
      "Training in progress @ global_step 170750, g_loss 0.564727, d_loss 0.488283 accuracy 0.703125\n",
      "Training in progress @ global_step 170800, g_loss 0.563478, d_loss 0.482575 accuracy 0.703125\n",
      "Training in progress @ global_step 170850, g_loss 0.565324, d_loss 0.489092 accuracy 0.671875\n",
      "Training in progress @ global_step 170900, g_loss 0.563887, d_loss 0.482558 accuracy 0.703125\n",
      "Training in progress @ global_step 170950, g_loss 0.565465, d_loss 0.479367 accuracy 0.734375\n",
      "Training in progress @ global_step 171000, g_loss 0.565971, d_loss 0.498387 accuracy 0.640625\n",
      "Training in progress @ global_step 171050, g_loss 0.565188, d_loss 0.482957 accuracy 0.703125\n",
      "Training in progress @ global_step 171100, g_loss 0.566468, d_loss 0.495427 accuracy 0.640625\n",
      "Training in progress @ global_step 171150, g_loss 0.567483, d_loss 0.491818 accuracy 0.703125\n",
      "Training in progress @ global_step 171200, g_loss 0.566101, d_loss 0.487722 accuracy 0.75\n",
      "Training in progress @ global_step 171250, g_loss 0.564901, d_loss 0.48294 accuracy 0.703125\n",
      "Training in progress @ global_step 171300, g_loss 0.567564, d_loss 0.486945 accuracy 0.6875\n",
      "Training in progress @ global_step 171350, g_loss 0.569986, d_loss 0.494075 accuracy 0.65625\n",
      "Training in progress @ global_step 171400, g_loss 0.566707, d_loss 0.487244 accuracy 0.6875\n",
      "Training in progress @ global_step 171450, g_loss 0.565376, d_loss 0.469803 accuracy 0.734375\n",
      "Training in progress @ global_step 171500, g_loss 0.566279, d_loss 0.496091 accuracy 0.65625\n",
      "Training in progress @ global_step 171550, g_loss 0.568605, d_loss 0.495496 accuracy 0.640625\n",
      "Training in progress @ global_step 171600, g_loss 0.570266, d_loss 0.49201 accuracy 0.640625\n",
      "Training in progress @ global_step 171650, g_loss 0.566907, d_loss 0.482803 accuracy 0.734375\n",
      "Training in progress @ global_step 171700, g_loss 0.568612, d_loss 0.489318 accuracy 0.703125\n",
      "Training in progress @ global_step 171750, g_loss 0.566439, d_loss 0.503084 accuracy 0.65625\n",
      "Training in progress @ global_step 171800, g_loss 0.571141, d_loss 0.500779 accuracy 0.640625\n",
      "Training in progress @ global_step 171850, g_loss 0.568194, d_loss 0.50614 accuracy 0.59375\n",
      "Training in progress @ global_step 171900, g_loss 0.570717, d_loss 0.493583 accuracy 0.671875\n",
      "Training in progress @ global_step 171950, g_loss 0.567539, d_loss 0.499454 accuracy 0.609375\n",
      "Training in progress @ global_step 172000, g_loss 0.573406, d_loss 0.485488 accuracy 0.65625\n",
      "Training in progress @ global_step 172050, g_loss 0.572753, d_loss 0.503977 accuracy 0.65625\n",
      "Training in progress @ global_step 172100, g_loss 0.570348, d_loss 0.516822 accuracy 0.59375\n",
      "Training in progress @ global_step 172150, g_loss 0.56857, d_loss 0.514672 accuracy 0.625\n",
      "Training in progress @ global_step 172200, g_loss 0.56662, d_loss 0.504083 accuracy 0.671875\n",
      "Training in progress @ global_step 172250, g_loss 0.571568, d_loss 0.519488 accuracy 0.625\n",
      "Training in progress @ global_step 172300, g_loss 0.570986, d_loss 0.506908 accuracy 0.59375\n",
      "Training in progress @ global_step 172350, g_loss 0.577314, d_loss 0.524904 accuracy 0.59375\n",
      "Training in progress @ global_step 172400, g_loss 0.57436, d_loss 0.517248 accuracy 0.609375\n",
      "Training in progress @ global_step 172450, g_loss 0.573951, d_loss 0.52102 accuracy 0.578125\n",
      "Training in progress @ global_step 172500, g_loss 0.569362, d_loss 0.534698 accuracy 0.5625\n",
      "Training in progress @ global_step 172550, g_loss 0.572828, d_loss 0.524862 accuracy 0.59375\n",
      "Training in progress @ global_step 172600, g_loss 0.573196, d_loss 0.529555 accuracy 0.53125\n",
      "Training in progress @ global_step 172650, g_loss 0.566769, d_loss 0.515748 accuracy 0.59375\n",
      "Training in progress @ global_step 172700, g_loss 0.571987, d_loss 0.521938 accuracy 0.59375\n",
      "Training in progress @ global_step 172750, g_loss 0.576813, d_loss 0.520069 accuracy 0.578125\n",
      "Training in progress @ global_step 172800, g_loss 0.572515, d_loss 0.523558 accuracy 0.5625\n",
      "Training in progress @ global_step 172850, g_loss 0.570489, d_loss 0.522126 accuracy 0.5625\n",
      "Training in progress @ global_step 172900, g_loss 0.574719, d_loss 0.519448 accuracy 0.5625\n",
      "Training in progress @ global_step 172950, g_loss 0.569685, d_loss 0.522844 accuracy 0.59375\n",
      "Training in progress @ global_step 173000, g_loss 0.575376, d_loss 0.527783 accuracy 0.515625\n",
      "Training in progress @ global_step 173050, g_loss 0.572592, d_loss 0.516123 accuracy 0.609375\n",
      "Training in progress @ global_step 173100, g_loss 0.57217, d_loss 0.531912 accuracy 0.546875\n",
      "Training in progress @ global_step 173150, g_loss 0.569939, d_loss 0.535664 accuracy 0.5\n",
      "Training in progress @ global_step 173200, g_loss 0.566696, d_loss 0.538359 accuracy 0.546875\n",
      "Training in progress @ global_step 173250, g_loss 0.574015, d_loss 0.535415 accuracy 0.5625\n",
      "Training in progress @ global_step 173300, g_loss 0.56914, d_loss 0.520921 accuracy 0.578125\n",
      "Training in progress @ global_step 173350, g_loss 0.570478, d_loss 0.506503 accuracy 0.609375\n",
      "Training in progress @ global_step 173400, g_loss 0.568538, d_loss 0.528824 accuracy 0.546875\n",
      "Training in progress @ global_step 173450, g_loss 0.570429, d_loss 0.530419 accuracy 0.578125\n",
      "Training in progress @ global_step 173500, g_loss 0.562957, d_loss 0.514254 accuracy 0.5625\n",
      "Training in progress @ global_step 173550, g_loss 0.568344, d_loss 0.500315 accuracy 0.625\n",
      "Training in progress @ global_step 173600, g_loss 0.566804, d_loss 0.510671 accuracy 0.59375\n",
      "Training in progress @ global_step 173650, g_loss 0.564179, d_loss 0.51667 accuracy 0.59375\n",
      "Training in progress @ global_step 173700, g_loss 0.573963, d_loss 0.509247 accuracy 0.5625\n",
      "Training in progress @ global_step 173750, g_loss 0.567178, d_loss 0.4956 accuracy 0.640625\n",
      "Training in progress @ global_step 173800, g_loss 0.566205, d_loss 0.504078 accuracy 0.625\n",
      "Training in progress @ global_step 173850, g_loss 0.56345, d_loss 0.505157 accuracy 0.609375\n",
      "Training in progress @ global_step 173900, g_loss 0.566961, d_loss 0.519364 accuracy 0.5625\n",
      "Training in progress @ global_step 173950, g_loss 0.570164, d_loss 0.51949 accuracy 0.53125\n",
      "Training in progress @ global_step 174000, g_loss 0.574249, d_loss 0.521861 accuracy 0.546875\n",
      "Training in progress @ global_step 174050, g_loss 0.573187, d_loss 0.510701 accuracy 0.59375\n",
      "Training in progress @ global_step 174100, g_loss 0.569329, d_loss 0.521175 accuracy 0.59375\n",
      "Training in progress @ global_step 174150, g_loss 0.566103, d_loss 0.492918 accuracy 0.65625\n",
      "Training in progress @ global_step 174200, g_loss 0.56841, d_loss 0.513584 accuracy 0.578125\n",
      "Training in progress @ global_step 174250, g_loss 0.570732, d_loss 0.498004 accuracy 0.640625\n",
      "Training in progress @ global_step 174300, g_loss 0.566323, d_loss 0.509924 accuracy 0.609375\n",
      "Training in progress @ global_step 174350, g_loss 0.565905, d_loss 0.487765 accuracy 0.71875\n",
      "Training in progress @ global_step 174400, g_loss 0.565508, d_loss 0.494436 accuracy 0.671875\n",
      "Training in progress @ global_step 174450, g_loss 0.562741, d_loss 0.491478 accuracy 0.625\n",
      "Training in progress @ global_step 174500, g_loss 0.565105, d_loss 0.479743 accuracy 0.71875\n",
      "Training in progress @ global_step 174550, g_loss 0.568398, d_loss 0.499283 accuracy 0.65625\n",
      "Training in progress @ global_step 174600, g_loss 0.566123, d_loss 0.475927 accuracy 0.75\n",
      "Training in progress @ global_step 174650, g_loss 0.567958, d_loss 0.490304 accuracy 0.6875\n",
      "Training in progress @ global_step 174700, g_loss 0.566785, d_loss 0.502657 accuracy 0.65625\n",
      "Training in progress @ global_step 174750, g_loss 0.566665, d_loss 0.497522 accuracy 0.640625\n",
      "Training in progress @ global_step 174800, g_loss 0.56682, d_loss 0.511468 accuracy 0.59375\n",
      "Training in progress @ global_step 174850, g_loss 0.570342, d_loss 0.493071 accuracy 0.65625\n",
      "Training in progress @ global_step 174900, g_loss 0.568607, d_loss 0.48461 accuracy 0.671875\n",
      "Training in progress @ global_step 174950, g_loss 0.569246, d_loss 0.486257 accuracy 0.71875\n",
      "Training in progress @ global_step 175000, g_loss 0.567021, d_loss 0.486856 accuracy 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 175050, g_loss 0.567474, d_loss 0.498921 accuracy 0.609375\n",
      "Training in progress @ global_step 175100, g_loss 0.565782, d_loss 0.470463 accuracy 0.78125\n",
      "Training in progress @ global_step 175150, g_loss 0.566468, d_loss 0.493019 accuracy 0.671875\n",
      "Training in progress @ global_step 175200, g_loss 0.564741, d_loss 0.49454 accuracy 0.640625\n",
      "Training in progress @ global_step 175250, g_loss 0.567557, d_loss 0.49835 accuracy 0.640625\n",
      "Training in progress @ global_step 175300, g_loss 0.566316, d_loss 0.481453 accuracy 0.71875\n",
      "Training in progress @ global_step 175350, g_loss 0.566331, d_loss 0.498599 accuracy 0.59375\n",
      "Training in progress @ global_step 175400, g_loss 0.564219, d_loss 0.491562 accuracy 0.71875\n",
      "Training in progress @ global_step 175450, g_loss 0.561916, d_loss 0.487726 accuracy 0.6875\n",
      "Training in progress @ global_step 175500, g_loss 0.566176, d_loss 0.4843 accuracy 0.6875\n",
      "Training in progress @ global_step 175550, g_loss 0.559421, d_loss 0.494726 accuracy 0.671875\n",
      "Training in progress @ global_step 175600, g_loss 0.566565, d_loss 0.484834 accuracy 0.703125\n",
      "Training in progress @ global_step 175650, g_loss 0.563281, d_loss 0.497808 accuracy 0.703125\n",
      "Training in progress @ global_step 175700, g_loss 0.565098, d_loss 0.490432 accuracy 0.71875\n",
      "Training in progress @ global_step 175750, g_loss 0.561414, d_loss 0.488364 accuracy 0.671875\n",
      "Training in progress @ global_step 175800, g_loss 0.561966, d_loss 0.510154 accuracy 0.609375\n",
      "Training in progress @ global_step 175850, g_loss 0.564057, d_loss 0.484211 accuracy 0.75\n",
      "Training in progress @ global_step 175900, g_loss 0.561789, d_loss 0.495879 accuracy 0.625\n",
      "Training in progress @ global_step 175950, g_loss 0.564441, d_loss 0.487403 accuracy 0.671875\n",
      "Training in progress @ global_step 176000, g_loss 0.559435, d_loss 0.508909 accuracy 0.59375\n",
      "Training in progress @ global_step 176050, g_loss 0.564511, d_loss 0.498597 accuracy 0.640625\n",
      "Training in progress @ global_step 176100, g_loss 0.564445, d_loss 0.486959 accuracy 0.6875\n",
      "Training in progress @ global_step 176150, g_loss 0.563997, d_loss 0.521987 accuracy 0.53125\n",
      "Training in progress @ global_step 176200, g_loss 0.562064, d_loss 0.507378 accuracy 0.59375\n",
      "Training in progress @ global_step 176250, g_loss 0.563232, d_loss 0.496028 accuracy 0.65625\n",
      "Training in progress @ global_step 176300, g_loss 0.561983, d_loss 0.507892 accuracy 0.578125\n",
      "Training in progress @ global_step 176350, g_loss 0.562408, d_loss 0.509654 accuracy 0.625\n",
      "Training in progress @ global_step 176400, g_loss 0.5614, d_loss 0.483094 accuracy 0.6875\n",
      "Training in progress @ global_step 176450, g_loss 0.561386, d_loss 0.494315 accuracy 0.6875\n",
      "Training in progress @ global_step 176500, g_loss 0.561649, d_loss 0.487593 accuracy 0.703125\n",
      "Training in progress @ global_step 176550, g_loss 0.565555, d_loss 0.501929 accuracy 0.65625\n",
      "Training in progress @ global_step 176600, g_loss 0.564133, d_loss 0.494328 accuracy 0.703125\n",
      "Training in progress @ global_step 176650, g_loss 0.56576, d_loss 0.51382 accuracy 0.609375\n",
      "Training in progress @ global_step 176700, g_loss 0.561758, d_loss 0.506806 accuracy 0.609375\n",
      "Training in progress @ global_step 176750, g_loss 0.561104, d_loss 0.508728 accuracy 0.609375\n",
      "Training in progress @ global_step 176800, g_loss 0.563216, d_loss 0.485239 accuracy 0.703125\n",
      "Training in progress @ global_step 176850, g_loss 0.563212, d_loss 0.500524 accuracy 0.625\n",
      "Training in progress @ global_step 176900, g_loss 0.562392, d_loss 0.505791 accuracy 0.625\n",
      "Training in progress @ global_step 176950, g_loss 0.562314, d_loss 0.500212 accuracy 0.640625\n",
      "Training in progress @ global_step 177000, g_loss 0.561919, d_loss 0.502425 accuracy 0.609375\n",
      "Training in progress @ global_step 177050, g_loss 0.562505, d_loss 0.498099 accuracy 0.703125\n",
      "Training in progress @ global_step 177100, g_loss 0.5643, d_loss 0.48914 accuracy 0.703125\n",
      "Training in progress @ global_step 177150, g_loss 0.563602, d_loss 0.499869 accuracy 0.625\n",
      "Training in progress @ global_step 177200, g_loss 0.564367, d_loss 0.485998 accuracy 0.671875\n",
      "Training in progress @ global_step 177250, g_loss 0.567974, d_loss 0.496505 accuracy 0.65625\n",
      "Training in progress @ global_step 177300, g_loss 0.563587, d_loss 0.496283 accuracy 0.71875\n",
      "Training in progress @ global_step 177350, g_loss 0.567993, d_loss 0.496129 accuracy 0.671875\n",
      "Training in progress @ global_step 177400, g_loss 0.566388, d_loss 0.49359 accuracy 0.6875\n",
      "Training in progress @ global_step 177450, g_loss 0.567309, d_loss 0.502849 accuracy 0.640625\n",
      "Training in progress @ global_step 177500, g_loss 0.566991, d_loss 0.489298 accuracy 0.703125\n",
      "Training in progress @ global_step 177550, g_loss 0.570016, d_loss 0.507308 accuracy 0.640625\n",
      "Training in progress @ global_step 177600, g_loss 0.56384, d_loss 0.491739 accuracy 0.671875\n",
      "Training in progress @ global_step 177650, g_loss 0.57053, d_loss 0.509493 accuracy 0.609375\n",
      "Training in progress @ global_step 177700, g_loss 0.564904, d_loss 0.494586 accuracy 0.609375\n",
      "Training in progress @ global_step 177750, g_loss 0.568646, d_loss 0.488169 accuracy 0.6875\n",
      "Training in progress @ global_step 177800, g_loss 0.569251, d_loss 0.493196 accuracy 0.640625\n",
      "Training in progress @ global_step 177850, g_loss 0.56999, d_loss 0.486722 accuracy 0.6875\n",
      "Training in progress @ global_step 177900, g_loss 0.564378, d_loss 0.478364 accuracy 0.703125\n",
      "Training in progress @ global_step 177950, g_loss 0.56392, d_loss 0.491532 accuracy 0.671875\n",
      "Training in progress @ global_step 178000, g_loss 0.56935, d_loss 0.486414 accuracy 0.6875\n",
      "Training in progress @ global_step 178050, g_loss 0.56645, d_loss 0.486619 accuracy 0.671875\n",
      "Training in progress @ global_step 178100, g_loss 0.567473, d_loss 0.487367 accuracy 0.703125\n",
      "Training in progress @ global_step 178150, g_loss 0.569783, d_loss 0.491394 accuracy 0.671875\n",
      "Training in progress @ global_step 178200, g_loss 0.572365, d_loss 0.506947 accuracy 0.578125\n",
      "Training in progress @ global_step 178250, g_loss 0.571544, d_loss 0.469606 accuracy 0.75\n",
      "Training in progress @ global_step 178300, g_loss 0.568008, d_loss 0.493838 accuracy 0.625\n",
      "Training in progress @ global_step 178350, g_loss 0.567618, d_loss 0.483682 accuracy 0.65625\n",
      "Training in progress @ global_step 178400, g_loss 0.567577, d_loss 0.491701 accuracy 0.625\n",
      "Training in progress @ global_step 178450, g_loss 0.569124, d_loss 0.497478 accuracy 0.609375\n",
      "Training in progress @ global_step 178500, g_loss 0.565282, d_loss 0.504983 accuracy 0.640625\n",
      "Training in progress @ global_step 178550, g_loss 0.567326, d_loss 0.48808 accuracy 0.6875\n",
      "Training in progress @ global_step 178600, g_loss 0.568026, d_loss 0.481769 accuracy 0.703125\n",
      "Training in progress @ global_step 178650, g_loss 0.571089, d_loss 0.492732 accuracy 0.625\n",
      "Training in progress @ global_step 178700, g_loss 0.573048, d_loss 0.493673 accuracy 0.671875\n",
      "Training in progress @ global_step 178750, g_loss 0.566389, d_loss 0.481649 accuracy 0.703125\n",
      "Training in progress @ global_step 178800, g_loss 0.568433, d_loss 0.492943 accuracy 0.6875\n",
      "Training in progress @ global_step 178850, g_loss 0.573202, d_loss 0.498779 accuracy 0.6875\n",
      "Training in progress @ global_step 178900, g_loss 0.565996, d_loss 0.498309 accuracy 0.625\n",
      "Training in progress @ global_step 178950, g_loss 0.570277, d_loss 0.501764 accuracy 0.609375\n",
      "Training in progress @ global_step 179000, g_loss 0.568388, d_loss 0.510902 accuracy 0.578125\n",
      "Training in progress @ global_step 179050, g_loss 0.568862, d_loss 0.503322 accuracy 0.609375\n",
      "Training in progress @ global_step 179100, g_loss 0.570706, d_loss 0.501584 accuracy 0.640625\n",
      "Training in progress @ global_step 179150, g_loss 0.569708, d_loss 0.496201 accuracy 0.609375\n",
      "Training in progress @ global_step 179200, g_loss 0.574374, d_loss 0.497377 accuracy 0.640625\n",
      "Training in progress @ global_step 179250, g_loss 0.573659, d_loss 0.518798 accuracy 0.53125\n",
      "Training in progress @ global_step 179300, g_loss 0.57223, d_loss 0.508878 accuracy 0.609375\n",
      "Training in progress @ global_step 179350, g_loss 0.56931, d_loss 0.499815 accuracy 0.671875\n",
      "Training in progress @ global_step 179400, g_loss 0.572513, d_loss 0.510012 accuracy 0.59375\n",
      "Training in progress @ global_step 179450, g_loss 0.568308, d_loss 0.501403 accuracy 0.640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 179500, g_loss 0.569793, d_loss 0.489233 accuracy 0.65625\n",
      "Training in progress @ global_step 179550, g_loss 0.569685, d_loss 0.51541 accuracy 0.59375\n",
      "Training in progress @ global_step 179600, g_loss 0.569319, d_loss 0.520716 accuracy 0.53125\n",
      "Training in progress @ global_step 179650, g_loss 0.564843, d_loss 0.50785 accuracy 0.59375\n",
      "Training in progress @ global_step 179700, g_loss 0.567146, d_loss 0.497283 accuracy 0.625\n",
      "Training in progress @ global_step 179750, g_loss 0.568313, d_loss 0.517273 accuracy 0.546875\n",
      "Training in progress @ global_step 179800, g_loss 0.566395, d_loss 0.507033 accuracy 0.578125\n",
      "Training in progress @ global_step 179850, g_loss 0.57188, d_loss 0.504947 accuracy 0.609375\n",
      "Training in progress @ global_step 179900, g_loss 0.565512, d_loss 0.500279 accuracy 0.65625\n",
      "Training in progress @ global_step 179950, g_loss 0.568103, d_loss 0.497234 accuracy 0.640625\n",
      "Training in progress @ global_step 180000, g_loss 0.569699, d_loss 0.503768 accuracy 0.640625\n",
      "Training in progress @ global_step 180050, g_loss 0.569348, d_loss 0.516142 accuracy 0.546875\n",
      "Training in progress @ global_step 180100, g_loss 0.571967, d_loss 0.508658 accuracy 0.578125\n",
      "Training in progress @ global_step 180150, g_loss 0.568195, d_loss 0.505396 accuracy 0.609375\n",
      "Training in progress @ global_step 180200, g_loss 0.570435, d_loss 0.50045 accuracy 0.671875\n",
      "Training in progress @ global_step 180250, g_loss 0.571308, d_loss 0.495967 accuracy 0.625\n",
      "Training in progress @ global_step 180300, g_loss 0.575656, d_loss 0.512351 accuracy 0.609375\n",
      "Training in progress @ global_step 180350, g_loss 0.567359, d_loss 0.496864 accuracy 0.640625\n",
      "Training in progress @ global_step 180400, g_loss 0.568482, d_loss 0.50802 accuracy 0.640625\n",
      "Training in progress @ global_step 180450, g_loss 0.568183, d_loss 0.507624 accuracy 0.625\n",
      "Training in progress @ global_step 180500, g_loss 0.572252, d_loss 0.499661 accuracy 0.609375\n",
      "Training in progress @ global_step 180550, g_loss 0.569322, d_loss 0.507573 accuracy 0.625\n",
      "Training in progress @ global_step 180600, g_loss 0.567298, d_loss 0.51435 accuracy 0.609375\n",
      "Training in progress @ global_step 180650, g_loss 0.567626, d_loss 0.496469 accuracy 0.65625\n",
      "Training in progress @ global_step 180700, g_loss 0.571076, d_loss 0.489999 accuracy 0.640625\n",
      "Training in progress @ global_step 180750, g_loss 0.56759, d_loss 0.491779 accuracy 0.71875\n",
      "Training in progress @ global_step 180800, g_loss 0.56877, d_loss 0.485193 accuracy 0.71875\n",
      "Training in progress @ global_step 180850, g_loss 0.571651, d_loss 0.492519 accuracy 0.65625\n",
      "Training in progress @ global_step 180900, g_loss 0.568952, d_loss 0.505063 accuracy 0.609375\n",
      "Training in progress @ global_step 180950, g_loss 0.575454, d_loss 0.481691 accuracy 0.71875\n",
      "Training in progress @ global_step 181000, g_loss 0.570224, d_loss 0.476746 accuracy 0.75\n",
      "Training in progress @ global_step 181050, g_loss 0.569328, d_loss 0.492473 accuracy 0.6875\n",
      "Training in progress @ global_step 181100, g_loss 0.57058, d_loss 0.501017 accuracy 0.640625\n",
      "Training in progress @ global_step 181150, g_loss 0.570099, d_loss 0.483624 accuracy 0.703125\n",
      "Training in progress @ global_step 181200, g_loss 0.569849, d_loss 0.48328 accuracy 0.703125\n",
      "Training in progress @ global_step 181250, g_loss 0.573768, d_loss 0.484421 accuracy 0.71875\n",
      "Training in progress @ global_step 181300, g_loss 0.570686, d_loss 0.475193 accuracy 0.75\n",
      "Training in progress @ global_step 181350, g_loss 0.569475, d_loss 0.491038 accuracy 0.65625\n",
      "Training in progress @ global_step 181400, g_loss 0.568603, d_loss 0.502946 accuracy 0.609375\n",
      "Training in progress @ global_step 181450, g_loss 0.570572, d_loss 0.48522 accuracy 0.6875\n",
      "Training in progress @ global_step 181500, g_loss 0.567142, d_loss 0.498054 accuracy 0.65625\n",
      "Training in progress @ global_step 181550, g_loss 0.568175, d_loss 0.487741 accuracy 0.6875\n",
      "Training in progress @ global_step 181600, g_loss 0.572256, d_loss 0.490708 accuracy 0.71875\n",
      "Training in progress @ global_step 181650, g_loss 0.570697, d_loss 0.483128 accuracy 0.6875\n",
      "Training in progress @ global_step 181700, g_loss 0.57308, d_loss 0.502511 accuracy 0.625\n",
      "Training in progress @ global_step 181750, g_loss 0.570966, d_loss 0.490831 accuracy 0.671875\n",
      "Training in progress @ global_step 181800, g_loss 0.56768, d_loss 0.494765 accuracy 0.640625\n",
      "Training in progress @ global_step 181850, g_loss 0.569782, d_loss 0.491767 accuracy 0.71875\n",
      "Training in progress @ global_step 181900, g_loss 0.56798, d_loss 0.486588 accuracy 0.6875\n",
      "Training in progress @ global_step 181950, g_loss 0.570182, d_loss 0.488847 accuracy 0.6875\n",
      "Training in progress @ global_step 182000, g_loss 0.565906, d_loss 0.507084 accuracy 0.59375\n",
      "Training in progress @ global_step 182050, g_loss 0.566981, d_loss 0.487724 accuracy 0.71875\n",
      "Training in progress @ global_step 182100, g_loss 0.563483, d_loss 0.505997 accuracy 0.640625\n",
      "Training in progress @ global_step 182150, g_loss 0.569482, d_loss 0.500733 accuracy 0.625\n",
      "Training in progress @ global_step 182200, g_loss 0.567659, d_loss 0.487078 accuracy 0.6875\n",
      "Training in progress @ global_step 182250, g_loss 0.56583, d_loss 0.504788 accuracy 0.609375\n",
      "Training in progress @ global_step 182300, g_loss 0.56381, d_loss 0.493134 accuracy 0.671875\n",
      "Training in progress @ global_step 182350, g_loss 0.5687, d_loss 0.507146 accuracy 0.609375\n",
      "Training in progress @ global_step 182400, g_loss 0.566102, d_loss 0.502902 accuracy 0.625\n",
      "Training in progress @ global_step 182450, g_loss 0.56412, d_loss 0.497544 accuracy 0.625\n",
      "Training in progress @ global_step 182500, g_loss 0.566951, d_loss 0.509651 accuracy 0.578125\n",
      "Training in progress @ global_step 182550, g_loss 0.565095, d_loss 0.506502 accuracy 0.59375\n",
      "Training in progress @ global_step 182600, g_loss 0.564291, d_loss 0.512133 accuracy 0.5625\n",
      "Training in progress @ global_step 182650, g_loss 0.563784, d_loss 0.503099 accuracy 0.578125\n",
      "Training in progress @ global_step 182700, g_loss 0.565109, d_loss 0.499633 accuracy 0.6875\n",
      "Training in progress @ global_step 182750, g_loss 0.563948, d_loss 0.507626 accuracy 0.59375\n",
      "Training in progress @ global_step 182800, g_loss 0.566172, d_loss 0.49501 accuracy 0.609375\n",
      "Training in progress @ global_step 182850, g_loss 0.567431, d_loss 0.501732 accuracy 0.609375\n",
      "Training in progress @ global_step 182900, g_loss 0.565552, d_loss 0.501286 accuracy 0.703125\n",
      "Training in progress @ global_step 182950, g_loss 0.562633, d_loss 0.502104 accuracy 0.640625\n",
      "Training in progress @ global_step 183000, g_loss 0.562644, d_loss 0.50435 accuracy 0.625\n",
      "Training in progress @ global_step 183050, g_loss 0.563298, d_loss 0.493052 accuracy 0.65625\n",
      "Training in progress @ global_step 183100, g_loss 0.566514, d_loss 0.506051 accuracy 0.625\n",
      "Training in progress @ global_step 183150, g_loss 0.561384, d_loss 0.503538 accuracy 0.625\n",
      "Training in progress @ global_step 183200, g_loss 0.562581, d_loss 0.505202 accuracy 0.625\n",
      "Training in progress @ global_step 183250, g_loss 0.56418, d_loss 0.495031 accuracy 0.65625\n",
      "Training in progress @ global_step 183300, g_loss 0.563689, d_loss 0.520614 accuracy 0.5625\n",
      "Training in progress @ global_step 183350, g_loss 0.566155, d_loss 0.515411 accuracy 0.578125\n",
      "Training in progress @ global_step 183400, g_loss 0.562396, d_loss 0.497095 accuracy 0.671875\n",
      "Training in progress @ global_step 183450, g_loss 0.564032, d_loss 0.481714 accuracy 0.6875\n",
      "Training in progress @ global_step 183500, g_loss 0.566411, d_loss 0.500677 accuracy 0.625\n",
      "Training in progress @ global_step 183550, g_loss 0.564728, d_loss 0.49954 accuracy 0.625\n",
      "Training in progress @ global_step 183600, g_loss 0.563854, d_loss 0.491282 accuracy 0.6875\n",
      "Training in progress @ global_step 183650, g_loss 0.562528, d_loss 0.480527 accuracy 0.734375\n",
      "Training in progress @ global_step 183700, g_loss 0.564008, d_loss 0.489565 accuracy 0.671875\n",
      "Training in progress @ global_step 183750, g_loss 0.568743, d_loss 0.493079 accuracy 0.65625\n",
      "Training in progress @ global_step 183800, g_loss 0.564448, d_loss 0.502894 accuracy 0.59375\n",
      "Training in progress @ global_step 183850, g_loss 0.563977, d_loss 0.479554 accuracy 0.671875\n",
      "Training in progress @ global_step 183900, g_loss 0.565956, d_loss 0.479534 accuracy 0.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 183950, g_loss 0.56802, d_loss 0.488942 accuracy 0.640625\n",
      "Training in progress @ global_step 184000, g_loss 0.561429, d_loss 0.4939 accuracy 0.65625\n",
      "Training in progress @ global_step 184050, g_loss 0.563987, d_loss 0.469755 accuracy 0.78125\n",
      "Training in progress @ global_step 184100, g_loss 0.567398, d_loss 0.494409 accuracy 0.6875\n",
      "Training in progress @ global_step 184150, g_loss 0.561183, d_loss 0.482084 accuracy 0.71875\n",
      "Training in progress @ global_step 184200, g_loss 0.574489, d_loss 0.4684 accuracy 0.78125\n",
      "Training in progress @ global_step 184250, g_loss 0.566191, d_loss 0.482861 accuracy 0.703125\n",
      "Training in progress @ global_step 184300, g_loss 0.570333, d_loss 0.495544 accuracy 0.671875\n",
      "Training in progress @ global_step 184350, g_loss 0.567939, d_loss 0.490816 accuracy 0.625\n",
      "Training in progress @ global_step 184400, g_loss 0.5675, d_loss 0.474844 accuracy 0.71875\n",
      "Training in progress @ global_step 184450, g_loss 0.572301, d_loss 0.471264 accuracy 0.75\n",
      "Training in progress @ global_step 184500, g_loss 0.567826, d_loss 0.476285 accuracy 0.765625\n",
      "Training in progress @ global_step 184550, g_loss 0.569747, d_loss 0.485495 accuracy 0.6875\n",
      "Training in progress @ global_step 184600, g_loss 0.571926, d_loss 0.464211 accuracy 0.796875\n",
      "Training in progress @ global_step 184650, g_loss 0.564989, d_loss 0.4687 accuracy 0.71875\n",
      "Training in progress @ global_step 184700, g_loss 0.56899, d_loss 0.458784 accuracy 0.8125\n",
      "Training in progress @ global_step 184750, g_loss 0.567941, d_loss 0.472932 accuracy 0.6875\n",
      "Training in progress @ global_step 184800, g_loss 0.569062, d_loss 0.485385 accuracy 0.671875\n",
      "Training in progress @ global_step 184850, g_loss 0.561377, d_loss 0.480964 accuracy 0.6875\n",
      "Training in progress @ global_step 184900, g_loss 0.565665, d_loss 0.474752 accuracy 0.734375\n",
      "Training in progress @ global_step 184950, g_loss 0.566782, d_loss 0.473682 accuracy 0.75\n",
      "Training in progress @ global_step 185000, g_loss 0.564797, d_loss 0.48572 accuracy 0.671875\n",
      "Training in progress @ global_step 185050, g_loss 0.567209, d_loss 0.478235 accuracy 0.6875\n",
      "Training in progress @ global_step 185100, g_loss 0.56788, d_loss 0.502533 accuracy 0.625\n",
      "Training in progress @ global_step 185150, g_loss 0.56768, d_loss 0.494879 accuracy 0.640625\n",
      "Training in progress @ global_step 185200, g_loss 0.565561, d_loss 0.497394 accuracy 0.640625\n",
      "Training in progress @ global_step 185250, g_loss 0.567238, d_loss 0.485477 accuracy 0.75\n",
      "Training in progress @ global_step 185300, g_loss 0.5656, d_loss 0.502523 accuracy 0.640625\n",
      "Training in progress @ global_step 185350, g_loss 0.562525, d_loss 0.494035 accuracy 0.671875\n",
      "Training in progress @ global_step 185400, g_loss 0.564118, d_loss 0.496304 accuracy 0.65625\n",
      "Training in progress @ global_step 185450, g_loss 0.563271, d_loss 0.51541 accuracy 0.578125\n",
      "Training in progress @ global_step 185500, g_loss 0.567351, d_loss 0.511606 accuracy 0.578125\n",
      "Training in progress @ global_step 185550, g_loss 0.562746, d_loss 0.500596 accuracy 0.65625\n",
      "Training in progress @ global_step 185600, g_loss 0.565188, d_loss 0.510913 accuracy 0.625\n",
      "Training in progress @ global_step 185650, g_loss 0.562002, d_loss 0.509671 accuracy 0.609375\n",
      "Training in progress @ global_step 185700, g_loss 0.560413, d_loss 0.499746 accuracy 0.640625\n",
      "Training in progress @ global_step 185750, g_loss 0.562828, d_loss 0.516761 accuracy 0.5625\n",
      "Training in progress @ global_step 185800, g_loss 0.558993, d_loss 0.518925 accuracy 0.53125\n",
      "Training in progress @ global_step 185850, g_loss 0.562565, d_loss 0.508155 accuracy 0.65625\n",
      "Training in progress @ global_step 185900, g_loss 0.565044, d_loss 0.515089 accuracy 0.5625\n",
      "Training in progress @ global_step 185950, g_loss 0.569146, d_loss 0.514739 accuracy 0.59375\n",
      "Training in progress @ global_step 186000, g_loss 0.562897, d_loss 0.516913 accuracy 0.546875\n",
      "Training in progress @ global_step 186050, g_loss 0.563476, d_loss 0.508896 accuracy 0.578125\n",
      "Training in progress @ global_step 186100, g_loss 0.563827, d_loss 0.51799 accuracy 0.546875\n",
      "Training in progress @ global_step 186150, g_loss 0.565046, d_loss 0.520833 accuracy 0.578125\n",
      "Training in progress @ global_step 186200, g_loss 0.566097, d_loss 0.512685 accuracy 0.546875\n",
      "Training in progress @ global_step 186250, g_loss 0.563882, d_loss 0.51452 accuracy 0.578125\n",
      "Training in progress @ global_step 186300, g_loss 0.560181, d_loss 0.522983 accuracy 0.53125\n",
      "Training in progress @ global_step 186350, g_loss 0.562263, d_loss 0.51055 accuracy 0.625\n",
      "Training in progress @ global_step 186400, g_loss 0.563741, d_loss 0.508088 accuracy 0.609375\n",
      "Training in progress @ global_step 186450, g_loss 0.558392, d_loss 0.516549 accuracy 0.578125\n",
      "Training in progress @ global_step 186500, g_loss 0.562016, d_loss 0.516589 accuracy 0.578125\n",
      "Training in progress @ global_step 186550, g_loss 0.563678, d_loss 0.51378 accuracy 0.59375\n",
      "Training in progress @ global_step 186600, g_loss 0.560959, d_loss 0.519239 accuracy 0.5625\n",
      "Training in progress @ global_step 186650, g_loss 0.557854, d_loss 0.515982 accuracy 0.5625\n",
      "Training in progress @ global_step 186700, g_loss 0.559005, d_loss 0.510736 accuracy 0.59375\n",
      "Training in progress @ global_step 186750, g_loss 0.560877, d_loss 0.509439 accuracy 0.546875\n",
      "Training in progress @ global_step 186800, g_loss 0.558619, d_loss 0.495712 accuracy 0.640625\n",
      "Training in progress @ global_step 186850, g_loss 0.557618, d_loss 0.499071 accuracy 0.609375\n",
      "Training in progress @ global_step 186900, g_loss 0.556252, d_loss 0.503333 accuracy 0.609375\n",
      "Training in progress @ global_step 186950, g_loss 0.561995, d_loss 0.49918 accuracy 0.609375\n",
      "Training in progress @ global_step 187000, g_loss 0.556568, d_loss 0.488718 accuracy 0.65625\n",
      "Training in progress @ global_step 187050, g_loss 0.560738, d_loss 0.495928 accuracy 0.65625\n",
      "Training in progress @ global_step 187100, g_loss 0.555514, d_loss 0.49292 accuracy 0.65625\n",
      "Training in progress @ global_step 187150, g_loss 0.558045, d_loss 0.493127 accuracy 0.625\n",
      "Training in progress @ global_step 187200, g_loss 0.561976, d_loss 0.505161 accuracy 0.640625\n",
      "Training in progress @ global_step 187250, g_loss 0.561658, d_loss 0.501351 accuracy 0.671875\n",
      "Training in progress @ global_step 187300, g_loss 0.559014, d_loss 0.486015 accuracy 0.671875\n",
      "Training in progress @ global_step 187350, g_loss 0.559407, d_loss 0.491509 accuracy 0.640625\n",
      "Training in progress @ global_step 187400, g_loss 0.562635, d_loss 0.478253 accuracy 0.734375\n",
      "Training in progress @ global_step 187450, g_loss 0.559141, d_loss 0.481939 accuracy 0.671875\n",
      "Training in progress @ global_step 187500, g_loss 0.561072, d_loss 0.475537 accuracy 0.6875\n",
      "Training in progress @ global_step 187550, g_loss 0.569335, d_loss 0.483399 accuracy 0.703125\n",
      "Training in progress @ global_step 187600, g_loss 0.565089, d_loss 0.482598 accuracy 0.75\n",
      "Training in progress @ global_step 187650, g_loss 0.560968, d_loss 0.489882 accuracy 0.703125\n",
      "Training in progress @ global_step 187700, g_loss 0.560885, d_loss 0.491399 accuracy 0.65625\n",
      "Training in progress @ global_step 187750, g_loss 0.562873, d_loss 0.482714 accuracy 0.6875\n",
      "Training in progress @ global_step 187800, g_loss 0.565961, d_loss 0.479564 accuracy 0.734375\n",
      "Training in progress @ global_step 187850, g_loss 0.559271, d_loss 0.486044 accuracy 0.671875\n",
      "Training in progress @ global_step 187900, g_loss 0.564035, d_loss 0.483353 accuracy 0.703125\n",
      "Training in progress @ global_step 187950, g_loss 0.559757, d_loss 0.481746 accuracy 0.671875\n",
      "Training in progress @ global_step 188000, g_loss 0.560728, d_loss 0.490929 accuracy 0.6875\n",
      "Training in progress @ global_step 188050, g_loss 0.557012, d_loss 0.498539 accuracy 0.640625\n",
      "Training in progress @ global_step 188100, g_loss 0.566768, d_loss 0.477796 accuracy 0.75\n",
      "Training in progress @ global_step 188150, g_loss 0.560063, d_loss 0.508967 accuracy 0.640625\n",
      "Training in progress @ global_step 188200, g_loss 0.563001, d_loss 0.487364 accuracy 0.734375\n",
      "Training in progress @ global_step 188250, g_loss 0.573005, d_loss 0.492075 accuracy 0.640625\n",
      "Training in progress @ global_step 188300, g_loss 0.562216, d_loss 0.484752 accuracy 0.6875\n",
      "Training in progress @ global_step 188350, g_loss 0.560864, d_loss 0.484098 accuracy 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 188400, g_loss 0.559087, d_loss 0.499719 accuracy 0.625\n",
      "Training in progress @ global_step 188450, g_loss 0.566468, d_loss 0.492966 accuracy 0.6875\n",
      "Training in progress @ global_step 188500, g_loss 0.560527, d_loss 0.488644 accuracy 0.671875\n",
      "Training in progress @ global_step 188550, g_loss 0.552958, d_loss 0.492553 accuracy 0.703125\n",
      "Training in progress @ global_step 188600, g_loss 0.561929, d_loss 0.494818 accuracy 0.6875\n",
      "Training in progress @ global_step 188650, g_loss 0.561939, d_loss 0.487799 accuracy 0.671875\n",
      "Training in progress @ global_step 188700, g_loss 0.560437, d_loss 0.494717 accuracy 0.671875\n",
      "Training in progress @ global_step 188750, g_loss 0.557129, d_loss 0.491547 accuracy 0.671875\n",
      "Training in progress @ global_step 188800, g_loss 0.563307, d_loss 0.496372 accuracy 0.640625\n",
      "Training in progress @ global_step 188850, g_loss 0.562626, d_loss 0.491491 accuracy 0.6875\n",
      "Training in progress @ global_step 188900, g_loss 0.557876, d_loss 0.505054 accuracy 0.609375\n",
      "Training in progress @ global_step 188950, g_loss 0.555329, d_loss 0.499181 accuracy 0.640625\n",
      "Training in progress @ global_step 189000, g_loss 0.562591, d_loss 0.506953 accuracy 0.65625\n",
      "Training in progress @ global_step 189050, g_loss 0.554256, d_loss 0.53247 accuracy 0.546875\n",
      "Training in progress @ global_step 189100, g_loss 0.559968, d_loss 0.516859 accuracy 0.640625\n",
      "Training in progress @ global_step 189150, g_loss 0.553482, d_loss 0.511521 accuracy 0.625\n",
      "Training in progress @ global_step 189200, g_loss 0.561049, d_loss 0.535785 accuracy 0.5625\n",
      "Training in progress @ global_step 189250, g_loss 0.560122, d_loss 0.512503 accuracy 0.609375\n",
      "Training in progress @ global_step 189300, g_loss 0.565541, d_loss 0.517517 accuracy 0.609375\n",
      "Training in progress @ global_step 189350, g_loss 0.554483, d_loss 0.508606 accuracy 0.65625\n",
      "Training in progress @ global_step 189400, g_loss 0.552754, d_loss 0.500482 accuracy 0.609375\n",
      "Training in progress @ global_step 189450, g_loss 0.551227, d_loss 0.517016 accuracy 0.546875\n",
      "Training in progress @ global_step 189500, g_loss 0.560103, d_loss 0.530592 accuracy 0.546875\n",
      "Training in progress @ global_step 189550, g_loss 0.553362, d_loss 0.514704 accuracy 0.609375\n",
      "Training in progress @ global_step 189600, g_loss 0.54629, d_loss 0.517747 accuracy 0.625\n",
      "Training in progress @ global_step 189650, g_loss 0.553007, d_loss 0.525842 accuracy 0.59375\n",
      "Training in progress @ global_step 189700, g_loss 0.548554, d_loss 0.519476 accuracy 0.578125\n",
      "Training in progress @ global_step 189750, g_loss 0.548456, d_loss 0.527049 accuracy 0.53125\n",
      "Training in progress @ global_step 189800, g_loss 0.54741, d_loss 0.509951 accuracy 0.578125\n",
      "Training in progress @ global_step 189850, g_loss 0.54594, d_loss 0.507272 accuracy 0.59375\n",
      "Training in progress @ global_step 189900, g_loss 0.548591, d_loss 0.498855 accuracy 0.59375\n",
      "Training in progress @ global_step 189950, g_loss 0.548274, d_loss 0.500503 accuracy 0.59375\n",
      "Training in progress @ global_step 190000, g_loss 0.536982, d_loss 0.506284 accuracy 0.625\n",
      "Training in progress @ global_step 190050, g_loss 0.550357, d_loss 0.507798 accuracy 0.59375\n",
      "Training in progress @ global_step 190100, g_loss 0.542895, d_loss 0.508155 accuracy 0.578125\n",
      "Training in progress @ global_step 190150, g_loss 0.549435, d_loss 0.503645 accuracy 0.609375\n",
      "Training in progress @ global_step 190200, g_loss 0.549511, d_loss 0.499761 accuracy 0.625\n",
      "Training in progress @ global_step 190250, g_loss 0.539986, d_loss 0.500201 accuracy 0.625\n",
      "Training in progress @ global_step 190300, g_loss 0.543872, d_loss 0.485307 accuracy 0.671875\n",
      "Training in progress @ global_step 190350, g_loss 0.544884, d_loss 0.496499 accuracy 0.65625\n",
      "Training in progress @ global_step 190400, g_loss 0.543636, d_loss 0.500064 accuracy 0.609375\n",
      "Training in progress @ global_step 190450, g_loss 0.544206, d_loss 0.495997 accuracy 0.609375\n",
      "Training in progress @ global_step 190500, g_loss 0.543786, d_loss 0.49713 accuracy 0.625\n",
      "Training in progress @ global_step 190550, g_loss 0.547834, d_loss 0.495551 accuracy 0.65625\n",
      "Training in progress @ global_step 190600, g_loss 0.545319, d_loss 0.487685 accuracy 0.671875\n",
      "Training in progress @ global_step 190650, g_loss 0.54802, d_loss 0.483886 accuracy 0.71875\n",
      "Training in progress @ global_step 190700, g_loss 0.546152, d_loss 0.484836 accuracy 0.65625\n",
      "Training in progress @ global_step 190750, g_loss 0.544429, d_loss 0.476047 accuracy 0.765625\n",
      "Training in progress @ global_step 190800, g_loss 0.54236, d_loss 0.472018 accuracy 0.71875\n",
      "Training in progress @ global_step 190850, g_loss 0.551422, d_loss 0.479003 accuracy 0.75\n",
      "Training in progress @ global_step 190900, g_loss 0.545548, d_loss 0.480003 accuracy 0.6875\n",
      "Training in progress @ global_step 190950, g_loss 0.551432, d_loss 0.481965 accuracy 0.6875\n",
      "Training in progress @ global_step 191000, g_loss 0.536752, d_loss 0.48016 accuracy 0.6875\n",
      "Training in progress @ global_step 191050, g_loss 0.545685, d_loss 0.466567 accuracy 0.765625\n",
      "Training in progress @ global_step 191100, g_loss 0.549957, d_loss 0.482108 accuracy 0.65625\n",
      "Training in progress @ global_step 191150, g_loss 0.55246, d_loss 0.467099 accuracy 0.75\n",
      "Training in progress @ global_step 191200, g_loss 0.545195, d_loss 0.459044 accuracy 0.765625\n",
      "Training in progress @ global_step 191250, g_loss 0.555453, d_loss 0.46815 accuracy 0.75\n",
      "Training in progress @ global_step 191300, g_loss 0.543081, d_loss 0.464997 accuracy 0.828125\n",
      "Training in progress @ global_step 191350, g_loss 0.547649, d_loss 0.482234 accuracy 0.671875\n",
      "Training in progress @ global_step 191400, g_loss 0.54435, d_loss 0.469856 accuracy 0.765625\n",
      "Training in progress @ global_step 191450, g_loss 0.550599, d_loss 0.462955 accuracy 0.765625\n",
      "Training in progress @ global_step 191500, g_loss 0.556294, d_loss 0.452839 accuracy 0.8125\n",
      "Training in progress @ global_step 191550, g_loss 0.551324, d_loss 0.473685 accuracy 0.71875\n",
      "Training in progress @ global_step 191600, g_loss 0.554503, d_loss 0.448686 accuracy 0.859375\n",
      "Training in progress @ global_step 191650, g_loss 0.552464, d_loss 0.464959 accuracy 0.75\n",
      "Training in progress @ global_step 191700, g_loss 0.549889, d_loss 0.486928 accuracy 0.71875\n",
      "Training in progress @ global_step 191750, g_loss 0.549957, d_loss 0.456543 accuracy 0.8125\n",
      "Training in progress @ global_step 191800, g_loss 0.554139, d_loss 0.453543 accuracy 0.796875\n",
      "Training in progress @ global_step 191850, g_loss 0.559746, d_loss 0.476822 accuracy 0.71875\n",
      "Training in progress @ global_step 191900, g_loss 0.557049, d_loss 0.467231 accuracy 0.71875\n",
      "Training in progress @ global_step 191950, g_loss 0.554167, d_loss 0.454594 accuracy 0.8125\n",
      "Training in progress @ global_step 192000, g_loss 0.559603, d_loss 0.471029 accuracy 0.8125\n",
      "Training in progress @ global_step 192050, g_loss 0.556793, d_loss 0.466876 accuracy 0.765625\n",
      "Training in progress @ global_step 192100, g_loss 0.551811, d_loss 0.482446 accuracy 0.703125\n",
      "Training in progress @ global_step 192150, g_loss 0.55042, d_loss 0.474065 accuracy 0.765625\n",
      "Training in progress @ global_step 192200, g_loss 0.558159, d_loss 0.484944 accuracy 0.6875\n",
      "Training in progress @ global_step 192250, g_loss 0.554196, d_loss 0.488208 accuracy 0.640625\n",
      "Training in progress @ global_step 192300, g_loss 0.552647, d_loss 0.48769 accuracy 0.671875\n",
      "Training in progress @ global_step 192350, g_loss 0.555017, d_loss 0.473229 accuracy 0.796875\n",
      "Training in progress @ global_step 192400, g_loss 0.550243, d_loss 0.467579 accuracy 0.78125\n",
      "Training in progress @ global_step 192450, g_loss 0.550392, d_loss 0.493811 accuracy 0.65625\n",
      "Training in progress @ global_step 192500, g_loss 0.552959, d_loss 0.489625 accuracy 0.65625\n",
      "Training in progress @ global_step 192550, g_loss 0.5536, d_loss 0.465631 accuracy 0.75\n",
      "Training in progress @ global_step 192600, g_loss 0.543744, d_loss 0.503323 accuracy 0.640625\n",
      "Training in progress @ global_step 192650, g_loss 0.5477, d_loss 0.486904 accuracy 0.671875\n",
      "Training in progress @ global_step 192700, g_loss 0.544721, d_loss 0.511882 accuracy 0.59375\n",
      "Training in progress @ global_step 192750, g_loss 0.546196, d_loss 0.48587 accuracy 0.703125\n",
      "Training in progress @ global_step 192800, g_loss 0.54532, d_loss 0.490076 accuracy 0.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 192850, g_loss 0.550009, d_loss 0.502982 accuracy 0.625\n",
      "Training in progress @ global_step 192900, g_loss 0.544419, d_loss 0.497131 accuracy 0.671875\n",
      "Training in progress @ global_step 192950, g_loss 0.554202, d_loss 0.506191 accuracy 0.65625\n",
      "Training in progress @ global_step 193000, g_loss 0.546372, d_loss 0.502434 accuracy 0.65625\n",
      "Training in progress @ global_step 193050, g_loss 0.549282, d_loss 0.500626 accuracy 0.703125\n",
      "Training in progress @ global_step 193100, g_loss 0.546164, d_loss 0.520806 accuracy 0.59375\n",
      "Training in progress @ global_step 193150, g_loss 0.538963, d_loss 0.502168 accuracy 0.625\n",
      "Training in progress @ global_step 193200, g_loss 0.532182, d_loss 0.505623 accuracy 0.59375\n",
      "Training in progress @ global_step 193250, g_loss 0.546952, d_loss 0.50462 accuracy 0.59375\n",
      "Training in progress @ global_step 193300, g_loss 0.541761, d_loss 0.516052 accuracy 0.59375\n",
      "Training in progress @ global_step 193350, g_loss 0.537077, d_loss 0.495284 accuracy 0.65625\n",
      "Training in progress @ global_step 193400, g_loss 0.536791, d_loss 0.50997 accuracy 0.625\n",
      "Training in progress @ global_step 193450, g_loss 0.538332, d_loss 0.513575 accuracy 0.59375\n",
      "Training in progress @ global_step 193500, g_loss 0.539217, d_loss 0.500771 accuracy 0.609375\n",
      "Training in progress @ global_step 193550, g_loss 0.539373, d_loss 0.508482 accuracy 0.59375\n",
      "Training in progress @ global_step 193600, g_loss 0.528367, d_loss 0.513019 accuracy 0.609375\n",
      "Training in progress @ global_step 193650, g_loss 0.538753, d_loss 0.494358 accuracy 0.671875\n",
      "Training in progress @ global_step 193700, g_loss 0.535971, d_loss 0.506297 accuracy 0.609375\n",
      "Training in progress @ global_step 193750, g_loss 0.540279, d_loss 0.514062 accuracy 0.578125\n",
      "Training in progress @ global_step 193800, g_loss 0.539924, d_loss 0.511313 accuracy 0.59375\n",
      "Training in progress @ global_step 193850, g_loss 0.54178, d_loss 0.516331 accuracy 0.546875\n",
      "Training in progress @ global_step 193900, g_loss 0.54843, d_loss 0.507883 accuracy 0.609375\n",
      "Training in progress @ global_step 193950, g_loss 0.537138, d_loss 0.506351 accuracy 0.59375\n",
      "Training in progress @ global_step 194000, g_loss 0.537774, d_loss 0.509608 accuracy 0.609375\n",
      "Training in progress @ global_step 194050, g_loss 0.536081, d_loss 0.495575 accuracy 0.609375\n",
      "Training in progress @ global_step 194100, g_loss 0.541964, d_loss 0.500544 accuracy 0.609375\n",
      "Training in progress @ global_step 194150, g_loss 0.532935, d_loss 0.486915 accuracy 0.671875\n",
      "Training in progress @ global_step 194200, g_loss 0.543787, d_loss 0.496308 accuracy 0.59375\n",
      "Training in progress @ global_step 194250, g_loss 0.535854, d_loss 0.494776 accuracy 0.65625\n",
      "Training in progress @ global_step 194300, g_loss 0.534678, d_loss 0.496017 accuracy 0.640625\n",
      "Training in progress @ global_step 194350, g_loss 0.541209, d_loss 0.490176 accuracy 0.671875\n",
      "Training in progress @ global_step 194400, g_loss 0.54152, d_loss 0.493546 accuracy 0.640625\n",
      "Training in progress @ global_step 194450, g_loss 0.538156, d_loss 0.491857 accuracy 0.609375\n",
      "Training in progress @ global_step 194500, g_loss 0.538894, d_loss 0.469899 accuracy 0.75\n",
      "Training in progress @ global_step 194550, g_loss 0.543202, d_loss 0.477937 accuracy 0.71875\n",
      "Training in progress @ global_step 194600, g_loss 0.539738, d_loss 0.481309 accuracy 0.65625\n",
      "Training in progress @ global_step 194650, g_loss 0.534786, d_loss 0.476202 accuracy 0.734375\n",
      "Training in progress @ global_step 194700, g_loss 0.541606, d_loss 0.471626 accuracy 0.71875\n",
      "Training in progress @ global_step 194750, g_loss 0.536971, d_loss 0.467079 accuracy 0.734375\n",
      "Training in progress @ global_step 194800, g_loss 0.539558, d_loss 0.487617 accuracy 0.640625\n",
      "Training in progress @ global_step 194850, g_loss 0.544405, d_loss 0.476067 accuracy 0.671875\n",
      "Training in progress @ global_step 194900, g_loss 0.540796, d_loss 0.475925 accuracy 0.71875\n",
      "Training in progress @ global_step 194950, g_loss 0.547526, d_loss 0.473937 accuracy 0.71875\n",
      "Training in progress @ global_step 195000, g_loss 0.541618, d_loss 0.475996 accuracy 0.65625\n",
      "Training in progress @ global_step 195050, g_loss 0.548226, d_loss 0.464163 accuracy 0.75\n",
      "Training in progress @ global_step 195100, g_loss 0.54766, d_loss 0.487 accuracy 0.59375\n",
      "Training in progress @ global_step 195150, g_loss 0.54632, d_loss 0.477328 accuracy 0.6875\n",
      "Training in progress @ global_step 195200, g_loss 0.555773, d_loss 0.47996 accuracy 0.734375\n",
      "Training in progress @ global_step 195250, g_loss 0.549044, d_loss 0.455793 accuracy 0.796875\n",
      "Training in progress @ global_step 195300, g_loss 0.548274, d_loss 0.461464 accuracy 0.734375\n",
      "Training in progress @ global_step 195350, g_loss 0.550701, d_loss 0.467283 accuracy 0.75\n",
      "Training in progress @ global_step 195400, g_loss 0.551242, d_loss 0.470481 accuracy 0.671875\n",
      "Training in progress @ global_step 195450, g_loss 0.548727, d_loss 0.458122 accuracy 0.765625\n",
      "Training in progress @ global_step 195500, g_loss 0.550382, d_loss 0.446751 accuracy 0.796875\n",
      "Training in progress @ global_step 195550, g_loss 0.55341, d_loss 0.468031 accuracy 0.71875\n",
      "Training in progress @ global_step 195600, g_loss 0.552787, d_loss 0.464307 accuracy 0.734375\n",
      "Training in progress @ global_step 195650, g_loss 0.547702, d_loss 0.458973 accuracy 0.765625\n",
      "Training in progress @ global_step 195700, g_loss 0.551449, d_loss 0.475946 accuracy 0.6875\n",
      "Training in progress @ global_step 195750, g_loss 0.548466, d_loss 0.470814 accuracy 0.71875\n",
      "Training in progress @ global_step 195800, g_loss 0.550801, d_loss 0.471719 accuracy 0.75\n",
      "Training in progress @ global_step 195850, g_loss 0.555557, d_loss 0.457966 accuracy 0.75\n",
      "Training in progress @ global_step 195900, g_loss 0.559121, d_loss 0.46365 accuracy 0.75\n",
      "Training in progress @ global_step 195950, g_loss 0.560586, d_loss 0.453525 accuracy 0.8125\n",
      "Training in progress @ global_step 196000, g_loss 0.560094, d_loss 0.450972 accuracy 0.8125\n",
      "Training in progress @ global_step 196050, g_loss 0.556714, d_loss 0.471607 accuracy 0.765625\n",
      "Training in progress @ global_step 196100, g_loss 0.556035, d_loss 0.45627 accuracy 0.78125\n",
      "Training in progress @ global_step 196150, g_loss 0.560092, d_loss 0.459776 accuracy 0.8125\n",
      "Training in progress @ global_step 196200, g_loss 0.557647, d_loss 0.450879 accuracy 0.796875\n",
      "Training in progress @ global_step 196250, g_loss 0.550786, d_loss 0.475435 accuracy 0.734375\n",
      "Training in progress @ global_step 196300, g_loss 0.555212, d_loss 0.460402 accuracy 0.71875\n",
      "Training in progress @ global_step 196350, g_loss 0.5615, d_loss 0.464909 accuracy 0.734375\n",
      "Training in progress @ global_step 196400, g_loss 0.54401, d_loss 0.46479 accuracy 0.71875\n",
      "Training in progress @ global_step 196450, g_loss 0.558205, d_loss 0.47198 accuracy 0.75\n",
      "Training in progress @ global_step 196500, g_loss 0.552714, d_loss 0.456273 accuracy 0.8125\n",
      "Training in progress @ global_step 196550, g_loss 0.556635, d_loss 0.468187 accuracy 0.71875\n",
      "Training in progress @ global_step 196600, g_loss 0.55385, d_loss 0.476767 accuracy 0.71875\n",
      "Training in progress @ global_step 196650, g_loss 0.555435, d_loss 0.475531 accuracy 0.75\n",
      "Training in progress @ global_step 196700, g_loss 0.553783, d_loss 0.460341 accuracy 0.796875\n",
      "Training in progress @ global_step 196750, g_loss 0.555436, d_loss 0.483219 accuracy 0.671875\n",
      "Training in progress @ global_step 196800, g_loss 0.551057, d_loss 0.489286 accuracy 0.6875\n",
      "Training in progress @ global_step 196850, g_loss 0.553895, d_loss 0.469425 accuracy 0.75\n",
      "Training in progress @ global_step 196900, g_loss 0.54154, d_loss 0.464221 accuracy 0.75\n",
      "Training in progress @ global_step 196950, g_loss 0.54784, d_loss 0.485368 accuracy 0.65625\n",
      "Training in progress @ global_step 197000, g_loss 0.545219, d_loss 0.465917 accuracy 0.734375\n",
      "Training in progress @ global_step 197050, g_loss 0.551697, d_loss 0.468088 accuracy 0.765625\n",
      "Training in progress @ global_step 197100, g_loss 0.54161, d_loss 0.481043 accuracy 0.6875\n",
      "Training in progress @ global_step 197150, g_loss 0.536681, d_loss 0.471201 accuracy 0.703125\n",
      "Training in progress @ global_step 197200, g_loss 0.539209, d_loss 0.496223 accuracy 0.6875\n",
      "Training in progress @ global_step 197250, g_loss 0.550854, d_loss 0.502345 accuracy 0.65625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 197300, g_loss 0.54759, d_loss 0.498986 accuracy 0.625\n",
      "Training in progress @ global_step 197350, g_loss 0.549878, d_loss 0.489479 accuracy 0.6875\n",
      "Training in progress @ global_step 197400, g_loss 0.5532, d_loss 0.500989 accuracy 0.671875\n",
      "Training in progress @ global_step 197450, g_loss 0.547268, d_loss 0.497349 accuracy 0.65625\n",
      "Training in progress @ global_step 197500, g_loss 0.547418, d_loss 0.507914 accuracy 0.609375\n",
      "Training in progress @ global_step 197550, g_loss 0.54471, d_loss 0.503714 accuracy 0.65625\n",
      "Training in progress @ global_step 197600, g_loss 0.549772, d_loss 0.480244 accuracy 0.71875\n",
      "Training in progress @ global_step 197650, g_loss 0.54658, d_loss 0.47915 accuracy 0.734375\n",
      "Training in progress @ global_step 197700, g_loss 0.540447, d_loss 0.50085 accuracy 0.65625\n",
      "Training in progress @ global_step 197750, g_loss 0.548926, d_loss 0.508176 accuracy 0.625\n",
      "Training in progress @ global_step 197800, g_loss 0.543444, d_loss 0.489451 accuracy 0.703125\n",
      "Training in progress @ global_step 197850, g_loss 0.543248, d_loss 0.483862 accuracy 0.6875\n",
      "Training in progress @ global_step 197900, g_loss 0.544762, d_loss 0.491325 accuracy 0.671875\n",
      "Training in progress @ global_step 197950, g_loss 0.544722, d_loss 0.501371 accuracy 0.640625\n",
      "Training in progress @ global_step 198000, g_loss 0.54429, d_loss 0.515874 accuracy 0.609375\n",
      "Training in progress @ global_step 198050, g_loss 0.542571, d_loss 0.482327 accuracy 0.703125\n",
      "Training in progress @ global_step 198100, g_loss 0.540789, d_loss 0.485006 accuracy 0.71875\n",
      "Training in progress @ global_step 198150, g_loss 0.537113, d_loss 0.513978 accuracy 0.59375\n",
      "Training in progress @ global_step 198200, g_loss 0.53933, d_loss 0.488239 accuracy 0.625\n",
      "Training in progress @ global_step 198250, g_loss 0.535413, d_loss 0.504568 accuracy 0.640625\n",
      "Training in progress @ global_step 198300, g_loss 0.528781, d_loss 0.511882 accuracy 0.578125\n",
      "Training in progress @ global_step 198350, g_loss 0.540756, d_loss 0.488309 accuracy 0.703125\n",
      "Training in progress @ global_step 198400, g_loss 0.538461, d_loss 0.501488 accuracy 0.625\n",
      "Training in progress @ global_step 198450, g_loss 0.532656, d_loss 0.503252 accuracy 0.578125\n",
      "Training in progress @ global_step 198500, g_loss 0.537565, d_loss 0.494189 accuracy 0.625\n",
      "Training in progress @ global_step 198550, g_loss 0.534392, d_loss 0.493042 accuracy 0.671875\n",
      "Training in progress @ global_step 198600, g_loss 0.542706, d_loss 0.483113 accuracy 0.671875\n",
      "Training in progress @ global_step 198650, g_loss 0.53815, d_loss 0.483337 accuracy 0.703125\n",
      "Training in progress @ global_step 198700, g_loss 0.535579, d_loss 0.491182 accuracy 0.640625\n",
      "Training in progress @ global_step 198750, g_loss 0.536001, d_loss 0.484031 accuracy 0.65625\n",
      "Training in progress @ global_step 198800, g_loss 0.526081, d_loss 0.480768 accuracy 0.65625\n",
      "Training in progress @ global_step 198850, g_loss 0.535226, d_loss 0.478734 accuracy 0.71875\n",
      "Training in progress @ global_step 198900, g_loss 0.54689, d_loss 0.487912 accuracy 0.734375\n",
      "Training in progress @ global_step 198950, g_loss 0.534987, d_loss 0.48416 accuracy 0.6875\n",
      "Training in progress @ global_step 199000, g_loss 0.528477, d_loss 0.479792 accuracy 0.625\n",
      "Training in progress @ global_step 199050, g_loss 0.540437, d_loss 0.482529 accuracy 0.671875\n",
      "Training in progress @ global_step 199100, g_loss 0.538645, d_loss 0.492748 accuracy 0.640625\n",
      "Training in progress @ global_step 199150, g_loss 0.54072, d_loss 0.475387 accuracy 0.71875\n",
      "Training in progress @ global_step 199200, g_loss 0.538394, d_loss 0.442487 accuracy 0.859375\n",
      "Training in progress @ global_step 199250, g_loss 0.537858, d_loss 0.45845 accuracy 0.75\n",
      "Training in progress @ global_step 199300, g_loss 0.529681, d_loss 0.481733 accuracy 0.65625\n",
      "Training in progress @ global_step 199350, g_loss 0.542772, d_loss 0.465868 accuracy 0.71875\n",
      "Training in progress @ global_step 199400, g_loss 0.541592, d_loss 0.457572 accuracy 0.78125\n",
      "Training in progress @ global_step 199450, g_loss 0.549902, d_loss 0.465409 accuracy 0.765625\n",
      "Training in progress @ global_step 199500, g_loss 0.533028, d_loss 0.427497 accuracy 0.9375\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        if not RESTORE:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer.add_graph(sess.graph)\n",
    "            saver = tf.train.Saver()\n",
    "        else: \n",
    "            latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "            print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        print \"Begin training ...\"\n",
    "        # Run training loop\n",
    "        for i in xrange(50000):\n",
    "            step = sess.run(global_step)\n",
    "\n",
    "            # Receive data (this will hang if IO thread is still running = this\n",
    "            # will wait for thread to finish & receive data)\n",
    "\n",
    "            sigma = max(0.5*(40000. - step) / (40000), 0.01)\n",
    "\n",
    "            # Update the generator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "\n",
    "            real_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "            fake_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "\n",
    "\n",
    "            [  acc_fake, _ ] = sess.run(\n",
    "                [accuracy_fake, \n",
    "                 generator_optimizer], \n",
    "                feed_dict = {noise_tensor: fake_input,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "            # Update the discriminator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "            [generated_mnist, _] = sess.run([fake_images, \n",
    "                                            discriminator_optimizer], \n",
    "                                            feed_dict = {noise_tensor : fake_input,\n",
    "                                                         real_flat : real_data,\n",
    "                                                         real_noise: real_noise_addition,\n",
    "                                                         fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            [summary, g_l, acc_fake, d_l_r, acc] = sess.run(\n",
    "                [merged_summary, g_loss, accuracy_fake,\n",
    "                 d_loss_real, total_accuracy],\n",
    "                feed_dict = {noise_tensor : fake,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "            if step != 0 and step % 500 == 0:\n",
    "                saver.save(\n",
    "                    sess,\n",
    "                    LOGDIR+\"/checkpoints/save\",\n",
    "                    global_step=step)\n",
    "\n",
    "\n",
    "            # train_writer.add_summary(summary, i)\n",
    "            # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "            if step % 50 == 0:\n",
    "                print 'Training in progress @ global_step %d, g_loss %g, d_loss %g accuracy %g' % (step, g_l, d_l_r, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, let's load this network back into memory and generate a few fake images for visualization.  As you'll see, this network does \"OK\" but not amazingly well.  In the next post, we'll see a deep convolutional network that does much better at generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model from ./mnist_gan_logs/lr_1e-06_neurons_1024/checkpoints/save-199500\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_gan_logs/lr_1e-06_neurons_1024/checkpoints/save-199500\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "        print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "        # We only need to make fake data and run it through the 'fake_images' tensor to see the output:\n",
    "        \n",
    "        fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "\n",
    "        [generated_images] = sess.run(\n",
    "                [fake_images], \n",
    "                feed_dict = {noise_tensor: fake_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to make it easier to draw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_images = numpy.reshape(generated_images, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJKCAYAAAA84QGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e88046d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(generate_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well ... It's OK.  It's very obviously not a digit, but it looks a bit like it *could* be a digit.  If you train this network on specific digits (1, 2,etc) I'm sure you will get much better performance.  Why not try it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
