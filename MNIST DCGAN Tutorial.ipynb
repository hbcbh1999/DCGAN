{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't looked at the previous tutorial, definitely check that out first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll extend the previous work to use more sophisticated networks.  In particular, I'll make the generator a deep convolutional network, and leave the discriminator as a shallow fully connected network.  This ought to produce much better digits that the previous fully connected networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use the mnist data set here.  See the previous tutorial for some mnist basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, you can specify which device to use.  The next cell will tell you what's available, and you can select from there.  By default, I select \"/gpu:0\" but you can change this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3843090529246354084\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11990623847\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9724674830396321\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()\n",
    "default_device = \"/gpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = mnist.train.next_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE=100 # Keep this even\n",
    "N_INITIAL_FILTERS=1024\n",
    "LOGDIR=\"./mnist_dcgan_logs/filters_{}_lr_{}\".format(N_INITIAL_FILTERS, BASE_LEARNING_RATE)\n",
    "RESTORE=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real images and random noise, so make a placeholder for both.  Additionally, based on this blog post (http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/) I add random gaussian noise to the real and fake images as they are fed to the discriminator to help stabalize training.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        # Input noise to the generator:\n",
    "        noise_tensor = tf.placeholder(tf.float32, [None, 10*10], name=\"noise\")\n",
    "#         fake_input   = tf.reshape(noise_tensor, (tf.shape(noise_tensor)[0], 10,10, 1))\n",
    "        fake_input   = noise_tensor\n",
    "    \n",
    "\n",
    "        # Placeholder for the discriminator input:\n",
    "        real_flat  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "        real_images  = tf.reshape(real_flat, (tf.shape(real_flat)[0], 28, 28, 1))\n",
    "\n",
    "        # We augment the input to the discriminator with gaussian noise\n",
    "        # This makes it harder for the discriminator to do it's job, preventing\n",
    "        # it from always \"winning\" the GAN min/max contest\n",
    "        real_noise = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"real_noise\")\n",
    "        fake_noise = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"fake_noise\")\n",
    "\n",
    "        real_images = real_noise + real_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input tensors (noise_tensor, real_images) are shaped in the 'flattened' way: (N/2, 100) for noise, (N/2, 784) for real images.  This lets me input the mnist images directly to tensorflow, as well as the noise.  They are then reshaped to be like tensorflow images (Batch, H, W, Filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to build the discriminator, using fully connected networks.  Note that a convolutional layer with the stride equal to the image size *is* a fully connected layer.\n",
    "\n",
    "This is unchanged from the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor, reuse):\n",
    "    # Use scoping to keep the variables nicely organized in the graph.\n",
    "    # Scoping is good practice always, but it's *essential* here as we'll see later on\n",
    "    with tf.variable_scope(\"mnist_discriminator\", reuse=reuse):\n",
    "\n",
    "        # Downsample with strided convolutions, use a few layers of convolutions:\n",
    "        x = tf.layers.conv2d(input_tensor,\n",
    "                             32,\n",
    "                             kernel_size = [5,5],\n",
    "                             strides=(1, 1),\n",
    "                             padding='valid',)\n",
    "        \n",
    "        # Batch norm, relu:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                          training=True,\n",
    "                                          trainable=True,\n",
    "                                          name=\"batch_norm_1\")\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Downsample convolution with stride 2:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             64,\n",
    "                             kernel_size=[3,3],\n",
    "                             strides=[2,2],\n",
    "                             padding='valid',\n",
    "                             name=\"conv2d_downsample_1\")\n",
    "        \n",
    "        # Batch norm, relu:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                          training=True,\n",
    "                                          trainable=True,\n",
    "                                          name=\"batch_norm_2\")\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        # Downsample convolution with stride 2:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             128,\n",
    "                             kernel_size=[3,3],\n",
    "                             strides=[2,2],\n",
    "                             padding='valid',\n",
    "                             name=\"conv2d_downsample_2\")\n",
    "        \n",
    "        \n",
    "        # Apply a 1x1 convolutoin and do global average pooling:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                          training=True,\n",
    "                                          trainable=True,\n",
    "                                          name=\"batch_norm_3\")\n",
    "        \n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             1,\n",
    "                             kernel_size=[1,1],\n",
    "                             strides=[1,1],\n",
    "                             padding='valid',\n",
    "                             name=\"conv2d1x1_downsample\")\n",
    "        \n",
    "                                          \n",
    "        # Pooling operation:\n",
    "        x = tf.layers.average_pooling2d(x,\n",
    "                                        pool_size=[5,5],\n",
    "                                        strides=[1,1],\n",
    "                                        name ='final_pooling')\n",
    "        \n",
    "        \n",
    "            \n",
    "        # Since we want to predict \"real\" or \"fake\", an output of 0 or 1 is desired.  sigmoid is perfect for this:\n",
    "        x = tf.nn.sigmoid(x, name=\"discriminator_sigmoid\")\n",
    "        #Reshape this to bring it down to just one output per image:\n",
    "        x = tf.reshape(x, (-1,))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        real_image_logits = build_discriminator(real_images, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "print real_image_logits.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to generate random images from noise:\n",
    "\n",
    "This function has been transformed into a deeper convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_tensor, n_initial_filters=512):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_generator\"):\n",
    "\n",
    "        # Map the input vector to the first convolutional space:\n",
    "        # It's 10x10x1, we want to get to 4x4x512\n",
    "        x = tf.layers.dense(input_tensor, 4*4*n_initial_filters, name=\"project\")\n",
    "        print x.get_shape()\n",
    "        x = tf.reshape(x, (-1, 4, 4, n_initial_filters), name=\"reshape\")\n",
    "        \n",
    "        print x.get_shape()\n",
    "        \n",
    "        # Apply batch norm, relu, and upsampling:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                center=False,\n",
    "                                scale=False,\n",
    "                                training=True,\n",
    "                                trainable=True,\n",
    "                                name=\"batch_norm_1\")\n",
    "        \n",
    "        x = tf.nn.relu(x, name=\"relu_1\")\n",
    "\n",
    "        # Upsample with a transposed convolution:\n",
    "        x = tf.layers.conv2d_transpose(x,\n",
    "                                       int(0.5*n_initial_filters),\n",
    "                                       kernel_size=[2,2],\n",
    "                                       strides=(2, 2),\n",
    "                                       name=\"conv2d_transpose_1\")\n",
    "\n",
    "        # Apply batch norm, relu, and upsampling again:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                center=False,\n",
    "                                scale=False,\n",
    "                                training=True,\n",
    "                                trainable=True,\n",
    "                                name=\"batch_norm_2\")\n",
    "        \n",
    "        x = tf.nn.relu(x, name=\"relu_2\")\n",
    "        \n",
    "        # Upsample with a transposed convolution:\n",
    "        x = tf.layers.conv2d_transpose(x,\n",
    "                                       int(0.25*n_initial_filters),\n",
    "                                       kernel_size=[2,2],\n",
    "                                       strides=(2, 2),\n",
    "                                       name=\"conv2d_transpose_2\")\n",
    "                                    \n",
    "        \n",
    "        # Apply batch norm, relu, and upsampling again:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                center=False,\n",
    "                                scale=False,\n",
    "                                training=True,\n",
    "                                trainable=True,\n",
    "                                name=\"batch_norm_3\")\n",
    "        \n",
    "        x = tf.nn.relu(x, name=\"relu_3\")\n",
    "                           \n",
    "        # Upsample with a transposed convolution:\n",
    "        x = tf.layers.conv2d_transpose(x,\n",
    "                                       int(0.125*n_initial_filters),\n",
    "                                       kernel_size=[2,2],\n",
    "                                       strides=(2, 2),\n",
    "                                       name=\"conv2d_transpose_3\")\n",
    "\n",
    "        # Apply batch norm, relu, and upsampling again:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                center=False,\n",
    "                                scale=False,\n",
    "                                training=True,\n",
    "                                trainable=True,\n",
    "                                name=\"batch_norm_4\")\n",
    "        \n",
    "        x = tf.nn.relu(x, name=\"relu_4\")\n",
    "        \n",
    "        # Map the space onto a 28x28x1 region:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             1,\n",
    "                             kernel_size=[5,5],\n",
    "                             strides=[1,1],\n",
    "                             name=\"final_conv2d\")\n",
    "        \n",
    "         \n",
    "        # The final non linearity applied here is to map the images onto the [-1,1] range.\n",
    "        x = tf.nn.tanh(x, name=\"generator_tanh\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16384)\n",
      "(?, 4, 4, 1024)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_images = build_generator(fake_input, n_initial_filters=N_INITIAL_FILTERS) + fake_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n",
      "(?, 100)\n"
     ]
    }
   ],
   "source": [
    "print fake_noise.get_shape()\n",
    "print fake_input.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to run the discriminator on the fake images, so set that up too.  Since it trains on both real and fake images, set reuse=True here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_image_logits = build_discriminator(fake_images, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the generator and discriminator seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    # Build the loss functions:\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "            #Discriminator loss on real images (classify as 1):\n",
    "            d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_image_logits,\n",
    "                labels = tf.ones_like(real_image_logits)))\n",
    "            #Discriminator loss on fake images (classify as 0):\n",
    "            d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "                labels = tf.zeros_like(fake_image_logits)))\n",
    "\n",
    "            # Total discriminator loss is the sum:\n",
    "            d_loss_total = d_loss_real + d_loss_fake\n",
    "\n",
    "            # This is the adverserial step: g_loss tries to optimize fake_logits to one,\n",
    "            # While d_loss_fake tries to optimize fake_logits to zero.\n",
    "#             g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_image_logits,\n",
    "#                 labels = tf.ones_like(fake_image_logits)))\n",
    "            g_loss = -tf.reduce_mean(tf.log(fake_image_logits))\n",
    "\n",
    "            # This code is useful if you'll use tensorboard to monitor training:\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Fake_Loss\", d_loss_fake)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss_total)\n",
    "            d_loss_summary = tf.summary.scalar(\"Generator_Loss\", g_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"accuracy\") as scope:\n",
    "            # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "            accuracy_real  = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_logits), \n",
    "                                                             tf.ones_like(real_image_logits)), \n",
    "                                                    tf.float32))\n",
    "            accuracy_fake  = tf.reduce_mean(tf.cast(tf.equal(tf.round(fake_image_logits), \n",
    "                                                             tf.zeros_like(fake_image_logits)), \n",
    "                                                    tf.float32))\n",
    "\n",
    "            total_accuracy = 0.5*(accuracy_fake +  accuracy_real)\n",
    "\n",
    "            # Again, useful for tensorboard:\n",
    "            acc_real_summary = tf.summary.scalar(\"Real_Accuracy\", accuracy_real)\n",
    "            acc_real_summary = tf.summary.scalar(\"Fake_Accuracy\", accuracy_fake)\n",
    "            acc_real_summary = tf.summary.scalar(\"Total_Accuracy\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            # Global steps are useful for restoring training:\n",
    "            global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            # Make sure the optimizers are only operating on their own variables:\n",
    "\n",
    "            all_variables      = tf.trainable_variables()\n",
    "            discriminator_vars = [v for v in all_variables if v.name.startswith('mnist_discriminator/')]\n",
    "            generator_vars     = [v for v in all_variables if v.name.startswith('mnist_generator/')]\n",
    "\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.9).minimize(\n",
    "                d_loss_total, global_step=global_step, var_list=discriminator_vars)\n",
    "            generator_optimizer     = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.9).minimize(\n",
    "                g_loss, global_step=global_step, var_list=generator_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        tf.summary.image('fake_images', fake_images, max_outputs=4)\n",
    "        tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Set up a saver:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ...\n",
      "Training in progress @ global_step 0, g_loss 0.488188, d_loss 0.433953 accuracy 0.5\n",
      "Training in progress @ global_step 50, g_loss 0.511086, d_loss 0.435609 accuracy 0.5\n",
      "Training in progress @ global_step 100, g_loss 0.549457, d_loss 0.437171 accuracy 0.5\n",
      "Training in progress @ global_step 150, g_loss 0.595206, d_loss 0.439455 accuracy 0.51\n",
      "Training in progress @ global_step 200, g_loss 0.654245, d_loss 0.442045 accuracy 0.54\n",
      "Training in progress @ global_step 250, g_loss 0.68026, d_loss 0.445601 accuracy 0.67\n",
      "Training in progress @ global_step 300, g_loss 0.716101, d_loss 0.445167 accuracy 0.76\n",
      "Training in progress @ global_step 350, g_loss 0.767672, d_loss 0.446099 accuracy 0.82\n",
      "Training in progress @ global_step 400, g_loss 0.822476, d_loss 0.44813 accuracy 0.9\n",
      "Training in progress @ global_step 450, g_loss 0.847979, d_loss 0.448853 accuracy 0.9\n",
      "Training in progress @ global_step 500, g_loss 0.862847, d_loss 0.451491 accuracy 0.89\n",
      "Training in progress @ global_step 550, g_loss 0.906384, d_loss 0.450299 accuracy 0.88\n",
      "Training in progress @ global_step 600, g_loss 0.945144, d_loss 0.453909 accuracy 0.9\n",
      "Training in progress @ global_step 650, g_loss 0.986665, d_loss 0.45203 accuracy 0.89\n",
      "Training in progress @ global_step 700, g_loss 1.02789, d_loss 0.451027 accuracy 0.89\n",
      "Training in progress @ global_step 750, g_loss 1.06758, d_loss 0.458048 accuracy 0.85\n",
      "Training in progress @ global_step 800, g_loss 1.10557, d_loss 0.454948 accuracy 0.87\n",
      "Training in progress @ global_step 850, g_loss 1.13843, d_loss 0.46172 accuracy 0.8\n",
      "Training in progress @ global_step 900, g_loss 1.17718, d_loss 0.459518 accuracy 0.83\n",
      "Training in progress @ global_step 950, g_loss 1.21262, d_loss 0.46183 accuracy 0.82\n",
      "Training in progress @ global_step 1000, g_loss 1.24238, d_loss 0.460036 accuracy 0.83\n",
      "Training in progress @ global_step 1050, g_loss 1.27001, d_loss 0.456543 accuracy 0.86\n",
      "Training in progress @ global_step 1100, g_loss 1.30418, d_loss 0.458544 accuracy 0.86\n",
      "Training in progress @ global_step 1150, g_loss 1.33182, d_loss 0.467048 accuracy 0.83\n",
      "Training in progress @ global_step 1200, g_loss 1.35408, d_loss 0.458145 accuracy 0.85\n",
      "Training in progress @ global_step 1250, g_loss 1.38358, d_loss 0.460201 accuracy 0.87\n",
      "Training in progress @ global_step 1300, g_loss 1.40824, d_loss 0.462351 accuracy 0.83\n",
      "Training in progress @ global_step 1350, g_loss 1.4372, d_loss 0.456753 accuracy 0.89\n",
      "Training in progress @ global_step 1400, g_loss 1.44612, d_loss 0.467094 accuracy 0.81\n",
      "Training in progress @ global_step 1450, g_loss 1.47186, d_loss 0.46413 accuracy 0.81\n",
      "Training in progress @ global_step 1500, g_loss 1.4961, d_loss 0.46409 accuracy 0.78\n",
      "Training in progress @ global_step 1550, g_loss 1.50118, d_loss 0.46353 accuracy 0.82\n",
      "Training in progress @ global_step 1600, g_loss 1.52399, d_loss 0.460275 accuracy 0.85\n",
      "Training in progress @ global_step 1650, g_loss 1.54055, d_loss 0.459508 accuracy 0.84\n",
      "Training in progress @ global_step 1700, g_loss 1.55384, d_loss 0.461654 accuracy 0.81\n",
      "Training in progress @ global_step 1750, g_loss 1.57629, d_loss 0.461176 accuracy 0.83\n",
      "Training in progress @ global_step 1800, g_loss 1.58776, d_loss 0.4552 accuracy 0.88\n",
      "Training in progress @ global_step 1850, g_loss 1.59808, d_loss 0.45767 accuracy 0.83\n",
      "Training in progress @ global_step 1900, g_loss 1.59936, d_loss 0.451107 accuracy 0.88\n",
      "Training in progress @ global_step 1950, g_loss 1.61841, d_loss 0.453099 accuracy 0.84\n",
      "Training in progress @ global_step 2000, g_loss 1.62054, d_loss 0.44338 accuracy 0.91\n",
      "Training in progress @ global_step 2050, g_loss 1.63567, d_loss 0.445972 accuracy 0.89\n",
      "Training in progress @ global_step 2100, g_loss 1.64722, d_loss 0.44225 accuracy 0.94\n",
      "Training in progress @ global_step 2150, g_loss 1.66246, d_loss 0.444484 accuracy 0.9\n",
      "Training in progress @ global_step 2200, g_loss 1.6709, d_loss 0.440625 accuracy 0.93\n",
      "Training in progress @ global_step 2250, g_loss 1.65979, d_loss 0.443918 accuracy 0.92\n",
      "Training in progress @ global_step 2300, g_loss 1.66037, d_loss 0.438361 accuracy 0.94\n",
      "Training in progress @ global_step 2350, g_loss 1.68424, d_loss 0.441002 accuracy 0.92\n",
      "Training in progress @ global_step 2400, g_loss 1.63784, d_loss 0.438384 accuracy 0.93\n",
      "Training in progress @ global_step 2450, g_loss 1.746, d_loss 0.435534 accuracy 0.93\n",
      "Training in progress @ global_step 2500, g_loss 1.71488, d_loss 0.436665 accuracy 0.92\n",
      "Training in progress @ global_step 2550, g_loss 1.74294, d_loss 0.433251 accuracy 0.92\n",
      "Training in progress @ global_step 2600, g_loss 1.75468, d_loss 0.439043 accuracy 0.94\n",
      "Training in progress @ global_step 2650, g_loss 1.8716, d_loss 0.435544 accuracy 0.9\n",
      "Training in progress @ global_step 2700, g_loss 1.9298, d_loss 0.43331 accuracy 0.92\n",
      "Training in progress @ global_step 2750, g_loss 1.62899, d_loss 0.435795 accuracy 0.93\n",
      "Training in progress @ global_step 2800, g_loss 1.6769, d_loss 0.431645 accuracy 0.94\n",
      "Training in progress @ global_step 2850, g_loss 1.70868, d_loss 0.440461 accuracy 0.93\n",
      "Training in progress @ global_step 2900, g_loss 1.71191, d_loss 0.428684 accuracy 0.91\n",
      "Training in progress @ global_step 2950, g_loss 1.72818, d_loss 0.429948 accuracy 0.97\n",
      "Training in progress @ global_step 3000, g_loss 1.75153, d_loss 0.432403 accuracy 0.91\n",
      "Training in progress @ global_step 3050, g_loss 1.75685, d_loss 0.433134 accuracy 0.95\n",
      "Training in progress @ global_step 3100, g_loss 1.783, d_loss 0.425357 accuracy 0.96\n",
      "Training in progress @ global_step 3150, g_loss 1.78055, d_loss 0.420752 accuracy 0.99\n",
      "Training in progress @ global_step 3200, g_loss 1.80082, d_loss 0.422907 accuracy 0.96\n",
      "Training in progress @ global_step 3250, g_loss 1.81599, d_loss 0.426156 accuracy 0.97\n",
      "Training in progress @ global_step 3300, g_loss 1.82247, d_loss 0.418425 accuracy 0.98\n",
      "Training in progress @ global_step 3350, g_loss 1.85233, d_loss 0.417966 accuracy 0.96\n",
      "Training in progress @ global_step 3400, g_loss 1.83548, d_loss 0.419629 accuracy 0.98\n",
      "Training in progress @ global_step 3450, g_loss 1.85941, d_loss 0.417985 accuracy 0.94\n",
      "Training in progress @ global_step 3500, g_loss 1.86162, d_loss 0.418179 accuracy 0.99\n",
      "Training in progress @ global_step 3550, g_loss 1.88883, d_loss 0.411857 accuracy 0.96\n",
      "Training in progress @ global_step 3600, g_loss 1.89126, d_loss 0.414744 accuracy 0.99\n",
      "Training in progress @ global_step 3650, g_loss 1.91846, d_loss 0.411877 accuracy 0.94\n",
      "Training in progress @ global_step 3700, g_loss 1.916, d_loss 0.405896 accuracy 0.99\n",
      "Training in progress @ global_step 3750, g_loss 1.96131, d_loss 0.403161 accuracy 0.98\n",
      "Training in progress @ global_step 3800, g_loss 1.94465, d_loss 0.40262 accuracy 0.99\n",
      "Training in progress @ global_step 3850, g_loss 1.97504, d_loss 0.400811 accuracy 0.99\n",
      "Training in progress @ global_step 3900, g_loss 1.98388, d_loss 0.404098 accuracy 0.98\n",
      "Training in progress @ global_step 3950, g_loss 2.0184, d_loss 0.395648 accuracy 0.98\n",
      "Training in progress @ global_step 4000, g_loss 2.05605, d_loss 0.394527 accuracy 0.99\n",
      "Training in progress @ global_step 4050, g_loss 2.05117, d_loss 0.396519 accuracy 1\n",
      "Training in progress @ global_step 4100, g_loss 2.06167, d_loss 0.388048 accuracy 1\n",
      "Training in progress @ global_step 4150, g_loss 2.08603, d_loss 0.387125 accuracy 0.99\n",
      "Training in progress @ global_step 4200, g_loss 2.1346, d_loss 0.391381 accuracy 0.97\n",
      "Training in progress @ global_step 4250, g_loss 2.09614, d_loss 0.391667 accuracy 0.97\n",
      "Training in progress @ global_step 4300, g_loss 2.13997, d_loss 0.387243 accuracy 0.99\n",
      "Training in progress @ global_step 4350, g_loss 2.18169, d_loss 0.385662 accuracy 0.99\n",
      "Training in progress @ global_step 4400, g_loss 2.19703, d_loss 0.38132 accuracy 0.99\n",
      "Training in progress @ global_step 4450, g_loss 2.21137, d_loss 0.379863 accuracy 0.99\n",
      "Training in progress @ global_step 4500, g_loss 2.20346, d_loss 0.375783 accuracy 1\n",
      "Training in progress @ global_step 4550, g_loss 2.24885, d_loss 0.3767 accuracy 1\n",
      "Training in progress @ global_step 4600, g_loss 2.23459, d_loss 0.382624 accuracy 0.99\n",
      "Training in progress @ global_step 4650, g_loss 2.29901, d_loss 0.370961 accuracy 1\n",
      "Training in progress @ global_step 4700, g_loss 2.31232, d_loss 0.370846 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 4750, g_loss 2.32777, d_loss 0.369125 accuracy 1\n",
      "Training in progress @ global_step 4800, g_loss 2.33457, d_loss 0.365469 accuracy 1\n",
      "Training in progress @ global_step 4850, g_loss 2.39858, d_loss 0.365368 accuracy 1\n",
      "Training in progress @ global_step 4900, g_loss 2.43737, d_loss 0.36984 accuracy 1\n",
      "Training in progress @ global_step 4950, g_loss 2.47639, d_loss 0.368772 accuracy 1\n",
      "Training in progress @ global_step 5000, g_loss 2.48535, d_loss 0.366425 accuracy 1\n",
      "Training in progress @ global_step 5050, g_loss 2.39527, d_loss 0.361025 accuracy 1\n",
      "Training in progress @ global_step 5100, g_loss 2.46425, d_loss 0.362443 accuracy 1\n",
      "Training in progress @ global_step 5150, g_loss 2.44796, d_loss 0.353455 accuracy 1\n",
      "Training in progress @ global_step 5200, g_loss 2.53974, d_loss 0.363029 accuracy 1\n",
      "Training in progress @ global_step 5250, g_loss 2.59844, d_loss 0.357153 accuracy 1\n",
      "Training in progress @ global_step 5300, g_loss 2.55237, d_loss 0.355292 accuracy 1\n",
      "Training in progress @ global_step 5350, g_loss 2.60065, d_loss 0.358387 accuracy 0.99\n",
      "Training in progress @ global_step 5400, g_loss 2.65399, d_loss 0.357099 accuracy 0.99\n",
      "Training in progress @ global_step 5450, g_loss 2.65087, d_loss 0.353316 accuracy 1\n",
      "Training in progress @ global_step 5500, g_loss 2.60539, d_loss 0.349519 accuracy 1\n",
      "Training in progress @ global_step 5550, g_loss 2.6884, d_loss 0.350318 accuracy 1\n",
      "Training in progress @ global_step 5600, g_loss 2.67683, d_loss 0.35109 accuracy 1\n",
      "Training in progress @ global_step 5650, g_loss 2.68638, d_loss 0.345282 accuracy 1\n",
      "Training in progress @ global_step 5700, g_loss 2.80388, d_loss 0.348804 accuracy 1\n",
      "Training in progress @ global_step 5750, g_loss 2.72806, d_loss 0.355528 accuracy 1\n",
      "Training in progress @ global_step 5800, g_loss 2.80003, d_loss 0.348709 accuracy 1\n",
      "Training in progress @ global_step 5850, g_loss 2.82775, d_loss 0.347493 accuracy 1\n",
      "Training in progress @ global_step 5900, g_loss 2.81698, d_loss 0.342245 accuracy 1\n",
      "Training in progress @ global_step 5950, g_loss 2.82116, d_loss 0.347614 accuracy 1\n",
      "Training in progress @ global_step 6000, g_loss 2.96923, d_loss 0.343746 accuracy 1\n",
      "Training in progress @ global_step 6050, g_loss 2.85468, d_loss 0.339787 accuracy 1\n",
      "Training in progress @ global_step 6100, g_loss 3.01894, d_loss 0.341236 accuracy 1\n",
      "Training in progress @ global_step 6150, g_loss 2.90495, d_loss 0.341543 accuracy 1\n",
      "Training in progress @ global_step 6200, g_loss 3.00468, d_loss 0.338625 accuracy 1\n",
      "Training in progress @ global_step 6250, g_loss 2.92886, d_loss 0.341355 accuracy 1\n",
      "Training in progress @ global_step 6300, g_loss 2.97227, d_loss 0.33935 accuracy 1\n",
      "Training in progress @ global_step 6350, g_loss 3.07268, d_loss 0.334906 accuracy 1\n",
      "Training in progress @ global_step 6400, g_loss 3.04007, d_loss 0.339697 accuracy 1\n",
      "Training in progress @ global_step 6450, g_loss 3.05927, d_loss 0.33575 accuracy 1\n",
      "Training in progress @ global_step 6500, g_loss 3.13362, d_loss 0.334359 accuracy 1\n",
      "Training in progress @ global_step 6550, g_loss 3.12733, d_loss 0.338946 accuracy 1\n",
      "Training in progress @ global_step 6600, g_loss 3.09057, d_loss 0.335263 accuracy 1\n",
      "Training in progress @ global_step 6650, g_loss 3.13527, d_loss 0.337318 accuracy 1\n",
      "Training in progress @ global_step 6700, g_loss 3.28953, d_loss 0.337382 accuracy 1\n",
      "Training in progress @ global_step 6750, g_loss 3.23204, d_loss 0.332969 accuracy 1\n",
      "Training in progress @ global_step 6800, g_loss 3.32134, d_loss 0.332974 accuracy 1\n",
      "Training in progress @ global_step 6850, g_loss 3.34427, d_loss 0.331892 accuracy 1\n",
      "Training in progress @ global_step 6900, g_loss 3.45225, d_loss 0.33465 accuracy 1\n",
      "Training in progress @ global_step 6950, g_loss 3.44662, d_loss 0.332179 accuracy 1\n",
      "Training in progress @ global_step 7000, g_loss 3.38239, d_loss 0.334255 accuracy 1\n",
      "Training in progress @ global_step 7050, g_loss 3.35121, d_loss 0.32968 accuracy 1\n",
      "Training in progress @ global_step 7100, g_loss 3.41405, d_loss 0.330622 accuracy 1\n",
      "Training in progress @ global_step 7150, g_loss 3.5207, d_loss 0.330959 accuracy 1\n",
      "Training in progress @ global_step 7200, g_loss 3.43323, d_loss 0.32929 accuracy 1\n",
      "Training in progress @ global_step 7250, g_loss 3.58644, d_loss 0.327024 accuracy 1\n",
      "Training in progress @ global_step 7300, g_loss 3.48605, d_loss 0.327477 accuracy 1\n",
      "Training in progress @ global_step 7350, g_loss 3.51315, d_loss 0.327069 accuracy 1\n",
      "Training in progress @ global_step 7400, g_loss 3.59777, d_loss 0.327228 accuracy 1\n",
      "Training in progress @ global_step 7450, g_loss 3.53692, d_loss 0.326284 accuracy 1\n",
      "Training in progress @ global_step 7500, g_loss 3.66977, d_loss 0.326185 accuracy 1\n",
      "Training in progress @ global_step 7550, g_loss 3.68286, d_loss 0.327668 accuracy 1\n",
      "Training in progress @ global_step 7600, g_loss 3.69825, d_loss 0.327159 accuracy 1\n",
      "Training in progress @ global_step 7650, g_loss 3.60397, d_loss 0.325617 accuracy 1\n",
      "Training in progress @ global_step 7700, g_loss 3.81601, d_loss 0.32625 accuracy 1\n",
      "Training in progress @ global_step 7750, g_loss 3.68113, d_loss 0.323992 accuracy 1\n",
      "Training in progress @ global_step 7800, g_loss 3.7933, d_loss 0.327767 accuracy 1\n",
      "Training in progress @ global_step 7850, g_loss 3.77444, d_loss 0.324146 accuracy 1\n",
      "Training in progress @ global_step 7900, g_loss 3.73616, d_loss 0.323867 accuracy 1\n",
      "Training in progress @ global_step 7950, g_loss 3.82817, d_loss 0.325369 accuracy 1\n",
      "Training in progress @ global_step 8000, g_loss 3.96972, d_loss 0.324802 accuracy 1\n",
      "Training in progress @ global_step 8050, g_loss 3.88476, d_loss 0.324088 accuracy 1\n",
      "Training in progress @ global_step 8100, g_loss 3.86199, d_loss 0.323648 accuracy 1\n",
      "Training in progress @ global_step 8150, g_loss 3.95371, d_loss 0.323461 accuracy 1\n",
      "Training in progress @ global_step 8200, g_loss 4.05459, d_loss 0.324184 accuracy 1\n",
      "Training in progress @ global_step 8250, g_loss 3.91004, d_loss 0.322849 accuracy 1\n",
      "Training in progress @ global_step 8300, g_loss 3.92292, d_loss 0.323765 accuracy 1\n",
      "Training in progress @ global_step 8350, g_loss 3.91759, d_loss 0.321436 accuracy 1\n",
      "Training in progress @ global_step 8400, g_loss 4.01775, d_loss 0.322132 accuracy 1\n",
      "Training in progress @ global_step 8450, g_loss 3.95739, d_loss 0.322116 accuracy 1\n",
      "Training in progress @ global_step 8500, g_loss 4.21041, d_loss 0.32126 accuracy 1\n",
      "Training in progress @ global_step 8550, g_loss 4.02077, d_loss 0.320337 accuracy 1\n",
      "Training in progress @ global_step 8600, g_loss 4.05078, d_loss 0.320498 accuracy 1\n",
      "Training in progress @ global_step 8650, g_loss 4.00227, d_loss 0.320313 accuracy 1\n",
      "Training in progress @ global_step 8700, g_loss 4.17045, d_loss 0.321031 accuracy 1\n",
      "Training in progress @ global_step 8750, g_loss 4.10153, d_loss 0.320551 accuracy 1\n",
      "Training in progress @ global_step 8800, g_loss 4.16763, d_loss 0.320533 accuracy 1\n",
      "Training in progress @ global_step 8850, g_loss 4.213, d_loss 0.320115 accuracy 1\n",
      "Training in progress @ global_step 8900, g_loss 4.25049, d_loss 0.320888 accuracy 1\n",
      "Training in progress @ global_step 8950, g_loss 4.28988, d_loss 0.320648 accuracy 1\n",
      "Training in progress @ global_step 9000, g_loss 4.22412, d_loss 0.319587 accuracy 1\n",
      "Training in progress @ global_step 9050, g_loss 4.38341, d_loss 0.319953 accuracy 1\n",
      "Training in progress @ global_step 9100, g_loss 4.17503, d_loss 0.319707 accuracy 1\n",
      "Training in progress @ global_step 9150, g_loss 4.28905, d_loss 0.320218 accuracy 1\n",
      "Training in progress @ global_step 9200, g_loss 4.26911, d_loss 0.319786 accuracy 1\n",
      "Training in progress @ global_step 9250, g_loss 4.37595, d_loss 0.320667 accuracy 1\n",
      "Training in progress @ global_step 9300, g_loss 4.40712, d_loss 0.320805 accuracy 1\n",
      "Training in progress @ global_step 9350, g_loss 4.33506, d_loss 0.320701 accuracy 1\n",
      "Training in progress @ global_step 9400, g_loss 4.5109, d_loss 0.319453 accuracy 1\n",
      "Training in progress @ global_step 9450, g_loss 4.2885, d_loss 0.321211 accuracy 1\n",
      "Training in progress @ global_step 9500, g_loss 4.41289, d_loss 0.318574 accuracy 1\n",
      "Training in progress @ global_step 9550, g_loss 4.38207, d_loss 0.318401 accuracy 1\n",
      "Training in progress @ global_step 9600, g_loss 4.52527, d_loss 0.31918 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 9650, g_loss 4.45863, d_loss 0.318823 accuracy 1\n",
      "Training in progress @ global_step 9700, g_loss 4.02696, d_loss 0.318093 accuracy 1\n",
      "Training in progress @ global_step 9750, g_loss 4.08821, d_loss 0.318965 accuracy 1\n",
      "Training in progress @ global_step 9800, g_loss 3.91542, d_loss 0.318484 accuracy 1\n",
      "Training in progress @ global_step 9850, g_loss 3.73565, d_loss 0.319782 accuracy 1\n",
      "Training in progress @ global_step 9900, g_loss 3.87359, d_loss 0.320983 accuracy 1\n",
      "Training in progress @ global_step 9950, g_loss 4.39162, d_loss 0.322068 accuracy 1\n",
      "Training in progress @ global_step 10000, g_loss 4.71012, d_loss 0.319594 accuracy 1\n",
      "Training in progress @ global_step 10050, g_loss 4.9604, d_loss 0.319368 accuracy 1\n",
      "Training in progress @ global_step 10100, g_loss 5.05723, d_loss 0.319313 accuracy 1\n",
      "Training in progress @ global_step 10150, g_loss 4.88235, d_loss 0.319196 accuracy 1\n",
      "Training in progress @ global_step 10200, g_loss 4.88475, d_loss 0.318351 accuracy 1\n",
      "Training in progress @ global_step 10250, g_loss 5.23525, d_loss 0.318497 accuracy 1\n",
      "Training in progress @ global_step 10300, g_loss 5.18275, d_loss 0.317158 accuracy 1\n",
      "Training in progress @ global_step 10350, g_loss 5.29582, d_loss 0.318065 accuracy 1\n",
      "Training in progress @ global_step 10400, g_loss 5.25902, d_loss 0.317394 accuracy 1\n",
      "Training in progress @ global_step 10450, g_loss 5.36166, d_loss 0.317294 accuracy 1\n",
      "Training in progress @ global_step 10500, g_loss 4.59966, d_loss 0.3175 accuracy 1\n",
      "Training in progress @ global_step 10550, g_loss 3.58461, d_loss 0.317422 accuracy 1\n",
      "Training in progress @ global_step 10600, g_loss 4.40879, d_loss 0.317676 accuracy 1\n",
      "Training in progress @ global_step 10650, g_loss 4.73198, d_loss 0.317593 accuracy 1\n",
      "Training in progress @ global_step 10700, g_loss 4.60177, d_loss 0.317649 accuracy 1\n",
      "Training in progress @ global_step 10750, g_loss 3.56516, d_loss 0.318845 accuracy 1\n",
      "Training in progress @ global_step 10800, g_loss 3.81361, d_loss 0.318484 accuracy 1\n",
      "Training in progress @ global_step 10850, g_loss 4.26813, d_loss 0.31978 accuracy 1\n",
      "Training in progress @ global_step 10900, g_loss 3.78735, d_loss 0.318962 accuracy 1\n",
      "Training in progress @ global_step 10950, g_loss 3.95479, d_loss 0.319727 accuracy 1\n",
      "Training in progress @ global_step 11000, g_loss 3.85622, d_loss 0.321056 accuracy 1\n",
      "Training in progress @ global_step 11050, g_loss 3.95387, d_loss 0.322361 accuracy 1\n",
      "Training in progress @ global_step 11100, g_loss 4.3167, d_loss 0.320089 accuracy 1\n",
      "Training in progress @ global_step 11150, g_loss 4.50025, d_loss 0.319942 accuracy 1\n",
      "Training in progress @ global_step 11200, g_loss 4.59051, d_loss 0.321176 accuracy 1\n",
      "Training in progress @ global_step 11250, g_loss 4.71712, d_loss 0.320501 accuracy 1\n",
      "Training in progress @ global_step 11300, g_loss 4.84125, d_loss 0.320109 accuracy 1\n",
      "Training in progress @ global_step 11350, g_loss 4.84568, d_loss 0.321224 accuracy 1\n",
      "Training in progress @ global_step 11400, g_loss 4.95561, d_loss 0.318657 accuracy 1\n",
      "Training in progress @ global_step 11450, g_loss 4.62689, d_loss 0.317853 accuracy 1\n",
      "Training in progress @ global_step 11500, g_loss 4.84492, d_loss 0.318121 accuracy 1\n",
      "Training in progress @ global_step 11550, g_loss 4.87027, d_loss 0.318235 accuracy 1\n",
      "Training in progress @ global_step 11600, g_loss 4.97283, d_loss 0.317854 accuracy 1\n",
      "Training in progress @ global_step 11650, g_loss 4.96411, d_loss 0.318045 accuracy 1\n",
      "Training in progress @ global_step 11700, g_loss 4.91935, d_loss 0.318471 accuracy 1\n",
      "Training in progress @ global_step 11750, g_loss 4.87513, d_loss 0.316758 accuracy 1\n",
      "Training in progress @ global_step 11800, g_loss 4.85744, d_loss 0.316894 accuracy 1\n",
      "Training in progress @ global_step 11850, g_loss 4.75987, d_loss 0.317531 accuracy 1\n",
      "Training in progress @ global_step 11900, g_loss 4.77914, d_loss 0.316954 accuracy 1\n",
      "Training in progress @ global_step 11950, g_loss 5.11642, d_loss 0.31673 accuracy 1\n",
      "Training in progress @ global_step 12000, g_loss 5.07092, d_loss 0.317454 accuracy 1\n",
      "Training in progress @ global_step 12050, g_loss 5.02705, d_loss 0.317227 accuracy 1\n",
      "Training in progress @ global_step 12100, g_loss 4.96984, d_loss 0.316758 accuracy 1\n",
      "Training in progress @ global_step 12150, g_loss 5.14759, d_loss 0.316571 accuracy 1\n",
      "Training in progress @ global_step 12200, g_loss 5.05118, d_loss 0.316453 accuracy 1\n",
      "Training in progress @ global_step 12250, g_loss 5.05824, d_loss 0.316663 accuracy 1\n",
      "Training in progress @ global_step 12300, g_loss 4.98566, d_loss 0.316705 accuracy 1\n",
      "Training in progress @ global_step 12350, g_loss 5.20677, d_loss 0.316576 accuracy 1\n",
      "Training in progress @ global_step 12400, g_loss 5.08308, d_loss 0.316845 accuracy 1\n",
      "Training in progress @ global_step 12450, g_loss 5.0078, d_loss 0.3169 accuracy 1\n",
      "Training in progress @ global_step 12500, g_loss 5.18405, d_loss 0.31686 accuracy 1\n",
      "Training in progress @ global_step 12550, g_loss 5.25303, d_loss 0.316183 accuracy 1\n",
      "Training in progress @ global_step 12600, g_loss 5.19529, d_loss 0.316325 accuracy 1\n",
      "Training in progress @ global_step 12650, g_loss 5.18978, d_loss 0.315946 accuracy 1\n",
      "Training in progress @ global_step 12700, g_loss 5.16078, d_loss 0.315662 accuracy 1\n",
      "Training in progress @ global_step 12750, g_loss 5.26459, d_loss 0.315496 accuracy 1\n",
      "Training in progress @ global_step 12800, g_loss 5.30105, d_loss 0.315939 accuracy 1\n",
      "Training in progress @ global_step 12850, g_loss 5.33609, d_loss 0.316473 accuracy 1\n",
      "Training in progress @ global_step 12900, g_loss 5.25808, d_loss 0.316548 accuracy 1\n",
      "Training in progress @ global_step 12950, g_loss 5.35788, d_loss 0.316077 accuracy 1\n",
      "Training in progress @ global_step 13000, g_loss 5.4873, d_loss 0.315364 accuracy 1\n",
      "Training in progress @ global_step 13050, g_loss 5.44734, d_loss 0.316316 accuracy 1\n",
      "Training in progress @ global_step 13100, g_loss 5.54844, d_loss 0.315343 accuracy 1\n",
      "Training in progress @ global_step 13150, g_loss 5.36005, d_loss 0.3159 accuracy 1\n",
      "Training in progress @ global_step 13200, g_loss 5.28503, d_loss 0.315513 accuracy 1\n",
      "Training in progress @ global_step 13250, g_loss 5.39204, d_loss 0.315672 accuracy 1\n",
      "Training in progress @ global_step 13300, g_loss 5.50413, d_loss 0.315902 accuracy 1\n",
      "Training in progress @ global_step 13350, g_loss 5.28148, d_loss 0.315384 accuracy 1\n",
      "Training in progress @ global_step 13400, g_loss 5.6066, d_loss 0.315156 accuracy 1\n",
      "Training in progress @ global_step 13450, g_loss 5.55683, d_loss 0.315314 accuracy 1\n",
      "Training in progress @ global_step 13500, g_loss 5.54259, d_loss 0.315192 accuracy 1\n",
      "Training in progress @ global_step 13550, g_loss 5.76319, d_loss 0.31515 accuracy 1\n",
      "Training in progress @ global_step 13600, g_loss 5.66207, d_loss 0.31572 accuracy 1\n",
      "Training in progress @ global_step 13650, g_loss 5.67366, d_loss 0.315143 accuracy 1\n",
      "Training in progress @ global_step 13700, g_loss 5.59405, d_loss 0.316098 accuracy 1\n",
      "Training in progress @ global_step 13750, g_loss 5.57768, d_loss 0.315023 accuracy 1\n",
      "Training in progress @ global_step 13800, g_loss 5.42462, d_loss 0.315132 accuracy 1\n",
      "Training in progress @ global_step 13850, g_loss 5.49774, d_loss 0.314997 accuracy 1\n",
      "Training in progress @ global_step 13900, g_loss 5.87794, d_loss 0.314764 accuracy 1\n",
      "Training in progress @ global_step 13950, g_loss 5.67016, d_loss 0.315311 accuracy 1\n",
      "Training in progress @ global_step 14000, g_loss 5.60394, d_loss 0.315622 accuracy 1\n",
      "Training in progress @ global_step 14050, g_loss 5.72856, d_loss 0.314888 accuracy 1\n",
      "Training in progress @ global_step 14100, g_loss 5.61254, d_loss 0.315697 accuracy 1\n",
      "Training in progress @ global_step 14150, g_loss 5.54835, d_loss 0.314982 accuracy 1\n",
      "Training in progress @ global_step 14200, g_loss 5.76332, d_loss 0.314739 accuracy 1\n",
      "Training in progress @ global_step 14250, g_loss 5.69288, d_loss 0.314452 accuracy 1\n",
      "Training in progress @ global_step 14300, g_loss 5.82127, d_loss 0.315377 accuracy 1\n",
      "Training in progress @ global_step 14350, g_loss 5.84446, d_loss 0.314828 accuracy 1\n",
      "Training in progress @ global_step 14400, g_loss 5.67373, d_loss 0.314851 accuracy 1\n",
      "Training in progress @ global_step 14450, g_loss 5.74611, d_loss 0.314476 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 14500, g_loss 5.65927, d_loss 0.314774 accuracy 1\n",
      "Training in progress @ global_step 14550, g_loss 6.08255, d_loss 0.31487 accuracy 1\n",
      "Training in progress @ global_step 14600, g_loss 5.78884, d_loss 0.31488 accuracy 1\n",
      "Training in progress @ global_step 14650, g_loss 5.90625, d_loss 0.314623 accuracy 1\n",
      "Training in progress @ global_step 14700, g_loss 5.93141, d_loss 0.314445 accuracy 1\n",
      "Training in progress @ global_step 14750, g_loss 5.8582, d_loss 0.314557 accuracy 1\n",
      "Training in progress @ global_step 14800, g_loss 5.79744, d_loss 0.314837 accuracy 1\n",
      "Training in progress @ global_step 14850, g_loss 5.85154, d_loss 0.314621 accuracy 1\n",
      "Training in progress @ global_step 14900, g_loss 6.07584, d_loss 0.314815 accuracy 1\n",
      "Training in progress @ global_step 14950, g_loss 5.88461, d_loss 0.314608 accuracy 1\n",
      "Training in progress @ global_step 15000, g_loss 6.04391, d_loss 0.314494 accuracy 1\n",
      "Training in progress @ global_step 15050, g_loss 5.95612, d_loss 0.315256 accuracy 1\n",
      "Training in progress @ global_step 15100, g_loss 5.85841, d_loss 0.314497 accuracy 1\n",
      "Training in progress @ global_step 15150, g_loss 5.92434, d_loss 0.314469 accuracy 1\n",
      "Training in progress @ global_step 15200, g_loss 6.01765, d_loss 0.314629 accuracy 1\n",
      "Training in progress @ global_step 15250, g_loss 5.92061, d_loss 0.31446 accuracy 1\n",
      "Training in progress @ global_step 15300, g_loss 6.02842, d_loss 0.314348 accuracy 1\n",
      "Training in progress @ global_step 15350, g_loss 5.93938, d_loss 0.314382 accuracy 1\n",
      "Training in progress @ global_step 15400, g_loss 6.00886, d_loss 0.314342 accuracy 1\n",
      "Training in progress @ global_step 15450, g_loss 6.05661, d_loss 0.314539 accuracy 1\n",
      "Training in progress @ global_step 15500, g_loss 6.04958, d_loss 0.314388 accuracy 1\n",
      "Training in progress @ global_step 15550, g_loss 6.03124, d_loss 0.314188 accuracy 1\n",
      "Training in progress @ global_step 15600, g_loss 6.07329, d_loss 0.314176 accuracy 1\n",
      "Training in progress @ global_step 15650, g_loss 6.08491, d_loss 0.314706 accuracy 1\n",
      "Training in progress @ global_step 15700, g_loss 6.14692, d_loss 0.314296 accuracy 1\n",
      "Training in progress @ global_step 15750, g_loss 6.14415, d_loss 0.314322 accuracy 1\n",
      "Training in progress @ global_step 15800, g_loss 6.25014, d_loss 0.314325 accuracy 1\n",
      "Training in progress @ global_step 15850, g_loss 6.24933, d_loss 0.314138 accuracy 1\n",
      "Training in progress @ global_step 15900, g_loss 6.31066, d_loss 0.314882 accuracy 1\n",
      "Training in progress @ global_step 15950, g_loss 6.31325, d_loss 0.314452 accuracy 1\n",
      "Training in progress @ global_step 16000, g_loss 6.38791, d_loss 0.314159 accuracy 1\n",
      "Training in progress @ global_step 16050, g_loss 6.28647, d_loss 0.314234 accuracy 1\n",
      "Training in progress @ global_step 16100, g_loss 6.25525, d_loss 0.314262 accuracy 1\n",
      "Training in progress @ global_step 16150, g_loss 6.3976, d_loss 0.314265 accuracy 1\n",
      "Training in progress @ global_step 16200, g_loss 6.42735, d_loss 0.314257 accuracy 1\n",
      "Training in progress @ global_step 16250, g_loss 6.2591, d_loss 0.314322 accuracy 1\n",
      "Training in progress @ global_step 16300, g_loss 6.42993, d_loss 0.314432 accuracy 1\n",
      "Training in progress @ global_step 16350, g_loss 6.41235, d_loss 0.3141 accuracy 1\n",
      "Training in progress @ global_step 16400, g_loss 6.35693, d_loss 0.314023 accuracy 1\n",
      "Training in progress @ global_step 16450, g_loss 6.33947, d_loss 0.314123 accuracy 1\n",
      "Training in progress @ global_step 16500, g_loss 6.3318, d_loss 0.314155 accuracy 1\n",
      "Training in progress @ global_step 16550, g_loss 6.43794, d_loss 0.314159 accuracy 1\n",
      "Training in progress @ global_step 16600, g_loss 6.43541, d_loss 0.314214 accuracy 1\n",
      "Training in progress @ global_step 16650, g_loss 6.5443, d_loss 0.314655 accuracy 1\n",
      "Training in progress @ global_step 16700, g_loss 6.46656, d_loss 0.314238 accuracy 1\n",
      "Training in progress @ global_step 16750, g_loss 6.31773, d_loss 0.314095 accuracy 1\n",
      "Training in progress @ global_step 16800, g_loss 6.56255, d_loss 0.313966 accuracy 1\n",
      "Training in progress @ global_step 16850, g_loss 6.37604, d_loss 0.313911 accuracy 1\n",
      "Training in progress @ global_step 16900, g_loss 6.5456, d_loss 0.313837 accuracy 1\n",
      "Training in progress @ global_step 16950, g_loss 6.49007, d_loss 0.314059 accuracy 1\n",
      "Training in progress @ global_step 17000, g_loss 6.59073, d_loss 0.313992 accuracy 1\n",
      "Training in progress @ global_step 17050, g_loss 6.50337, d_loss 0.314 accuracy 1\n",
      "Training in progress @ global_step 17100, g_loss 6.74686, d_loss 0.313917 accuracy 1\n",
      "Training in progress @ global_step 17150, g_loss 6.31095, d_loss 0.31404 accuracy 1\n",
      "Training in progress @ global_step 17200, g_loss 6.64299, d_loss 0.313904 accuracy 1\n",
      "Training in progress @ global_step 17250, g_loss 6.46494, d_loss 0.314122 accuracy 1\n",
      "Training in progress @ global_step 17300, g_loss 6.66832, d_loss 0.313887 accuracy 1\n",
      "Training in progress @ global_step 17350, g_loss 6.64424, d_loss 0.313856 accuracy 1\n",
      "Training in progress @ global_step 17400, g_loss 6.5722, d_loss 0.313944 accuracy 1\n",
      "Training in progress @ global_step 17450, g_loss 6.86065, d_loss 0.314194 accuracy 1\n",
      "Training in progress @ global_step 17500, g_loss 6.83425, d_loss 0.31384 accuracy 1\n",
      "Training in progress @ global_step 17550, g_loss 6.73488, d_loss 0.313959 accuracy 1\n",
      "Training in progress @ global_step 17600, g_loss 6.48824, d_loss 0.313895 accuracy 1\n",
      "Training in progress @ global_step 17650, g_loss 6.61907, d_loss 0.313954 accuracy 1\n",
      "Training in progress @ global_step 17700, g_loss 6.66045, d_loss 0.313924 accuracy 1\n",
      "Training in progress @ global_step 17750, g_loss 6.72186, d_loss 0.313844 accuracy 1\n",
      "Training in progress @ global_step 17800, g_loss 6.6992, d_loss 0.313783 accuracy 1\n",
      "Training in progress @ global_step 17850, g_loss 6.79864, d_loss 0.313834 accuracy 1\n",
      "Training in progress @ global_step 17900, g_loss 6.72163, d_loss 0.313934 accuracy 1\n",
      "Training in progress @ global_step 17950, g_loss 6.83928, d_loss 0.314185 accuracy 1\n",
      "Training in progress @ global_step 18000, g_loss 6.8341, d_loss 0.31383 accuracy 1\n",
      "Training in progress @ global_step 18050, g_loss 6.74941, d_loss 0.313828 accuracy 1\n",
      "Training in progress @ global_step 18100, g_loss 6.85635, d_loss 0.313798 accuracy 1\n",
      "Training in progress @ global_step 18150, g_loss 6.78405, d_loss 0.313755 accuracy 1\n",
      "Training in progress @ global_step 18200, g_loss 6.91763, d_loss 0.313848 accuracy 1\n",
      "Training in progress @ global_step 18250, g_loss 7.15721, d_loss 0.313751 accuracy 1\n",
      "Training in progress @ global_step 18300, g_loss 7.00227, d_loss 0.313782 accuracy 1\n",
      "Training in progress @ global_step 18350, g_loss 6.8573, d_loss 0.313849 accuracy 1\n",
      "Training in progress @ global_step 18400, g_loss 6.86425, d_loss 0.313766 accuracy 1\n",
      "Training in progress @ global_step 18450, g_loss 6.88417, d_loss 0.313742 accuracy 1\n",
      "Training in progress @ global_step 18500, g_loss 6.91559, d_loss 0.313723 accuracy 1\n",
      "Training in progress @ global_step 18550, g_loss 7.02034, d_loss 0.313702 accuracy 1\n",
      "Training in progress @ global_step 18600, g_loss 7.13191, d_loss 0.313699 accuracy 1\n",
      "Training in progress @ global_step 18650, g_loss 6.94515, d_loss 0.313785 accuracy 1\n",
      "Training in progress @ global_step 18700, g_loss 6.97523, d_loss 0.313721 accuracy 1\n",
      "Training in progress @ global_step 18750, g_loss 7.12482, d_loss 0.313771 accuracy 1\n",
      "Training in progress @ global_step 18800, g_loss 6.99612, d_loss 0.313728 accuracy 1\n",
      "Training in progress @ global_step 18850, g_loss 6.97507, d_loss 0.313676 accuracy 1\n",
      "Training in progress @ global_step 18900, g_loss 6.96121, d_loss 0.313612 accuracy 1\n",
      "Training in progress @ global_step 18950, g_loss 6.92685, d_loss 0.313756 accuracy 1\n",
      "Training in progress @ global_step 19000, g_loss 7.2194, d_loss 0.313725 accuracy 1\n",
      "Training in progress @ global_step 19050, g_loss 6.94392, d_loss 0.313744 accuracy 1\n",
      "Training in progress @ global_step 19100, g_loss 7.18775, d_loss 0.313612 accuracy 1\n",
      "Training in progress @ global_step 19150, g_loss 7.18604, d_loss 0.313689 accuracy 1\n",
      "Training in progress @ global_step 19200, g_loss 7.26337, d_loss 0.313617 accuracy 1\n",
      "Training in progress @ global_step 19250, g_loss 7.14557, d_loss 0.313642 accuracy 1\n",
      "Training in progress @ global_step 19300, g_loss 7.2507, d_loss 0.313609 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 19350, g_loss 7.28782, d_loss 0.313652 accuracy 1\n",
      "Training in progress @ global_step 19400, g_loss 7.13433, d_loss 0.313717 accuracy 1\n",
      "Training in progress @ global_step 19450, g_loss 7.26991, d_loss 0.313576 accuracy 1\n",
      "Training in progress @ global_step 19500, g_loss 7.37079, d_loss 0.313639 accuracy 1\n",
      "Training in progress @ global_step 19550, g_loss 7.21707, d_loss 0.313592 accuracy 1\n",
      "Training in progress @ global_step 19600, g_loss 7.28772, d_loss 0.31355 accuracy 1\n",
      "Training in progress @ global_step 19650, g_loss 7.19255, d_loss 0.313564 accuracy 1\n",
      "Training in progress @ global_step 19700, g_loss 7.41966, d_loss 0.313658 accuracy 1\n",
      "Training in progress @ global_step 19750, g_loss 7.46326, d_loss 0.313637 accuracy 1\n",
      "Training in progress @ global_step 19800, g_loss 7.4276, d_loss 0.313579 accuracy 1\n",
      "Training in progress @ global_step 19850, g_loss 7.27704, d_loss 0.313601 accuracy 1\n",
      "Training in progress @ global_step 19900, g_loss 7.35567, d_loss 0.313533 accuracy 1\n",
      "Training in progress @ global_step 19950, g_loss 7.41585, d_loss 0.31357 accuracy 1\n",
      "Training in progress @ global_step 20000, g_loss 7.24426, d_loss 0.31358 accuracy 1\n",
      "Training in progress @ global_step 20050, g_loss 7.2987, d_loss 0.313513 accuracy 1\n",
      "Training in progress @ global_step 20100, g_loss 7.5397, d_loss 0.313511 accuracy 1\n",
      "Training in progress @ global_step 20150, g_loss 7.49575, d_loss 0.313516 accuracy 1\n",
      "Training in progress @ global_step 20200, g_loss 7.51869, d_loss 0.313635 accuracy 1\n",
      "Training in progress @ global_step 20250, g_loss 7.63246, d_loss 0.31363 accuracy 1\n",
      "Training in progress @ global_step 20300, g_loss 7.39987, d_loss 0.313502 accuracy 1\n",
      "Training in progress @ global_step 20350, g_loss 7.58353, d_loss 0.31361 accuracy 1\n",
      "Training in progress @ global_step 20400, g_loss 7.47409, d_loss 0.313581 accuracy 1\n",
      "Training in progress @ global_step 20450, g_loss 7.52829, d_loss 0.313577 accuracy 1\n",
      "Training in progress @ global_step 20500, g_loss 7.59141, d_loss 0.313516 accuracy 1\n",
      "Training in progress @ global_step 20550, g_loss 7.42056, d_loss 0.313467 accuracy 1\n",
      "Training in progress @ global_step 20600, g_loss 7.62413, d_loss 0.313543 accuracy 1\n",
      "Training in progress @ global_step 20650, g_loss 7.70488, d_loss 0.313653 accuracy 1\n",
      "Training in progress @ global_step 20700, g_loss 7.45871, d_loss 0.313472 accuracy 1\n",
      "Training in progress @ global_step 20750, g_loss 7.53984, d_loss 0.313535 accuracy 1\n",
      "Training in progress @ global_step 20800, g_loss 7.43384, d_loss 0.313485 accuracy 1\n",
      "Training in progress @ global_step 20850, g_loss 7.76219, d_loss 0.313567 accuracy 1\n",
      "Training in progress @ global_step 20900, g_loss 7.5394, d_loss 0.313462 accuracy 1\n",
      "Training in progress @ global_step 20950, g_loss 7.72283, d_loss 0.313524 accuracy 1\n",
      "Training in progress @ global_step 21000, g_loss 7.83317, d_loss 0.313525 accuracy 1\n",
      "Training in progress @ global_step 21050, g_loss 7.77182, d_loss 0.313553 accuracy 1\n",
      "Training in progress @ global_step 21100, g_loss 7.72121, d_loss 0.313477 accuracy 1\n",
      "Training in progress @ global_step 21150, g_loss 7.72446, d_loss 0.313499 accuracy 1\n",
      "Training in progress @ global_step 21200, g_loss 7.90543, d_loss 0.313482 accuracy 1\n",
      "Training in progress @ global_step 21250, g_loss 7.66033, d_loss 0.31345 accuracy 1\n",
      "Training in progress @ global_step 21300, g_loss 7.78235, d_loss 0.313488 accuracy 1\n",
      "Training in progress @ global_step 21350, g_loss 7.71572, d_loss 0.313443 accuracy 1\n",
      "Training in progress @ global_step 21400, g_loss 7.80186, d_loss 0.31351 accuracy 1\n",
      "Training in progress @ global_step 21450, g_loss 7.91696, d_loss 0.313437 accuracy 1\n",
      "Training in progress @ global_step 21500, g_loss 7.82916, d_loss 0.313442 accuracy 1\n",
      "Training in progress @ global_step 21550, g_loss 8.08585, d_loss 0.313486 accuracy 1\n",
      "Training in progress @ global_step 21600, g_loss 8.14401, d_loss 0.31346 accuracy 1\n",
      "Training in progress @ global_step 21650, g_loss 8.20192, d_loss 0.313582 accuracy 1\n",
      "Training in progress @ global_step 21700, g_loss 7.88506, d_loss 0.313462 accuracy 1\n",
      "Training in progress @ global_step 21750, g_loss 7.87569, d_loss 0.313455 accuracy 1\n",
      "Training in progress @ global_step 21800, g_loss 7.85237, d_loss 0.313444 accuracy 1\n",
      "Training in progress @ global_step 21850, g_loss 7.87834, d_loss 0.313423 accuracy 1\n",
      "Training in progress @ global_step 21900, g_loss 7.82841, d_loss 0.313444 accuracy 1\n",
      "Training in progress @ global_step 21950, g_loss 8.0945, d_loss 0.313456 accuracy 1\n",
      "Training in progress @ global_step 22000, g_loss 7.94452, d_loss 0.313403 accuracy 1\n",
      "Training in progress @ global_step 22050, g_loss 7.96784, d_loss 0.313403 accuracy 1\n",
      "Training in progress @ global_step 22100, g_loss 7.98196, d_loss 0.313406 accuracy 1\n",
      "Training in progress @ global_step 22150, g_loss 8.1559, d_loss 0.313433 accuracy 1\n",
      "Training in progress @ global_step 22200, g_loss 8.11124, d_loss 0.313402 accuracy 1\n",
      "Training in progress @ global_step 22250, g_loss 8.23653, d_loss 0.313425 accuracy 1\n",
      "Training in progress @ global_step 22300, g_loss 8.0964, d_loss 0.31343 accuracy 1\n",
      "Training in progress @ global_step 22350, g_loss 8.03106, d_loss 0.313427 accuracy 1\n",
      "Training in progress @ global_step 22400, g_loss 8.09362, d_loss 0.313409 accuracy 1\n",
      "Training in progress @ global_step 22450, g_loss 8.13928, d_loss 0.313431 accuracy 1\n",
      "Training in progress @ global_step 22500, g_loss 8.2046, d_loss 0.31342 accuracy 1\n",
      "Training in progress @ global_step 22550, g_loss 8.28979, d_loss 0.313403 accuracy 1\n",
      "Training in progress @ global_step 22600, g_loss 7.99872, d_loss 0.313545 accuracy 1\n",
      "Training in progress @ global_step 22650, g_loss 8.0831, d_loss 0.313433 accuracy 1\n",
      "Training in progress @ global_step 22700, g_loss 8.15879, d_loss 0.313402 accuracy 1\n",
      "Training in progress @ global_step 22750, g_loss 8.16723, d_loss 0.313384 accuracy 1\n",
      "Training in progress @ global_step 22800, g_loss 8.28465, d_loss 0.313376 accuracy 1\n",
      "Training in progress @ global_step 22850, g_loss 8.16281, d_loss 0.313391 accuracy 1\n",
      "Training in progress @ global_step 22900, g_loss 8.06614, d_loss 0.313382 accuracy 1\n",
      "Training in progress @ global_step 22950, g_loss 8.19615, d_loss 0.31338 accuracy 1\n",
      "Training in progress @ global_step 23000, g_loss 8.28315, d_loss 0.313406 accuracy 1\n",
      "Training in progress @ global_step 23050, g_loss 8.43813, d_loss 0.313383 accuracy 1\n",
      "Training in progress @ global_step 23100, g_loss 8.37571, d_loss 0.313361 accuracy 1\n",
      "Training in progress @ global_step 23150, g_loss 8.22948, d_loss 0.313419 accuracy 1\n",
      "Training in progress @ global_step 23200, g_loss 8.46999, d_loss 0.313384 accuracy 1\n",
      "Training in progress @ global_step 23250, g_loss 8.43609, d_loss 0.313377 accuracy 1\n",
      "Training in progress @ global_step 23300, g_loss 8.18797, d_loss 0.313379 accuracy 1\n",
      "Training in progress @ global_step 23350, g_loss 8.34919, d_loss 0.313361 accuracy 1\n",
      "Training in progress @ global_step 23400, g_loss 8.54761, d_loss 0.313373 accuracy 1\n",
      "Training in progress @ global_step 23450, g_loss 8.20404, d_loss 0.313348 accuracy 1\n",
      "Training in progress @ global_step 23500, g_loss 8.38208, d_loss 0.313399 accuracy 1\n",
      "Training in progress @ global_step 23550, g_loss 8.5969, d_loss 0.313401 accuracy 1\n",
      "Training in progress @ global_step 23600, g_loss 8.43847, d_loss 0.31338 accuracy 1\n",
      "Training in progress @ global_step 23650, g_loss 8.66362, d_loss 0.313358 accuracy 1\n",
      "Training in progress @ global_step 23700, g_loss 8.61056, d_loss 0.313365 accuracy 1\n",
      "Training in progress @ global_step 23750, g_loss 8.55747, d_loss 0.313361 accuracy 1\n",
      "Training in progress @ global_step 23800, g_loss 8.38027, d_loss 0.313356 accuracy 1\n",
      "Training in progress @ global_step 23850, g_loss 8.60685, d_loss 0.313355 accuracy 1\n",
      "Training in progress @ global_step 23900, g_loss 8.51509, d_loss 0.313365 accuracy 1\n",
      "Training in progress @ global_step 23950, g_loss 8.54451, d_loss 0.313358 accuracy 1\n",
      "Training in progress @ global_step 24000, g_loss 8.61136, d_loss 0.313352 accuracy 1\n",
      "Training in progress @ global_step 24050, g_loss 8.66383, d_loss 0.313409 accuracy 1\n",
      "Training in progress @ global_step 24100, g_loss 8.51159, d_loss 0.313362 accuracy 1\n",
      "Training in progress @ global_step 24150, g_loss 8.50254, d_loss 0.313346 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 24200, g_loss 8.48414, d_loss 0.313352 accuracy 1\n",
      "Training in progress @ global_step 24250, g_loss 8.48465, d_loss 0.313364 accuracy 1\n",
      "Training in progress @ global_step 24300, g_loss 8.70068, d_loss 0.313335 accuracy 1\n",
      "Training in progress @ global_step 24350, g_loss 8.81672, d_loss 0.313356 accuracy 1\n",
      "Training in progress @ global_step 24400, g_loss 8.9163, d_loss 0.313357 accuracy 1\n",
      "Training in progress @ global_step 24450, g_loss 8.65425, d_loss 0.313338 accuracy 1\n",
      "Training in progress @ global_step 24500, g_loss 8.65417, d_loss 0.31335 accuracy 1\n",
      "Training in progress @ global_step 24550, g_loss 8.68648, d_loss 0.313344 accuracy 1\n",
      "Training in progress @ global_step 24600, g_loss 8.7376, d_loss 0.313368 accuracy 1\n",
      "Training in progress @ global_step 24650, g_loss 8.81355, d_loss 0.313324 accuracy 1\n",
      "Training in progress @ global_step 24700, g_loss 8.71531, d_loss 0.313353 accuracy 1\n",
      "Training in progress @ global_step 24750, g_loss 8.7234, d_loss 0.313318 accuracy 1\n",
      "Training in progress @ global_step 24800, g_loss 8.74364, d_loss 0.313339 accuracy 1\n",
      "Training in progress @ global_step 24850, g_loss 8.75419, d_loss 0.313327 accuracy 1\n",
      "Training in progress @ global_step 24900, g_loss 8.72356, d_loss 0.31335 accuracy 1\n",
      "Training in progress @ global_step 24950, g_loss 8.97132, d_loss 0.313346 accuracy 1\n",
      "Training in progress @ global_step 25000, g_loss 8.73394, d_loss 0.313346 accuracy 1\n",
      "Training in progress @ global_step 25050, g_loss 8.72405, d_loss 0.31337 accuracy 1\n",
      "Training in progress @ global_step 25100, g_loss 8.5814, d_loss 0.313338 accuracy 1\n",
      "Training in progress @ global_step 25150, g_loss 8.78316, d_loss 0.313338 accuracy 1\n",
      "Training in progress @ global_step 25200, g_loss 8.99033, d_loss 0.313328 accuracy 1\n",
      "Training in progress @ global_step 25250, g_loss 8.76319, d_loss 0.313337 accuracy 1\n",
      "Training in progress @ global_step 25300, g_loss 8.88618, d_loss 0.313326 accuracy 1\n",
      "Training in progress @ global_step 25350, g_loss 8.89326, d_loss 0.313325 accuracy 1\n",
      "Training in progress @ global_step 25400, g_loss 8.88413, d_loss 0.313325 accuracy 1\n",
      "Training in progress @ global_step 25450, g_loss 8.95062, d_loss 0.313322 accuracy 1\n",
      "Training in progress @ global_step 25500, g_loss 8.9651, d_loss 0.313328 accuracy 1\n",
      "Training in progress @ global_step 25550, g_loss 8.72941, d_loss 0.313339 accuracy 1\n",
      "Training in progress @ global_step 25600, g_loss 8.84902, d_loss 0.313329 accuracy 1\n",
      "Training in progress @ global_step 25650, g_loss 9.16017, d_loss 0.313345 accuracy 1\n",
      "Training in progress @ global_step 25700, g_loss 9.15665, d_loss 0.313329 accuracy 1\n",
      "Training in progress @ global_step 25750, g_loss 8.91797, d_loss 0.313317 accuracy 1\n",
      "Training in progress @ global_step 25800, g_loss 9.04071, d_loss 0.313317 accuracy 1\n",
      "Training in progress @ global_step 25850, g_loss 9.0849, d_loss 0.313312 accuracy 1\n",
      "Training in progress @ global_step 25900, g_loss 9.212, d_loss 0.313311 accuracy 1\n",
      "Training in progress @ global_step 25950, g_loss 9.14028, d_loss 0.313321 accuracy 1\n",
      "Training in progress @ global_step 26000, g_loss 9.01212, d_loss 0.313321 accuracy 1\n",
      "Training in progress @ global_step 26050, g_loss 9.09962, d_loss 0.313315 accuracy 1\n",
      "Training in progress @ global_step 26100, g_loss 9.30663, d_loss 0.313308 accuracy 1\n",
      "Training in progress @ global_step 26150, g_loss 8.98426, d_loss 0.313322 accuracy 1\n",
      "Training in progress @ global_step 26200, g_loss 9.02066, d_loss 0.313327 accuracy 1\n",
      "Training in progress @ global_step 26250, g_loss 9.08403, d_loss 0.313317 accuracy 1\n",
      "Training in progress @ global_step 26300, g_loss 9.20319, d_loss 0.313312 accuracy 1\n",
      "Training in progress @ global_step 26350, g_loss 9.15208, d_loss 0.313313 accuracy 1\n",
      "Training in progress @ global_step 26400, g_loss 9.12372, d_loss 0.313312 accuracy 1\n",
      "Training in progress @ global_step 26450, g_loss 9.00617, d_loss 0.313309 accuracy 1\n",
      "Training in progress @ global_step 26500, g_loss 9.12987, d_loss 0.31331 accuracy 1\n",
      "Training in progress @ global_step 26550, g_loss 9.40081, d_loss 0.313307 accuracy 1\n",
      "Training in progress @ global_step 26600, g_loss 9.32288, d_loss 0.313305 accuracy 1\n",
      "Training in progress @ global_step 26650, g_loss 9.32385, d_loss 0.313303 accuracy 1\n",
      "Training in progress @ global_step 26700, g_loss 9.2585, d_loss 0.313313 accuracy 1\n",
      "Training in progress @ global_step 26750, g_loss 9.36267, d_loss 0.313308 accuracy 1\n",
      "Training in progress @ global_step 26800, g_loss 9.34376, d_loss 0.313306 accuracy 1\n",
      "Training in progress @ global_step 26850, g_loss 9.3917, d_loss 0.313305 accuracy 1\n",
      "Training in progress @ global_step 26900, g_loss 9.31685, d_loss 0.313305 accuracy 1\n",
      "Training in progress @ global_step 26950, g_loss 9.26794, d_loss 0.313317 accuracy 1\n",
      "Training in progress @ global_step 27000, g_loss 9.40702, d_loss 0.313313 accuracy 1\n",
      "Training in progress @ global_step 27050, g_loss 9.16335, d_loss 0.313306 accuracy 1\n",
      "Training in progress @ global_step 27100, g_loss 9.36536, d_loss 0.313319 accuracy 1\n",
      "Training in progress @ global_step 27150, g_loss 9.28157, d_loss 0.313305 accuracy 1\n",
      "Training in progress @ global_step 27200, g_loss 9.52019, d_loss 0.313311 accuracy 1\n",
      "Training in progress @ global_step 27250, g_loss 9.47253, d_loss 0.313301 accuracy 1\n",
      "Training in progress @ global_step 27300, g_loss 9.73664, d_loss 0.313296 accuracy 1\n",
      "Training in progress @ global_step 27350, g_loss 9.69476, d_loss 0.313302 accuracy 1\n",
      "Training in progress @ global_step 27400, g_loss 9.25726, d_loss 0.313313 accuracy 1\n",
      "Training in progress @ global_step 27450, g_loss 9.69553, d_loss 0.313291 accuracy 1\n",
      "Training in progress @ global_step 27500, g_loss 9.65825, d_loss 0.3133 accuracy 1\n",
      "Training in progress @ global_step 27550, g_loss 9.49609, d_loss 0.313309 accuracy 1\n",
      "Training in progress @ global_step 27600, g_loss 9.42779, d_loss 0.313304 accuracy 1\n",
      "Training in progress @ global_step 27650, g_loss 9.65118, d_loss 0.313301 accuracy 1\n",
      "Training in progress @ global_step 27700, g_loss 9.53329, d_loss 0.313291 accuracy 1\n",
      "Training in progress @ global_step 27750, g_loss 9.76142, d_loss 0.313295 accuracy 1\n",
      "Training in progress @ global_step 27800, g_loss 9.68284, d_loss 0.313293 accuracy 1\n",
      "Training in progress @ global_step 27850, g_loss 9.55528, d_loss 0.313294 accuracy 1\n",
      "Training in progress @ global_step 27900, g_loss 9.49895, d_loss 0.313301 accuracy 1\n",
      "Training in progress @ global_step 27950, g_loss 9.61477, d_loss 0.313295 accuracy 1\n",
      "Training in progress @ global_step 28000, g_loss 9.51988, d_loss 0.313298 accuracy 1\n",
      "Training in progress @ global_step 28050, g_loss 9.85038, d_loss 0.313293 accuracy 1\n",
      "Training in progress @ global_step 28100, g_loss 9.56105, d_loss 0.313295 accuracy 1\n",
      "Training in progress @ global_step 28150, g_loss 9.47314, d_loss 0.313289 accuracy 1\n",
      "Training in progress @ global_step 28200, g_loss 9.54335, d_loss 0.313304 accuracy 1\n",
      "Training in progress @ global_step 28250, g_loss 9.78353, d_loss 0.3133 accuracy 1\n",
      "Training in progress @ global_step 28300, g_loss 9.63987, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 28350, g_loss 9.70197, d_loss 0.313295 accuracy 1\n",
      "Training in progress @ global_step 28400, g_loss 9.52425, d_loss 0.313288 accuracy 1\n",
      "Training in progress @ global_step 28450, g_loss 9.90096, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 28500, g_loss 9.56975, d_loss 0.313288 accuracy 1\n",
      "Training in progress @ global_step 28550, g_loss 9.7183, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 28600, g_loss 9.68158, d_loss 0.31329 accuracy 1\n",
      "Training in progress @ global_step 28650, g_loss 9.55319, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 28700, g_loss 9.94051, d_loss 0.313292 accuracy 1\n",
      "Training in progress @ global_step 28750, g_loss 9.86722, d_loss 0.313284 accuracy 1\n",
      "Training in progress @ global_step 28800, g_loss 9.78339, d_loss 0.313293 accuracy 1\n",
      "Training in progress @ global_step 28850, g_loss 9.94488, d_loss 0.313289 accuracy 1\n",
      "Training in progress @ global_step 28900, g_loss 9.89004, d_loss 0.313285 accuracy 1\n",
      "Training in progress @ global_step 28950, g_loss 9.6008, d_loss 0.313291 accuracy 1\n",
      "Training in progress @ global_step 29000, g_loss 9.74666, d_loss 0.313281 accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress @ global_step 29050, g_loss 9.87853, d_loss 0.313285 accuracy 1\n",
      "Training in progress @ global_step 29100, g_loss 9.77891, d_loss 0.313283 accuracy 1\n",
      "Training in progress @ global_step 29150, g_loss 10.1754, d_loss 0.313289 accuracy 1\n",
      "Training in progress @ global_step 29200, g_loss 9.973, d_loss 0.313284 accuracy 1\n",
      "Training in progress @ global_step 29250, g_loss 9.95415, d_loss 0.313284 accuracy 1\n",
      "Training in progress @ global_step 29300, g_loss 9.95313, d_loss 0.313285 accuracy 1\n",
      "Training in progress @ global_step 29350, g_loss 9.93464, d_loss 0.31329 accuracy 1\n",
      "Training in progress @ global_step 29400, g_loss 10.1624, d_loss 0.313283 accuracy 1\n",
      "Training in progress @ global_step 29450, g_loss 10.1239, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 29500, g_loss 9.89496, d_loss 0.313289 accuracy 1\n",
      "Training in progress @ global_step 29550, g_loss 9.85584, d_loss 0.313279 accuracy 1\n",
      "Training in progress @ global_step 29600, g_loss 10.038, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 29650, g_loss 10.1442, d_loss 0.313282 accuracy 1\n",
      "Training in progress @ global_step 29700, g_loss 10.2586, d_loss 0.313283 accuracy 1\n",
      "Training in progress @ global_step 29750, g_loss 9.94847, d_loss 0.313279 accuracy 1\n",
      "Training in progress @ global_step 29800, g_loss 10.0763, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 29850, g_loss 10.0128, d_loss 0.313284 accuracy 1\n",
      "Training in progress @ global_step 29900, g_loss 10.0797, d_loss 0.313279 accuracy 1\n",
      "Training in progress @ global_step 29950, g_loss 10.4265, d_loss 0.313288 accuracy 1\n",
      "Training in progress @ global_step 30000, g_loss 10.3551, d_loss 0.31328 accuracy 1\n",
      "Training in progress @ global_step 30050, g_loss 10.4021, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 30100, g_loss 10.3334, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 30150, g_loss 10.2396, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 30200, g_loss 10.1614, d_loss 0.313276 accuracy 1\n",
      "Training in progress @ global_step 30250, g_loss 10.2958, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 30300, g_loss 10.0742, d_loss 0.31328 accuracy 1\n",
      "Training in progress @ global_step 30350, g_loss 10.1234, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 30400, g_loss 10.207, d_loss 0.313279 accuracy 1\n",
      "Training in progress @ global_step 30450, g_loss 10.5407, d_loss 0.313283 accuracy 1\n",
      "Training in progress @ global_step 30500, g_loss 10.4129, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 30550, g_loss 10.3785, d_loss 0.31328 accuracy 1\n",
      "Training in progress @ global_step 30600, g_loss 10.4413, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 30650, g_loss 10.334, d_loss 0.313286 accuracy 1\n",
      "Training in progress @ global_step 30700, g_loss 10.2745, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 30750, g_loss 10.3878, d_loss 0.313276 accuracy 1\n",
      "Training in progress @ global_step 30800, g_loss 10.5686, d_loss 0.313276 accuracy 1\n",
      "Training in progress @ global_step 30850, g_loss 10.2988, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 30900, g_loss 10.5169, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 30950, g_loss 10.4851, d_loss 0.313281 accuracy 1\n",
      "Training in progress @ global_step 31000, g_loss 10.4432, d_loss 0.313283 accuracy 1\n",
      "Training in progress @ global_step 31050, g_loss 10.4774, d_loss 0.313274 accuracy 1\n",
      "Training in progress @ global_step 31100, g_loss 10.5098, d_loss 0.31328 accuracy 1\n",
      "Training in progress @ global_step 31150, g_loss 10.3979, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 31200, g_loss 10.5332, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 31250, g_loss 10.6896, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 31300, g_loss 10.5325, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 31350, g_loss 10.7172, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 31400, g_loss 10.655, d_loss 0.313282 accuracy 1\n",
      "Training in progress @ global_step 31450, g_loss 10.4483, d_loss 0.313274 accuracy 1\n",
      "Training in progress @ global_step 31500, g_loss 10.6348, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 31550, g_loss 10.6823, d_loss 0.313274 accuracy 1\n",
      "Training in progress @ global_step 31600, g_loss 10.6057, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 31650, g_loss 10.5887, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 31700, g_loss 10.5708, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 31750, g_loss 10.7635, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 31800, g_loss 10.5827, d_loss 0.313287 accuracy 1\n",
      "Training in progress @ global_step 31850, g_loss 10.6262, d_loss 0.313276 accuracy 1\n",
      "Training in progress @ global_step 31900, g_loss 10.75, d_loss 0.313274 accuracy 1\n",
      "Training in progress @ global_step 31950, g_loss 10.8969, d_loss 0.313271 accuracy 1\n",
      "Training in progress @ global_step 32000, g_loss 10.6801, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 32050, g_loss 10.6466, d_loss 0.313276 accuracy 1\n",
      "Training in progress @ global_step 32100, g_loss 10.7639, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 32150, g_loss 10.6235, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 32200, g_loss 10.5575, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 32250, g_loss 10.7631, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 32300, g_loss 10.6479, d_loss 0.313277 accuracy 1\n",
      "Training in progress @ global_step 32350, g_loss 11.0452, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 32400, g_loss 10.9492, d_loss 0.313271 accuracy 1\n",
      "Training in progress @ global_step 32450, g_loss 10.7798, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 32500, g_loss 10.966, d_loss 0.313268 accuracy 1\n",
      "Training in progress @ global_step 32550, g_loss 10.9498, d_loss 0.313271 accuracy 1\n",
      "Training in progress @ global_step 32600, g_loss 11.0647, d_loss 0.313274 accuracy 1\n",
      "Training in progress @ global_step 32650, g_loss 11.1263, d_loss 0.31327 accuracy 1\n",
      "Training in progress @ global_step 32700, g_loss 11.1026, d_loss 0.313271 accuracy 1\n",
      "Training in progress @ global_step 32750, g_loss 11.0501, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 32800, g_loss 10.9932, d_loss 0.313273 accuracy 1\n",
      "Training in progress @ global_step 32850, g_loss 10.7115, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 32900, g_loss 11.0929, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 32950, g_loss 11.0927, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33000, g_loss 10.8503, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33050, g_loss 11.0184, d_loss 0.313271 accuracy 1\n",
      "Training in progress @ global_step 33100, g_loss 11.0651, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 33150, g_loss 10.8888, d_loss 0.313275 accuracy 1\n",
      "Training in progress @ global_step 33200, g_loss 11.005, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33250, g_loss 10.9606, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33300, g_loss 11.1844, d_loss 0.313268 accuracy 1\n",
      "Training in progress @ global_step 33350, g_loss 11.1894, d_loss 0.313272 accuracy 1\n",
      "Training in progress @ global_step 33400, g_loss 11.2893, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33450, g_loss 11.119, d_loss 0.313268 accuracy 1\n",
      "Training in progress @ global_step 33500, g_loss 11.0483, d_loss 0.313268 accuracy 1\n",
      "Training in progress @ global_step 33550, g_loss 11.3002, d_loss 0.313268 accuracy 1\n",
      "Training in progress @ global_step 33600, g_loss 11.3458, d_loss 0.313269 accuracy 1\n",
      "Training in progress @ global_step 33650, g_loss 11.2407, d_loss 0.313269 accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4140e8b998a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                                                          \u001b[0mreal_flat\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                                          \u001b[0mreal_noise\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreal_noise_addition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                                          fake_noise: fake_noise_addition})\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        if not RESTORE:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer.add_graph(sess.graph)\n",
    "            saver = tf.train.Saver()\n",
    "        else: \n",
    "            latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "            print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        print \"Begin training ...\"\n",
    "        # Run training loop\n",
    "        for i in xrange(50000):\n",
    "            step = sess.run(global_step)\n",
    "\n",
    "            # Receive data (this will hang if IO thread is still running = this\n",
    "            # will wait for thread to finish & receive data)\n",
    "\n",
    "            sigma = max(0.5*(40000. - step) / (40000), 0.05)\n",
    "\n",
    "            # Update the generator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake_input = numpy.random.uniform(-5, 5, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "\n",
    "            real_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "            fake_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),28,28,1))\n",
    "\n",
    "\n",
    "            [  acc_fake, _ ] = sess.run(\n",
    "                [accuracy_fake, \n",
    "                 generator_optimizer], \n",
    "                feed_dict = {noise_tensor: fake_input,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "            # Update the discriminator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake = numpy.random.uniform(-5, 5, (int(BATCH_SIZE*0.5), 10*10))\n",
    "            real_data, label = mnist.train.next_batch(int(BATCH_SIZE*0.5))\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "            [generated_mnist, _] = sess.run([fake_images, \n",
    "                                            discriminator_optimizer], \n",
    "                                            feed_dict = {noise_tensor : fake_input,\n",
    "                                                         real_flat : real_data,\n",
    "                                                         real_noise: real_noise_addition,\n",
    "                                                         fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            [summary, g_l, acc_fake, d_l_r, acc] = sess.run(\n",
    "                [merged_summary, g_loss, accuracy_fake,\n",
    "                 d_loss_real, total_accuracy],\n",
    "                feed_dict = {noise_tensor : fake,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "            if step != 0 and step % 500 == 0:\n",
    "                saver.save(\n",
    "                    sess,\n",
    "                    LOGDIR+\"/checkpoints/save\",\n",
    "                    global_step=step)\n",
    "\n",
    "\n",
    "            # train_writer.add_summary(summary, i)\n",
    "            # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "            if step % 50 == 0:\n",
    "                print 'Training in progress @ global_step %d, g_loss %g, d_loss %g accuracy %g' % (step, g_l, d_l_r, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, let's load this network back into memory and generate a few fake images for visualization.  As you'll see, this network does \"OK\" but not amazingly well.  In the next post, we'll see a deep convolutional network that does much better at generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "        print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "        # We only need to make fake data and run it through the 'fake_images' tensor to see the output:\n",
    "        \n",
    "        fake_input = numpy.random.uniform(-1, 1, (int(BATCH_SIZE*0.5), 10*10))\n",
    "\n",
    "        [generated_images] = sess.run(\n",
    "                [fake_images], \n",
    "                feed_dict = {noise_tensor: fake_input})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to make it easier to draw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_images = numpy.reshape(generated_images, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(generate_images[index], cmap=\"Greys\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well ... It's OK.  It's very obviously not a digit, but it looks a bit like it *could* be a digit.  If you train this network on specific digits (1, 2,etc) I'm sure you will get much better performance.  Why not try it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
