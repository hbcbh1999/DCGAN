{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't looked at the previous tutorial, definitely check that out first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I'll extend the previous work to use more sophisticated networks.  In particular, I'll make the generator a deep convolutional network, and leave the discriminator as a shallow fully connected network.  This ought to produce much better digits that the previous fully connected networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll use the mnist data set here.  See the previous tutorial for some mnist basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, you can specify which device to use.  The next cell will tell you what's available, and you can select from there.  By default, I select \"/gpu:0\" but you can change this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15015752007795085796\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11990623847\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16509079372038761857\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()\n",
    "default_device = \"/gpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = mnist.train.next_batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.000005\n",
    "BATCH_SIZE=128\n",
    "N_INITIAL_FILTERS=64\n",
    "INCLUDE_NOISE=True\n",
    "MAX_EPOCH=100\n",
    "LOGDIR=\"./mnist_dcgan_logs/filters_{}_lr_{}_noise_{}\".format(N_INITIAL_FILTERS, BASE_LEARNING_RATE, INCLUDE_NOISE)\n",
    "RESTORE=False\n",
    "TRAINING=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real images and random noise, so make a placeholder for both.  Additionally, based on this blog post (http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/) I add random gaussian noise to the real and fake images as they are fed to the discriminator to help stabalize training.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        # Input noise to the generator:\n",
    "        noise_tensor = tf.placeholder(tf.float32, [None, 10*10], name=\"noise\")\n",
    "#         fake_input   = tf.reshape(noise_tensor, (tf.shape(noise_tensor)[0], 10,10, 1))\n",
    "        fake_input   = noise_tensor\n",
    "    \n",
    "\n",
    "        # Placeholder for the discriminator input:\n",
    "        real_flat  = tf.placeholder(tf.float32, [None, 784], name='x')\n",
    "        real_input  = tf.reshape(real_flat, (tf.shape(real_flat)[0], 28, 28, 1))\n",
    "\n",
    "        # We augment the input to the discriminator with gaussian noise\n",
    "        # This makes it harder for the discriminator to do it's job, preventing\n",
    "        # it from always \"winning\" the GAN min/max contest\n",
    "        real_noise = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"real_noise\")\n",
    "        fake_noise = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"fake_noise\")\n",
    "\n",
    "        real_images = real_noise + real_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input tensors (noise_tensor, real_images) are shaped in the 'flattened' way: (N/2, 100) for noise, (N/2, 784) for real images.  This lets me input the mnist images directly to tensorflow, as well as the noise.  They are then reshaped to be like tensorflow images (Batch, H, W, Filters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function to build the discriminator, using fully connected networks.  Note that a convolutional layer with the stride equal to the image size *is* a fully connected layer.\n",
    "\n",
    "This is unchanged from the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_tensor, is_training=True, reuse=False):\n",
    "    # Use scoping to keep the variables nicely organized in the graph.\n",
    "    # Scoping is good practice always, but it's *essential* here as we'll see later on\n",
    "    with tf.variable_scope(\"mnist_discriminator\", reuse=reuse):\n",
    "\n",
    "        x = input_tensor\n",
    "        current_filters=16\n",
    "        \n",
    "        # Downsample with strided convolutions, use a few layers of convolutions:\n",
    "        for i in xrange(3):\n",
    "        \n",
    "            # Batch norm:\n",
    "            with tf.variable_scope(\"conv2d_{}\".format(i)):\n",
    "                x = tf.layers.batch_normalization(x,\n",
    "                                              training=is_training,\n",
    "                                              trainable=True)\n",
    "\n",
    "                x = tf.layers.conv2d(x,\n",
    "                                     current_filters,\n",
    "                                     kernel_size = [3,3],\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',)\n",
    "\n",
    "\n",
    "                x = tf.nn.relu(x)\n",
    "            # Double the number of filters:\n",
    "            current_filters = 2*current_filters\n",
    "\n",
    "            with tf.variable_scope(\"downsample_{}\".format(i)):\n",
    "                # Batch norm, relu:\n",
    "                x = tf.layers.batch_normalization(x,\n",
    "                                                  training=is_training,\n",
    "                                                  trainable=True)\n",
    "        \n",
    "                # Downsample convolution with stride 2:\n",
    "                x = tf.layers.conv2d(x,\n",
    "                                     current_filters,\n",
    "                                     kernel_size=[3,3],\n",
    "                                     strides=[2,2],\n",
    "                                     padding='valid')\n",
    "\n",
    "\n",
    "\n",
    "                x = tf.nn.relu(x)\n",
    "   \n",
    "            with tf.variable_scope(\"conv2d_post_downsample_{}\".format(i)):\n",
    "                x = tf.layers.batch_normalization(x,\n",
    "                                              training=is_training,\n",
    "                                              trainable=True)\n",
    "\n",
    "                x = tf.layers.conv2d(x,\n",
    "                                     current_filters,\n",
    "                                     kernel_size = [3,3],\n",
    "                                     strides=(1, 1),\n",
    "                                     padding='same',)\n",
    "\n",
    "\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "\n",
    "        # Apply a 1x1 convolutoin and do global average pooling:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"batch_norm_4\")\n",
    "    \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             1,\n",
    "                             kernel_size=[1,1],\n",
    "                             strides=[1,1],\n",
    "                             padding='valid',\n",
    "                             name=\"conv2d1x1_downsample\")\n",
    "        \n",
    "                                          \n",
    "        # Pooling operation:\n",
    "        x = tf.layers.average_pooling2d(x,\n",
    "                                        pool_size=[2,2],\n",
    "                                        strides=[1,1],\n",
    "                                        name ='final_pooling')\n",
    "        \n",
    "        # Since we want to predict \"real\" or \"fake\", an output of 0 or 1 is desired.  sigmoid is perfect for this:\n",
    "        x = tf.nn.sigmoid(x, name=\"discriminator_sigmoid\")\n",
    "        #Reshape this to bring it down to just one output per image:\n",
    "        x = tf.reshape(x, (-1,))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        real_image_logits = build_discriminator(real_images, is_training=TRAINING, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "print real_image_logits.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to generate random images from noise:\n",
    "\n",
    "This function has been transformed into a deeper convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_tensor, n_initial_filters=512, is_training=True):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_generator\"):\n",
    "        current_filters = n_initial_filters\n",
    "\n",
    "        # Map the input vector to the first convolutional space:\n",
    "        # It's 10x10x1, we want to get to 4x4x512\n",
    "        with tf.variable_scope(\"project_and_reshape\"):\n",
    "            x = tf.layers.dense(input_tensor, 4*4*n_initial_filters, name=\"project\")\n",
    "            x = tf.reshape(x, (-1, 4, 4, n_initial_filters), name=\"reshape\")\n",
    "        \n",
    "        with tf.variable_scope(\"initial_conv2d\"):\n",
    "            x = tf.layers.batch_normalization(x,\n",
    "                                    center=False,\n",
    "                                    scale=False,\n",
    "                                    training=is_training,\n",
    "                                    trainable=True)\n",
    "\n",
    "            #Apply conv2d without changing shape:\n",
    "            x = tf.layers.conv2d(x, current_filters, kernel_size=[1,1],)\n",
    "\n",
    "            # Activation:\n",
    "            x = tf.nn.relu(x)\n",
    "\n",
    "        for i in xrange(3):\n",
    "            current_filters=int(0.5*current_filters)\n",
    "            \n",
    "        \n",
    "            with tf.variable_scope(\"upsample_{}\".format(i)):\n",
    "                # Apply batch norm, relu, and upsampling:\n",
    "                x = tf.layers.batch_normalization(x,\n",
    "                                        center=False,\n",
    "                                        scale=False,\n",
    "                                        training=is_training,\n",
    "                                        trainable=False)\n",
    "                x = tf.layers.conv2d_transpose(x,\n",
    "                                           current_filters,\n",
    "                                           kernel_size=[2,2],\n",
    "                                           strides=(2, 2))\n",
    "\n",
    "                x = tf.nn.relu(x)\n",
    "\n",
    "            with tf.variable_scope(\"same_scale_conv2d_{}\".format(i)):\n",
    "                x = tf.layers.batch_normalization(x,\n",
    "                                        center=False,\n",
    "                                        scale=False,\n",
    "                                        training=is_training,\n",
    "                                        trainable=False)\n",
    "\n",
    "                #Apply conv2d without changing shape:\n",
    "                x = tf.layers.conv2d(x, current_filters, kernel_size=[1,1],name=\"conv2d_1\")\n",
    "\n",
    "                # Activation:\n",
    "                x = tf.nn.relu(x, name=\"relu\")\n",
    "            print x.get_shape()\n",
    "        \n",
    "        \n",
    "        # Map the space onto a 28x28x1 region:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             1,\n",
    "                             kernel_size=[5,5],\n",
    "                             strides=[1,1],\n",
    "                             name=\"final_conv2d\")\n",
    "        \n",
    "        print x.get_shape()\n",
    "         \n",
    "        # The final non linearity applied here is to map the images onto the [-1,1] range.\n",
    "        x = tf.nn.tanh(x, name=\"generator_tanh\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 32)\n",
      "(?, 16, 16, 16)\n",
      "(?, 32, 32, 8)\n",
      "(?, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_images = build_generator(fake_input, n_initial_filters=N_INITIAL_FILTERS, is_training=TRAINING) + fake_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n",
      "(?, 100)\n"
     ]
    }
   ],
   "source": [
    "print fake_noise.get_shape()\n",
    "print fake_input.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to run the discriminator on the fake images, so set that up too.  Since it trains on both real and fake images, set reuse=True here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        fake_image_logits = build_discriminator(fake_images, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the generator and discriminator seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    # Build the loss functions:\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "            tf.summary.histogram(\"RealImageLogits\",real_image_logits)\n",
    "            tf.summary.histogram(\"FakeImageLogits\",fake_image_logits)\n",
    "            \n",
    "            #Discriminator loss on real images (classify as 1):\n",
    "            d_loss_total = -tf.reduce_mean(tf.log(real_image_logits) + tf.log(1. - fake_image_logits))\n",
    "\n",
    "            # This is the adverserial step: g_loss tries to optimize fake_logits to one,\n",
    "            # While d_loss_fake tries to optimize fake_logits to zero.\n",
    "            g_loss = -tf.reduce_mean(tf.log(fake_image_logits))\n",
    "\n",
    "            # This code is useful if you'll use tensorboard to monitor training:\n",
    "#             d_loss_summary = tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "#             d_loss_summary = tf.summary.scalar(\"Discriminator_Fake_Loss\", d_loss_fake)\n",
    "            d_loss_summary = tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss_total)\n",
    "            d_loss_summary = tf.summary.scalar(\"Generator_Loss\", g_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"accuracy\") as scope:\n",
    "            # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "            accuracy_real  = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_logits), \n",
    "                                                             tf.ones_like(real_image_logits)), \n",
    "                                                    tf.float32))\n",
    "            accuracy_fake  = tf.reduce_mean(tf.cast(tf.equal(tf.round(fake_image_logits), \n",
    "                                                             tf.zeros_like(fake_image_logits)), \n",
    "                                                    tf.float32))\n",
    "\n",
    "            total_accuracy = 0.5*(accuracy_fake +  accuracy_real)\n",
    "\n",
    "            # Again, useful for tensorboard:\n",
    "            acc_real_summary = tf.summary.scalar(\"Real_Accuracy\", accuracy_real)\n",
    "            acc_real_summary = tf.summary.scalar(\"Fake_Accuracy\", accuracy_fake)\n",
    "            acc_real_summary = tf.summary.scalar(\"Total_Accuracy\", total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            # Global steps are useful for restoring training:\n",
    "            global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            # Make sure the optimizers are only operating on their own variables:\n",
    "\n",
    "            all_variables      = tf.trainable_variables()\n",
    "            discriminator_vars = [v for v in all_variables if v.name.startswith('mnist_discriminator/')]\n",
    "            generator_vars     = [v for v in all_variables if v.name.startswith('mnist_generator/')]\n",
    "\n",
    "\n",
    "            discriminator_optimizer = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                d_loss_total, global_step=global_step, var_list=discriminator_vars)\n",
    "            generator_optimizer     = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                g_loss, global_step=global_step, var_list=generator_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        tf.summary.image('fake_images', fake_images, max_outputs=4)\n",
    "        tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Set up a saver:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ...\n",
      "Training in progress @ epoch 0, g_loss 0.754054, d_loss 1.44749 accuracy 0.289062\n",
      "Training in progress @ epoch 0.0533333, g_loss 0.753239, d_loss 1.44303 accuracy 0.75\n",
      "Training in progress @ epoch 0.106667, g_loss 0.75639, d_loss 1.44156 accuracy 0.742188\n",
      "Training in progress @ epoch 0.16, g_loss 0.731471, d_loss 1.41202 accuracy 0.746094\n",
      "Training in progress @ epoch 0.213333, g_loss 0.734356, d_loss 1.40955 accuracy 0.761719\n",
      "Training in progress @ epoch 0.266667, g_loss 0.735929, d_loss 1.40531 accuracy 0.746094\n",
      "Training in progress @ epoch 0.32, g_loss 0.741778, d_loss 1.40401 accuracy 0.753906\n",
      "Training in progress @ epoch 0.373333, g_loss 0.737714, d_loss 1.39078 accuracy 0.757812\n",
      "Training in progress @ epoch 0.426667, g_loss 0.742154, d_loss 1.38292 accuracy 0.761719\n",
      "Training in progress @ epoch 0.48, g_loss 0.750216, d_loss 1.37389 accuracy 0.78125\n",
      "Training in progress @ epoch 0.533333, g_loss 0.75271, d_loss 1.35127 accuracy 0.800781\n",
      "Training in progress @ epoch 0.586667, g_loss 0.763253, d_loss 1.32177 accuracy 0.777344\n",
      "Training in progress @ epoch 0.64, g_loss 0.775815, d_loss 1.26556 accuracy 0.78125\n",
      "Training in progress @ epoch 0.693333, g_loss 0.784203, d_loss 1.15124 accuracy 0.785156\n",
      "Training in progress @ epoch 0.746667, g_loss 0.815301, d_loss 1.00683 accuracy 0.816406\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: cross_entropy/RealImageLogits\n\t [[Node: cross_entropy/RealImageLogits = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](cross_entropy/RealImageLogits/tag, mnist_discriminator/Reshape/_439)]]\n\nCaused by op u'cross_entropy/RealImageLogits', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-32caf267d269>\", line 6, in <module>\n    tf.summary.histogram(\"RealImageLogits\",real_image_logits)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: cross_entropy/RealImageLogits\n\t [[Node: cross_entropy/RealImageLogits = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](cross_entropy/RealImageLogits/tag, mnist_discriminator/Reshape/_439)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f08a19e8fcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                              \u001b[0mreal_flat\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                              \u001b[0mreal_noise\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreal_noise_addition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                              fake_noise: fake_noise_addition})\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: cross_entropy/RealImageLogits\n\t [[Node: cross_entropy/RealImageLogits = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](cross_entropy/RealImageLogits/tag, mnist_discriminator/Reshape/_439)]]\n\nCaused by op u'cross_entropy/RealImageLogits', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-32caf267d269>\", line 6, in <module>\n    tf.summary.histogram(\"RealImageLogits\",real_image_logits)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/summary/summary.py\", line 192, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_logging_ops.py\", line 129, in _histogram_summary\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: cross_entropy/RealImageLogits\n\t [[Node: cross_entropy/RealImageLogits = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](cross_entropy/RealImageLogits/tag, mnist_discriminator/Reshape/_439)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        if not RESTORE:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer.add_graph(sess.graph)\n",
    "            saver = tf.train.Saver()\n",
    "        else: \n",
    "            latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "            print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        print \"Begin training ...\"\n",
    "        # Run training loop\n",
    "        for i in xrange(5000000):\n",
    "            step = sess.run(global_step)\n",
    "\n",
    "            # Receive data (this will hang if IO thread is still running = this\n",
    "            # will wait for thread to finish & receive data)\n",
    "            epoch = (1.0*i*BATCH_SIZE) / 60000.\n",
    "            if (epoch > MAX_EPOCH):\n",
    "                break\n",
    "            sigma = max(0.5*(10. - epoch) / (10), 0.05)\n",
    "            \n",
    "            # Update the generator:\n",
    "            # Prepare the input to the networks:\n",
    "            fake_input = numpy.random.normal(loc=0, scale=1, size=(BATCH_SIZE, 100))\n",
    "            real_data, label = mnist.train.next_batch(BATCH_SIZE)\n",
    "            real_data = 2*(real_data - 0.5)\n",
    "            if INCLUDE_NOISE:\n",
    "                real_noise_addition = numpy.random.normal(scale=sigma,size=(BATCH_SIZE,28,28,1))\n",
    "                fake_noise_addition = numpy.random.normal(scale=sigma,size=(BATCH_SIZE,28,28,1))\n",
    "            else:\n",
    "                real_noise_addition = numpy.zeros((BATCH_SIZE, 28,28, 1))\n",
    "                fake_noise_addition = numpy.zeros((BATCH_SIZE, 28,28, 1))\n",
    "\n",
    "            # Update the discriminator:\n",
    "            [_] = sess.run([discriminator_optimizer], \n",
    "                                            feed_dict = {noise_tensor : fake_input,\n",
    "                                                         real_flat : real_data,\n",
    "                                                         real_noise: real_noise_addition,\n",
    "                                                         fake_noise: fake_noise_addition})\n",
    "\n",
    "            # Update the generator:\n",
    "            fake_input = numpy.random.normal(loc=0, scale=1, size=(BATCH_SIZE, 100))\n",
    "#             real_data, label = mnist.train.next_batch(BATCH_SIZE)\n",
    "#             real_data = 2*(real_data - 0.5)\n",
    "            if INCLUDE_NOISE:\n",
    "                fake_noise_addition = numpy.random.normal(scale=sigma,size=(BATCH_SIZE,28,28,1))\n",
    "            else:\n",
    "                fake_noise_addition = numpy.zeros((BATCH_SIZE, 28, 28, 1))\n",
    "\n",
    "            \n",
    "            [ _ ] = sess.run([generator_optimizer], \n",
    "                feed_dict = {noise_tensor: fake_input,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "            \n",
    "            # Run a summary step:\n",
    "            [summary, g_l, d_l, acc] = sess.run(\n",
    "                [merged_summary, g_loss, d_loss_total, total_accuracy],\n",
    "                feed_dict = {noise_tensor : fake_input,\n",
    "                             real_flat : real_data,\n",
    "                             real_noise: real_noise_addition,\n",
    "                             fake_noise: fake_noise_addition})\n",
    "\n",
    "\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            if step != 0 and step % 500 == 0:\n",
    "                saver.save(\n",
    "                    sess,\n",
    "                    LOGDIR+\"/checkpoints/save\",\n",
    "                    global_step=step)\n",
    "\n",
    "\n",
    "            # train_writer.add_summary(summary, i)\n",
    "            # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "            if step % 50 == 0:\n",
    "                print 'Training in progress @ epoch %g, g_loss %g, d_loss %g accuracy %g' % (epoch, g_l, d_l, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, let's load this network back into memory and generate a few fake images for visualization.  As you'll see, this network does \"OK\" but not amazingly well.  In the next post, we'll see a deep convolutional network that does much better at generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        sess = tf.InteractiveSession()\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "        print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "        # We only need to make fake data and run it through the 'fake_images' tensor to see the output:\n",
    "        \n",
    "        fake_input = numpy.random.normal(loc=0, scale=1, size=(BATCH_SIZE, 100))\n",
    "        fake_noise_addition = numpy.zeros((BATCH_SIZE, 28, 28,1))\n",
    "\n",
    "        [generated_images] = sess.run(\n",
    "                [fake_images], \n",
    "                feed_dict = {noise_tensor: fake_input,\n",
    "                            fake_noise: fake_noise_addition})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape to make it easier to draw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_images = numpy.reshape(generated_images, (-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tile together lots of digits to see how the network is doing:\n",
    "final_image = numpy.zeros((280, 280))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        index = numpy.random.randint(BATCH_SIZE)\n",
    "        final_image[i*28:(i+1)*28, j*28:(j+1)*28] = generated_images[index].reshape(28, 28)\n",
    "        \n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(final_image, cmap=\"Greys\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it's just OK. I still think we can do better, but at least it's a demonstration of DCGAN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
