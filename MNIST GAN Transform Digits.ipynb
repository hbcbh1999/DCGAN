{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network Tutorial 00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll transform digits from one type to another with a deep neural network.  This is much more similar to the tasks we need to perform in physics (make simulation look like data), so it's more on point to that task.\n",
    "\n",
    "Additionally, there is the added bonus of classifying the transformed digits.  We should be able to fool not just a generic discriminator, but if we tranform 3 into 8, it should get classified as 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we still want to use the mnist digits, but we also want to keep track of the labels.  Since we'll be doing classification AND discrimination of real/fake, we'll want to have the one-hot labels this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow, you can specify which device to use.  The next cell will tell you what's available, and you can select from there.  By default, I select \"/gpu:0\" but you can change this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17578764505616291766\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11990623847\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 13103216971126205142\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print device_lib.list_local_devices()\n",
    "default_device = \"/gpu:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To make this a little nicer, I have a class that embeds images into 32x32 array with a random offset.  Setting one hot to true this time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mnist_helper(object):\n",
    "  \"\"\"docstring for mnist_helper\"\"\"\n",
    "  def __init__(self):\n",
    "    super(mnist_helper, self).__init__()\n",
    "    self.mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        \n",
    "\n",
    "  def next_multi_image_train(self, batch_size=16, return_labels=True):\n",
    "      # Build mnist images, each embedded into a 32x32 space\n",
    "      # instead of 28x28\n",
    "      batch, labels = self.mnist.train.next_batch(batch_size)\n",
    "      # build an output data container:\n",
    "      output = numpy.zeros((batch_size, 32, 32)) - 1\n",
    "      # Copy images to output:\n",
    "      image = 0\n",
    "      for i in xrange(batch_size):\n",
    "          x_start = numpy.random.randint(4)\n",
    "          y_start = numpy.random.randint(4)\n",
    "          output[i,x_start:x_start+28, y_start:y_start+28] = 2*(batch[image].reshape((28,28))) -1\n",
    "          image+= 1\n",
    "      if return_labels:\n",
    "        return numpy.reshape(output, [batch_size, 32, 32, 1]), labels\n",
    "      else:            \n",
    "        return numpy.reshape(output, [batch_size, 32, 32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_fetcher = mnist_helper()\n",
    "mnist_images, mnist_labels = data_fetcher.next_multi_image_train(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've loaded the mnist data from the tensorflow helper class, and asked for the next batch of images and labels.  Let's view those to see what the data looks like before delving into the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 32, 32, 1)\n",
      "mnist_labels.shape: (5, 10)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n",
    "print \"mnist_labels.shape: {}\".format(mnist_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, variables are arrays with the outermost dimension equal to 5.  The images, though, comes unpacked as a 1D array per image instead of a 2D array.  We can reshape this to what we're more familar with, since we know mnist images are 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_images.shape: (5, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print \"mnist_images.shape: {}\".format(mnist_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib gives a good interface for viewing these images in a notebook (or even in general, in python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is labeled as [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJKCAYAAAA84QGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzWVaxHEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83ba8c41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = numpy.random.randint(5)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mnist_images[index,:,:,0], cmap=\"Greys\", interpolation=\"none\")\n",
    "print \"This image is labeled as {}\".format(mnist_labels[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn like this, you can see each individual pixel clearly.  It's not a high resolution image, but you can clearly tell what the digit is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swapping labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the easiest first task in this problem is to make careful swaps of the labels in a deterministic way.  Specifically:\n",
    "\n",
    "* 0 &rightarrow; 6\n",
    "* 1 &rightarrow; 7\n",
    "* 2 &rightarrow; 5\n",
    "* 3 &rightarrow; 8\n",
    "* 4 &rightarrow; 9\n",
    "* 5 &rightarrow; 2\n",
    "* 6 &rightarrow; 0\n",
    "* 7 &rightarrow; 1\n",
    "* 8 &rightarrow; 3\n",
    "* 9 &rightarrow; 4\n",
    "\n",
    "Notice that these are closed cycles: each digit, if permuted twice, ought to map back to the original digit.  We can directly specify the permutation such that numpy can permute the digits for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation= numpy.array( [6, 7, 5, 8, 9, 2, 0, 1, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify we can do the permutation with the one hot tensors.  First we will collapse them into flat label list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = data_fetcher.next_multi_image_train(batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def permute_labels(labels, one_hot=True):\n",
    "    if not one_hot:\n",
    "        output = numpy.ndarray(labels.shape, dtype=int)\n",
    "        for i in xrange(len(permutation)):\n",
    "            p = permutation[i]\n",
    "            output[numpy.where(labels==i)] = int(p)\n",
    "        return output\n",
    "    else:\n",
    "        output = numpy.zeros(labels.shape)\n",
    "        flattened_labels = numpy.argmax(labels, axis=-1)\n",
    "        permuted_labels = permute_labels(flattened_labels, one_hot=False)\n",
    "        for i in xrange(len(permuted_labels)):\n",
    "            output[i,permuted_labels[i]] = 1\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels:\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[7 2 8 9 0]\n"
     ]
    }
   ],
   "source": [
    "print \"Original labels:\\n{}\".format(mnist_labels)\n",
    "mnist_flattened = numpy.argmax(mnist_labels, axis=-1)\n",
    "print mnist_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 3, 4, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute_labels(mnist_flattened, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute_labels(mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model for a GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start to put together a network for the GAN, first by defining some useful constants that we'll need to call on multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.00001\n",
    "BATCH_SIZE=50 # Keep this even\n",
    "DISCRIMINATOR_WEIGHT = 1.0\n",
    "CLASSIFIER_WEIGHT = 1.0\n",
    "RESTORE=False\n",
    "ADD_OUTPUT_NOISE=True\n",
    "N_PRETRAIN_ITERATIONS=5000\n",
    "LOGDIR=\"./mnist_trans_logs/output_noise_{}\".format(ADD_OUTPUT_NOISE)\n",
    "TRAINING=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's make sure we have the same graph by defining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the placeholders for the input variables.  We'll need to input both real image, labels and random noise, so make a placeholder for everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "\n",
    "        # Placeholder for the real input images:\n",
    "        real_images_input  = tf.placeholder(tf.float32, (None, 32, 32, 1), name=\"real_images_input\")\n",
    "        # Placeholder for the real labels:\n",
    "        real_labels  = tf.placeholder(tf.float32, [None, 10], name=\"real_labels\")\n",
    "   \n",
    "        # Placeholder for images that are going to be transformed\n",
    "        pre_trans_images  = tf.placeholder(tf.float32, (None, 32, 32, 1), name=\"pre_trans_images\")\n",
    "        # Placeholder for the transformed labels:\n",
    "        trans_labels = tf.placeholder(tf.float32, [None, 10], name=\"trans_labels\")\n",
    "           \n",
    "\n",
    "        # We augment the input to the discriminator with gaussian noise\n",
    "        # This makes it harder for the discriminator to do it's job, preventing\n",
    "        # it from always \"winning\" the GAN min/max contest\n",
    "        real_noise  = tf.placeholder(tf.float32, [None, 32, 32, 1], name=\"real_noise\")\n",
    "        trans_noise = tf.placeholder(tf.float32, [None, 32, 32, 1], name=\"fake_noise\")\n",
    "\n",
    "        # Add the noise to the real images:\n",
    "        real_images = real_images_input + real_noise\n",
    "        # trans_noise gets added later\n",
    "        \n",
    "        # To skip noise addition, we just feed in 0s at training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the networks.  In this case, there are 3: a transformer, a discriminator, and a classifier.  We will do the transformer first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function to transform images into new images.  This will be a residual network that is connected across spatial maps (U-ResNet, essentially).  It will return in the end a tensor of (B, 32, 32, 1) that matches the input shape. I'm not sure if it's better to use that directly as the created digits, or as a residual.\n",
    "\n",
    "Some notes on the network here:\n",
    "* Let's do this as a residual network\n",
    "* Each step of the network in downsampling will include a residual block and then a downsampling block\n",
    "* Downsampling will be done with strided convolutions\n",
    "* Upsampling will be a transpose convolution and then a residual block\n",
    "* The filters of each downsampling block (just before downsampling) will be concatenated with the filters of each upsampling block, just before the residual block\n",
    "\n",
    "Below, I define a residual block function, an upsample and a downsample block function.  These are reusable in the discriminator, transformer, and classifier.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(input_tensor,\n",
    "                   is_training,\n",
    "                   kernel=[3, 3],\n",
    "                   stride=[1, 1],\n",
    "                   name=\"\",\n",
    "                   reuse=False):\n",
    "    \"\"\"\n",
    "    @brief      Create a residual block and apply it to the input tensor\n",
    "\n",
    "    @param      input_tensor  The input tensor\n",
    "    @param      kernel        Size of convolutional kernel to apply\n",
    "    @param      n_filters     Number of output filters\n",
    "\n",
    "    @return     { Tensor with the residual network applied }\n",
    "    \"\"\"\n",
    "\n",
    "    # Residual block has the identity path summed with the output of\n",
    "    # BN/Relu/Conv2d applied twice\n",
    "\n",
    "    # Assuming channels last here:\n",
    "    n_filters = input_tensor.shape[-1]\n",
    "\n",
    "    with tf.variable_scope(name + \"_0\"):\n",
    "        # Batch normalization is applied first:\n",
    "        x = tf.layers.batch_normalization(input_tensor,\n",
    "                                          axis=-1,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNorm\",\n",
    "                                          reuse=reuse)\n",
    "\n",
    "\n",
    "        # Conv2d:\n",
    "        x = tf.layers.conv2d(x, n_filters,\n",
    "                             kernel_size=kernel,\n",
    "                             strides=stride,\n",
    "                             padding='same',\n",
    "                             trainable=is_training,\n",
    "                             name=\"Conv2D\",\n",
    "                             reuse=reuse)\n",
    "        # ReLU:\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "    # Apply everything a second time:\n",
    "    with tf.variable_scope(name + \"_1\"):\n",
    "\n",
    "        # Batch normalization is applied first:\n",
    "        x = tf.layers.batch_normalization(x,\n",
    "                                          axis=-1,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNorm\",\n",
    "                                          reuse=reuse)\n",
    "\n",
    "        # Conv2d:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             n_filters,\n",
    "                             kernel_size=kernel,\n",
    "                             strides=stride,\n",
    "                             padding='same',\n",
    "                             trainable=is_training,\n",
    "                             name=\"Conv2D\",\n",
    "                             reuse=reuse)\n",
    "\n",
    "    # Sum the input and the output:\n",
    "    with tf.variable_scope(name+\"_addition\"):\n",
    "        x = tf.add(x, input_tensor, name=\"Add\")\n",
    "        x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downsample_block(input_tensor,\n",
    "                     is_training,\n",
    "                     kernel=[3, 3],\n",
    "                     name=\"\",\n",
    "                     increase_filters=True,\n",
    "                     reuse = False):\n",
    "    \"\"\"\n",
    "    @brief      Create a residual block and apply it to the input tensor\n",
    "\n",
    "    @param      input_tensor  The input tensor\n",
    "    @param      kernel        Size of convolutional kernel to apply\n",
    "    @param      n_filters     Number of output filters\n",
    "\n",
    "    @return     { Tensor with the residual network applied }\n",
    "    \"\"\"\n",
    "\n",
    "    # Residual block has the identity path summed with the output of\n",
    "    # BN/Relu/Conv2d applied twice\n",
    "\n",
    "    # Assuming channels last here:\n",
    "    if not increase_filters:\n",
    "        n_filters = input_tensor.get_shape().as_list()[-1]\n",
    "    else:\n",
    "        n_filters = 2*input_tensor.get_shape().as_list()[-1]\n",
    "\n",
    "    with tf.variable_scope(name + \"_0\"):\n",
    "        # Batch normalization is applied first:\n",
    "        x = tf.layers.batch_normalization(input_tensor,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNorm\",\n",
    "                                          reuse=reuse)\n",
    "\n",
    "\n",
    "        # Conv2d:\n",
    "        x = tf.layers.conv2d(x, n_filters,\n",
    "                             kernel_size=kernel,\n",
    "                             strides=[2, 2],\n",
    "                             padding='same',\n",
    "                             trainable=is_training,\n",
    "                             name=\"Conv2D\",\n",
    "                             reuse=reuse)\n",
    "\n",
    "        # ReLU:\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsample_block(input_tensor,\n",
    "                   is_training,\n",
    "                   kernel=[3, 3],\n",
    "                   alpha = 0.0,\n",
    "                   name=\"\"):\n",
    "    \"\"\"\n",
    "    @brief      Create a residual block and apply it to the input tensor\n",
    "\n",
    "    @param      input_tensor  The input tensor\n",
    "    @param      kernel        Size of convolutional kernel to apply\n",
    "    @param      n_filters     Number of output filters\n",
    "\n",
    "    @return     { Tensor with the residual network applied }\n",
    "    \"\"\"\n",
    "\n",
    "    # Residual block has the identity path summed with the output of\n",
    "    # BN/Relu/Conv2d applied twice\n",
    "\n",
    "    # Assuming channels last here:\n",
    "    n_filters = int(0.5*input_tensor.get_shape().as_list()[-1])\n",
    "    # Prevent attempts to produce no filters at the next stage up:\n",
    "    if n_filters == 0:\n",
    "        n_filters = 1\n",
    "\n",
    "    with tf.variable_scope(name + \"_0\"):\n",
    "        # Batch normalization is applied first:\n",
    "        x = tf.layers.batch_normalization(input_tensor,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNorm\",\n",
    "                                          reuse=None)\n",
    "\n",
    "        # Conv2d:\n",
    "        x = tf.layers.conv2d_transpose(x, n_filters,\n",
    "                             kernel_size=kernel,\n",
    "                             strides=[2, 2],\n",
    "                             padding='same',\n",
    "                             trainable=True,\n",
    "                             name=\"Conv2DTrans\",\n",
    "                             reuse=None)\n",
    "        # ReLU:\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_transformer(input_tensor, n_initial_filters=32, is_training=True):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_transformer\"):\n",
    "       \n",
    "\n",
    "        # To make this code cleaner, the upsampling, downsampling, and residual block steps have all be \n",
    "        # written as functions above.\n",
    "        \n",
    "        # Map the input tensor with an intial convolution:\n",
    "        x = tf.layers.batch_normalization(input_tensor,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNormInitial\",\n",
    "                                          reuse=None)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, n_initial_filters, \n",
    "                            kernel_size=[5,5],\n",
    "                            strides=[1,1],\n",
    "                            padding=\"same\",\n",
    "                            trainable=True,\n",
    "                            name=\"InitialConv2D\")\n",
    "        \n",
    "        # Apply a series of downsampling blocks to take the spatial dimensions down as follows:\n",
    "        # (B, 32, 32, n_initial_filters)\n",
    "        # (B, 16, 16, 2*n_initial_filters)\n",
    "        # (B, 8, 8, 4*n_initial_filters)\n",
    "        # (B, 4, 4, 8*n_initial_filters)\n",
    "        # After each residual block, store the output to connect to the upsampling block\n",
    "        \n",
    "        feature_maps=dict()\n",
    "        for i in [32, 16, 8, 4]:\n",
    "            x = residual_block(x,\n",
    "                               is_training,\n",
    "                               name=\"residual_block_down_{}\".format(i))\n",
    "            feature_maps[i] = x\n",
    "            # Downsample:\n",
    "            x = downsample_block(x, is_training,\n",
    "                                name=\"downsample_block_{}\".format(i))\n",
    "\n",
    "            \n",
    "        # Now at the bottom of the network, apply a residual block:\n",
    "        x = residual_block(x, is_training,\n",
    "                           name=\"residual_block_bottom\")\n",
    "        \n",
    "        # Begin to upsample the network:\n",
    "        for i in [4, 8, 16, 32]:\n",
    "            x = upsample_block(x, is_training,\n",
    "                               name=\"upsample_block_{}\".format(i))\n",
    "            \n",
    "            \n",
    "            n_filters = x.get_shape().as_list()[-1]\n",
    "            \n",
    "            # Concatenate the appropriate filters:\n",
    "            x = tf.concat([x, feature_maps[i]], -1)\n",
    "            \n",
    "            # Apply a 1x1 filter to preserve the number of filters:\n",
    "            x = tf.layers.conv2d(x, n_filters,\n",
    "                                 kernel_size=[1,1],\n",
    "                                 strides=[1,1],\n",
    "                                 name =\"bottleneck_up_{}\".format(i))\n",
    "            \n",
    "            # Apply a residual block:\n",
    "            x = residual_block(x,\n",
    "                               is_training,\n",
    "                               name=\"residual_block_up_{}\".format(i))\n",
    "            \n",
    "        # Finally, we apply a bottleneck to map the tensors to a (B, 32, 32, 1) shaped output:\n",
    "        x = tf.layers.conv2d(x, 1,\n",
    "                             kernel_size=[1,1],\n",
    "                             strides=[1,1],\n",
    "                             name =\"FinalBottleneck\")\n",
    "\n",
    "        # Final activation is tanh\n",
    "        x = tf.nn.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        transformed_images = build_transformer(pre_trans_images, is_training=TRAINING)\n",
    "        transformed_images = transformed_images + trans_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classifier and Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next network will take the images, either real or transformed, and classify them into what digit they are.  We'll use the same weights for the real and transformed classifier, and the architecture will be similar to the downsample path of the transformer network.  The output here will contain one extra digit that will go to a sigmoid, not a softmax, to classify as real or not real (transformed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_classifier(input_tensor, n_initial_filters=16, is_training=True, reuse=False):\n",
    "    # Again, scoping is essential here:\n",
    "    with tf.variable_scope(\"mnist_classifier\", reuse=reuse):\n",
    "       \n",
    "\n",
    "        # To make this code cleaner, the upsampling, downsampling, and residual block steps have all be \n",
    "        # written as functions above.\n",
    "        \n",
    "        # Map the input tensor with an intial convolution:\n",
    "        x = tf.layers.batch_normalization(input_tensor,\n",
    "                                          training=is_training,\n",
    "                                          trainable=True,\n",
    "                                          name=\"BatchNormInitial\")\n",
    "        \n",
    "        x = tf.layers.conv2d(x, n_initial_filters, \n",
    "                            kernel_size=[5,5],\n",
    "                            strides=[1,1],\n",
    "                            padding=\"same\",\n",
    "                            trainable=True,\n",
    "                            name=\"InitialConv2D\")\n",
    "        \n",
    "        # Apply a series of downsampling blocks to take the spatial dimensions down as follows:\n",
    "        # (B, 32, 32, n_initial_filters)\n",
    "        # (B, 16, 16, 2*n_initial_filters)\n",
    "        # (B, 8, 8, 4*n_initial_filters)\n",
    "        # (B, 4, 4, 8*n_initial_filters)\n",
    "        # (B, 4, 4, 10) # Preparation for global average pooling\n",
    "        # After each residual block, store the output to connect to the upsampling block\n",
    "        \n",
    "        for i in [32, 16, 8, 4]:\n",
    "            x = residual_block(x,\n",
    "                               is_training,\n",
    "                               name=\"residual_block_down_{}\".format(i))\n",
    "            # Downsample:\n",
    "            if i != 4:\n",
    "                x = downsample_block(x, is_training,\n",
    "                                    name=\"downsample_block_{}\".format(i))\n",
    "\n",
    "\n",
    "        x = residual_block(x, is_training,\n",
    "                          name = \"final_residual_block\")\n",
    "            \n",
    "        # Finally, we apply a bottleneck to map the tensors to a (B, 4, 4, 11) shaped output:\n",
    "        x = tf.layers.conv2d(x, 11,\n",
    "                             kernel_size=[1,1],\n",
    "                             strides=[1,1],\n",
    "                             name =\"FinalBottleneck\")\n",
    "        \n",
    "        # Apply Global Average pooling:\n",
    "        x = tf.layers.average_pooling2d(x,\n",
    "                                        pool_size=[4,4],\n",
    "                                        strides=[1,1])\n",
    "\n",
    "        x = tf.reshape(x, [-1, 11])\n",
    "        \n",
    "        classification, discrimination = tf.split(x, [10,1], -1)\n",
    "        # For classification, final activation is softmax:\n",
    "        classification = tf.nn.softmax(classification)\n",
    "        discrimination = tf.nn.sigmoid(discrimination)\n",
    "        return classification, discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print real_images.get_shape()\n",
    "print transformed_images.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        real_image_classification, real_image_discrimination = build_classifier(real_images, is_training=TRAINING) \n",
    "        trans_image_classification, trans_image_discrimination = build_classifier(transformed_images, \n",
    "                                                                                  reuse = True, \n",
    "                                                                                  is_training=TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "print real_image_classification.get_shape()\n",
    "print real_image_discrimination.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a good amount of code happening above.  To summarize it, we have two sets of real input images and real labels.  On one path, we take a set of real images and ask the transformer network to turn them into alternate digits according to the permutation above.  On the other path, we do nothing to the input images. \n",
    "\n",
    "The Adversarial network does two tasks with the same network: decide if an image is real or transformed, and classify the image to a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our loss functions.  Note that we have to define the loss function for the real and transformed data seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    # Build the loss functions:\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"cross_entropy\") as scope:\n",
    "\n",
    "            #########################################################################\n",
    "            # Build the loss functions for the discriminator first:\n",
    "            #########################################################################\n",
    "            \n",
    "            # Discriminator loss on real images (classify as 1):\n",
    "            d_loss_real = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=real_image_discrimination,\n",
    "                                                        labels = tf.ones_like(real_image_discrimination)))\n",
    "            #Discriminator loss on transformed images (classify as 0):\n",
    "            d_loss_trans = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=trans_image_discrimination,\n",
    "                                                        labels = tf.zeros_like(trans_image_discrimination)))\n",
    "\n",
    "            # Total discriminator loss is the sum:\n",
    "            d_loss = d_loss_real + d_loss_trans\n",
    "\n",
    "            #########################################################################\n",
    "            # Build a loss function for the transformer to be incorrectly classifier:\n",
    "            #########################################################################\n",
    "            \n",
    "            t_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=trans_image_discrimination,\n",
    "                                                        labels = tf.ones_like(trans_image_discrimination)))\n",
    "\n",
    "            #########################################################################\n",
    "            # Build a loss function for the classifier for real and fake data:\n",
    "            #########################################################################\n",
    "\n",
    "            c_loss_real  = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=real_image_classification,\n",
    "                                                        labels=real_labels))\n",
    "            \n",
    "            c_loss_trans =tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=trans_image_classification,\n",
    "                                                        labels=trans_labels))\n",
    "            \n",
    "            # We don't define a classifier total loss here.  Instead, we'll train the\n",
    "            # classifier on real digits only, and use the trained classifer to \n",
    "            # drive the transformer\n",
    "            \n",
    "            t_loss_total = DISCRIMINATOR_WEIGHT*t_loss + CLASSIFIER_WEIGHT*c_loss_trans\n",
    "            d_loss_total = DISCRIMINATOR_WEIGHT*d_loss + CLASSIFIER_WEIGHT*c_loss_real\n",
    "            \n",
    "            # This code is useful if you'll use tensorboard to monitor training:\n",
    "            tf.summary.scalar(\"Discriminator_Real_Loss\", d_loss_real)\n",
    "            tf.summary.scalar(\"Discriminator_Trans_Loss\", d_loss_trans)\n",
    "            tf.summary.scalar(\"Discriminator_Total_Loss\", d_loss)\n",
    "            tf.summary.scalar(\"Transformer_Loss\", t_loss)\n",
    "            tf.summary.scalar(\"Classifier_Real_Loss\", c_loss_real)\n",
    "            tf.summary.scalar(\"Classifier_Trans_Loss\", c_loss_trans)\n",
    "            tf.summary.scalar(\"TransformerOptimizedLoss\",t_loss_total)\n",
    "            tf.summary.scalar(\"AdversaryOptimizedLoss\",d_loss_total)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also useful to compute accuracy, just to see how the training is going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"accuracy\") as scope:\n",
    "            # Compute the discriminator accuracy on real data, fake data, and total:\n",
    "            d_accuracy_real   = tf.reduce_mean(tf.cast(tf.equal(tf.round(real_image_discrimination), \n",
    "                                                             tf.ones_like(real_image_discrimination)), \n",
    "                                                    tf.float32))\n",
    "            d_accuracy_trans  = tf.reduce_mean(tf.cast(tf.equal(tf.round(trans_image_discrimination), \n",
    "                                                             tf.zeros_like(trans_image_discrimination)), \n",
    "                                                    tf.float32))\n",
    "\n",
    "            d_total_accuracy  = 0.5*(d_accuracy_trans +  d_accuracy_real)\n",
    "\n",
    "            c_accuracy_real = tf.reduce_mean(\n",
    "                tf.cast(tf.equal(tf.argmax(real_image_classification), \n",
    "                                 tf.argmax(real_labels)), \n",
    "                        tf.float32))\n",
    "            \n",
    "            c_accuracy_trans = tf.reduce_mean(\n",
    "                tf.cast(tf.equal(tf.argmax(trans_image_classification), \n",
    "                                 tf.argmax(trans_labels)), \n",
    "                        tf.float32))\n",
    "            \n",
    "            # Again, useful for tensorboard:\n",
    "            tf.summary.scalar(\"Discriminator_Real_Accuracy\", d_accuracy_real)\n",
    "            tf.summary.scalar(\"Discriminator_Trans_Accuracy\", d_accuracy_trans)\n",
    "            tf.summary.scalar(\"Discriminator_Total_Accuracy\", d_total_accuracy)\n",
    "            tf.summary.scalar(\"Classifier_Real_Accuracy\", c_accuracy_real)\n",
    "            tf.summary.scalar(\"Classifier_Trans_Accuracy\", c_accuracy_trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the generator and discriminator to compete and update seperately, we use two distinct optimizers.  This step is why it was essential earlier to have the scopes different for the generator and optimizer: we can select all variables in each scope to go to their own optimizer.  So, even though the generator loss calculation runs the discriminator, the update step for the generator **only** affects the variables inside the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"training\") as scope:\n",
    "            # Global steps are useful for restoring training:\n",
    "            global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "            # Make sure the optimizers are only operating on their own variables:\n",
    "\n",
    "            all_variables      = tf.trainable_variables()\n",
    "            generator_vars     = [v for v in all_variables if v.name.startswith('mnist_transformer/')]\n",
    "            classifier_vars     = [v for v in all_variables if v.name.startswith('mnist_classifier/')]\n",
    "\n",
    "            \n",
    "            transformer_optimizer   = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                t_loss_total, global_step=global_step, var_list=generator_vars)\n",
    "            classifier_optimizer    = tf.train.AdamOptimizer(BASE_LEARNING_RATE, 0.5).minimize(\n",
    "                d_loss_total, global_step=global_step, var_list=classifier_vars)\n",
    "\n",
    "            # This optimizer is for pretraining the classification task:\n",
    "            pretrain_class_optimizer = tf.train.AdamOptimizer(10*BASE_LEARNING_RATE).minimize(\n",
    "                c_loss_real, global_step=global_step, var_list=classifier_vars)\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to snapshot images into tensorboard to see how things are going, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        tf.summary.image('trans_images', transformed_images, max_outputs=4)\n",
    "        tf.summary.image('real_images', real_images, max_outputs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of philosophys on training GANs.  Here, we'll do something simple and just alternate updates. To save the network and keep track of training variables, set up a summary writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(default_device):\n",
    "    with g.as_default():\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Set up a saver:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a session for training using an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Begin training ...\n",
      "Training in progress @ global_step 0,\n",
      "Training in progress @ global_step 150,\n",
      "Training in progress @ global_step 300,\n",
      "Training in progress @ global_step 450,\n",
      "Training in progress @ global_step 600,\n",
      "Training in progress @ global_step 750,\n",
      "Training in progress @ global_step 900,\n",
      "Training in progress @ global_step 1050,\n",
      "Training in progress @ global_step 1200,\n",
      "Training in progress @ global_step 1350,\n",
      "Training in progress @ global_step 1500,\n",
      "Training in progress @ global_step 1650,\n",
      "Training in progress @ global_step 1800,\n",
      "Training in progress @ global_step 1950,\n",
      "Training in progress @ global_step 2100,\n",
      "Training in progress @ global_step 2250,\n",
      "Training in progress @ global_step 2400,\n",
      "Training in progress @ global_step 2550,\n",
      "Training in progress @ global_step 2700,\n",
      "Training in progress @ global_step 2850,\n",
      "Training in progress @ global_step 3000,\n",
      "Training in progress @ global_step 3150,\n",
      "Training in progress @ global_step 3300,\n",
      "Training in progress @ global_step 3450,\n",
      "Training in progress @ global_step 3600,\n",
      "Training in progress @ global_step 3750,\n",
      "Training in progress @ global_step 3900,\n",
      "Training in progress @ global_step 4050,\n",
      "Training in progress @ global_step 4200,\n",
      "Training in progress @ global_step 4350,\n",
      "Training in progress @ global_step 4500,\n",
      "Training in progress @ global_step 4650,\n",
      "Training in progress @ global_step 4800,\n",
      "Training in progress @ global_step 4950,\n",
      "Training in progress @ global_step 5200,\n",
      "Training in progress @ global_step 5500,\n",
      "Training in progress @ global_step 5800,\n",
      "Training in progress @ global_step 6100,\n",
      "Training in progress @ global_step 6400,\n",
      "Training in progress @ global_step 6700,\n",
      "Training in progress @ global_step 7000,\n",
      "Training in progress @ global_step 7300,\n",
      "Training in progress @ global_step 7600,\n",
      "Training in progress @ global_step 7900,\n",
      "Training in progress @ global_step 8200,\n",
      "Training in progress @ global_step 8500,\n"
     ]
    }
   ],
   "source": [
    "if TRAINING:\n",
    "    with tf.device(default_device):\n",
    "        with g.as_default():\n",
    "            sess = tf.InteractiveSession()\n",
    "            if not RESTORE:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                train_writer.add_graph(sess.graph)\n",
    "                saver = tf.train.Saver()\n",
    "            else: \n",
    "                latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "                print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "\n",
    "            data_fetcher = mnist_helper()\n",
    "\n",
    "            print \"Begin training ...\"\n",
    "            # Run training loop\n",
    "            for i in xrange(50000):\n",
    "                step = sess.run(global_step)\n",
    "\n",
    "\n",
    "                # As a reminder, here are the variables we need to supply values for:\n",
    "\n",
    "                #######################################################################################\n",
    "                ##  # Placeholder for the real input images:\n",
    "                ##  real_images_input  = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "                ##  # Placeholder for the real labels:\n",
    "                ##  real_labels  = tf.placeholder(tf.float32, [None, 10])\n",
    "                ##  \n",
    "                ##  # Placeholder for images that are going to be transformed\n",
    "                ##  pre_trans_images  = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "                ##  # Placeholder for the transformed labels:\n",
    "                ##  trans_labels = tf.placeholder(tf.float32, [None, 10])\n",
    "                ##  \n",
    "                ##  \n",
    "                ##  # We augment the input to the discriminator with gaussian noise\n",
    "                ##  # This makes it harder for the discriminator to do it's job, preventing\n",
    "                ##  # it from always \"winning\" the GAN min/max contest\n",
    "                ##  real_noise  = tf.placeholder(tf.float32, [None, 32, 32, 1], name=\"real_noise\")\n",
    "                ##  trans_noise = tf.placeholder(tf.float32, [None, 32, 32, 1], name=\"fake_noise\")\n",
    "                #######################################################################################\n",
    "\n",
    "                # Let's generate the noise tensors:\n",
    "\n",
    "                sigma = max(0.25*(30000. - step) / (30000), 0.02)\n",
    "                if ADD_OUTPUT_NOISE:\n",
    "                    real_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "                    trans_noise_addition = numpy.random.normal(scale=sigma,size=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "                else:\n",
    "                    real_noise_addition = numpy.zeros(shape=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "                    trans_noise_addition = numpy.zeros(shape=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "\n",
    "                # Get a set of real images and labels to transform:\n",
    "                pre_transformed_images, pre_transformed_labels = data_fetcher.next_multi_image_train(int(BATCH_SIZE*0.5))\n",
    "                # Get a set of real images and labels to do nothing to:\n",
    "                not_transformed_images, not_transformed_labels = data_fetcher.next_multi_image_train(int(BATCH_SIZE*0.5))\n",
    "\n",
    "                # Permute the labels that need to be permuted:\n",
    "                transformed_labels = permute_labels(pre_transformed_labels, one_hot=True)\n",
    "\n",
    "                # We have to update the discriminator, classifier, and generator.\n",
    "\n",
    "                \n",
    "                fd = {real_images_input : not_transformed_images,\n",
    "                      real_labels       : not_transformed_labels,\n",
    "                      pre_trans_images  : pre_transformed_images,\n",
    "                      trans_labels      : transformed_labels,\n",
    "                      real_noise        : real_noise_addition,\n",
    "                      trans_noise       : trans_noise_addition}\n",
    "\n",
    "                \n",
    "                #Pretrain the classification network for a few iterations:\n",
    "                if step < N_PRETRAIN_ITERATIONS: \n",
    "                    sess.run(pretrain_class_optimizer, feed_dict=fd)\n",
    "                else:\n",
    "                    sess.run(classifier_optimizer, feed_dict=fd)\n",
    "                    \n",
    "                # Update the transformer only after the pretraining:\n",
    "                if step >= N_PRETRAIN_ITERATIONS:\n",
    "                    sess.run(transformer_optimizer, feed_dict=fd)\n",
    "                    \n",
    "                summary = sess.run(merged_summary,feed_dict = fd)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "\n",
    "                if step != 0 and step % 500 == 0:\n",
    "                    saver.save(\n",
    "                        sess,\n",
    "                        LOGDIR+\"/checkpoints/save\",\n",
    "                        global_step=step)\n",
    "\n",
    "\n",
    "                # train_writer.add_summary(summary, i)\n",
    "                # sys.stdout.write('Training in progress @ step %d\\n' % (step))\n",
    "                if i % 150 == 0:\n",
    "                    print 'Training in progress @ global_step %d,' % (step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's take a look at some of the output of the network to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    with tf.device(default_device):\n",
    "        with g.as_default():\n",
    "            sess = tf.InteractiveSession()\n",
    "            latest_checkpoint = tf.train.latest_checkpoint(LOGDIR+\"/checkpoints/\")\n",
    "            print \"Restoring model from {}\".format(latest_checkpoint)\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "            data_fetcher = mnist_helper()\n",
    "\n",
    "            # For this, we want to be able to compare original to transformed images.  \n",
    "            pre_transformed_images, pre_transformed_labels = data_fetcher.next_multi_image_train(int(BATCH_SIZE*0.5))\n",
    "\n",
    "\n",
    "            # No need to add noise to the images, so pass in zeros for noise.\n",
    "            real_noise_addition = numpy.zeros(shape=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "            trans_noise_addition = numpy.zeros(shape=(int(BATCH_SIZE*0.5),32,32,1))\n",
    "\n",
    "\n",
    "            fd = {real_images_input : pre_transformed_images,\n",
    "                  real_labels       : pre_transformed_labels,\n",
    "                  pre_trans_images  : pre_transformed_images,\n",
    "                  real_noise        : real_noise_addition,\n",
    "                  trans_noise       : trans_noise_addition}\n",
    "\n",
    "\n",
    "            [generated_images, real_class, trans_class] = sess.run(\n",
    "                    [transformed_images, \n",
    "                     real_image_classification, \n",
    "                     trans_image_classification], \n",
    "                    feed_dict = fd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's take a look at these images.  We will draw the original on the left and the transformed image on the right, as well as the labels for both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(generated_images)):\n",
    "    original = pre_transformed_images[i].reshape((32,32))\n",
    "    new_img = generated_images[i].reshape((32,32))\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(20, 10))\n",
    "    ax1.imshow(original, cmap=\"Greys\", interpolation=\"none\")\n",
    "    ax2.imshow(new_img, cmap=\"Greys\", interpolation=\"none\")\n",
    "    plt.show()\n",
    "    \n",
    "    real_label = numpy.argmax(pre_transformed_labels[i])\n",
    "    real_prob = real_class[i][real_label]\n",
    "    selected_label = numpy.argmax(real_class[i])\n",
    "    print \"Original label is {}, selected label is {}, classification probability as {} is {}.\".format(\n",
    "        real_label, selected_label, real_label, real_prob )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
